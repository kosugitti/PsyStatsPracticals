# Statistical Hypothesis Testing (Null Hypothesis Statistical Testing)

Null hypothesis testing is likely a representative scene of using statistics in psychology.
These procedures are formalized, and some statistical packages can automatically provide results just by specifying the type of data. It has the significant advantage of yielding the same results for anyone who carries out the procedures and allowing for automation of these procedures. Its downside, however, is that beginners might obtain erroneous results without fully understanding the mechanisms, or malicious users could manipulate the results to their advantage. Scientific endeavors don't typically anticipate malevolent practitioners, and if such misconduct emerges, the only recourse is to expose and address it after the fact. Regrettably, we often see careless mistakes and unintended misuse among beginners.

In psychology, the failure to replicate the results of previous studies is referred to as the replication crisis, one cause of which is considered to be the improper use of statistical methods [@Ikeda2016]. Let's take a moment to revisit and carefully examine the procedures and logic behind null hypothesis testing.

## The Logic and Procedures of Null Hypothesis Testing

### The Purpose of Null Hypothesis Testing

The null hypothesis testing is a framework used to determine whether the findings obtained from the data collected through experiments or surveys carry any significance, and whether these can be generalized as characteristics of the population. It can be thought of as a kind of game with explicit methods and criteria. That's because null hypothesis testing involves a showdown between two models (approaches) known as the "null hypothesis" and the "alternative hypothesis," using a criterion called the "significance level" to determine the victor. The reason we use terms like victories or defeats is that the null hypothesis and the alternative hypothesis are in an exclusive relationship - it won't end in a conclusion where both are correct or both are wrong. However, we must remember, since the judgments are based on stochastic statistical logic, the results also involve elements of probability. The likelihood of erroneously adjudging the alternative hypothesis as right when the null hypothesis is, in fact, correct is not nil. Conversely, there is also a chance of wrongly ruling that the alternative hypothesis is incorrect (or the null hypothesis is correct) when in reality the null hypothesis is not correct. The former is known as a "Type 1 Error", and the latter as a "Type 2 Error". While we would like both these probabilities to be zero, that's not how it turns out, so when we denote Type 1 error as $\alpha$ and Type 2 error as $\beta$, we aim to keep both under a certain level. The null hypothesis testing was generalized and regulated procedure for this purpose. The aforementioned significance level is the allowable level for this $\alpha$, which is commonly set at 5% in psychology.

Thus, the concept of null hypothesis testing is fundamentally misguided by the idea of "manipulating to become significant," as the original intention is to control errors. Moreover, since statistical estimation is a mathematical procedure combined with a human intervention to make judgments that are easy for humans to accept, care should be taken not to attach excessive meaning to the results of null hypothesis testing or to become overly excited or discouraged by them.

### Procedure for Null Hypothesis Testing

When we generalize the procedure for null hypothesis testing, it becomes as follows.

1. Setting up the null hypothesis and alternative hypothesis.
2. Choose a Test Statistic.
3. Establishing the criteria for judgement.
4. Calculate the test statistic.
5. Make a decision.

Null hypothesis testing is applied to instances to determine whether there is a difference in the mean values between groups or whether there is statistical significance in the correlation coefficients.
Of course, this discussion is about estimating a population from a sample, and does not pertain to contexts such as theoretically judging the truth in physics, or instances where information of the entire population is available, like a census. Additionally, let's remind ourselves that the sample size is small and the confidence interval of the sample statistic is large, which means it cannot be determined without a framework.

As we cannot understand the state of the population, we set up a hypothesis. The Null Hypothesis reflects the idea of an "empty" hypothesis, stating that there is no mean difference (the difference is zero, $\mu_1 - \mu_2 = 0$) or the population correlation is zero ($\rho = 0$). The Alternative Hypothesis is created as a hypothesis that has an exclusive relationship with the Null Hypothesis, hence it is expressed in terms like "there is not no difference ($\mu_1 - \mu_2 \neq 0$)" and "the correlation is not zero ($\rho \neq 0$)".

You may wonder why we start with the Null Hypothesis being zero. This is because when we think about two exclusive hypotheses, there are countless situations in which the state is not zero, so it cannot be specifically asserted as a hypothesis (it wouldn't make sense to keep testing it indefinitely when the difference is 1, 1.1, 1.11, and so on).

The selection of test statistics is generally presented in a given manner, such as using 't' for the difference in mean values between two groups, 'F' for three or more groups, and 't' for testing correlation coefficients. These statistics are selected based on mathematical and statistical evidence. It's common to use a threshold of a 5% level for deciding. As the calculation of test statistics can be mechanically done according to algorithms, decisions are made based on objective indicators. Consequently, if we can categorize "what null hypothesis should be set in which situation", then this entire procedure can be carried out automatically.

However, let's take a careful look at the process again here, following each step meticulously.

## Testing the Correlation Coefficient

We will look at the test of the correlation coefficient, commonly referred to as the "correlation absence test." Instead of checking how big the correlation is or how significant it is, we check if it's not uncorrelated. Of course, if the sample correlation is calculated and isn't zero, then it's not uncorrelated. What we want to consider here is that the population correlation is not zero. In other words, even if the population correlation is in a state of zero, the fact that the sample correlation isn't zero is a natural thing under the context of sampling a small sample.

Let's check it out. First, consider creating an uncorrelated dataset. Use the `MASS` package in R, and generate random numbers from the probability distribution function of a multivariate normal distribution.

```{r,zeroR}
library(MASS)
set.seed(12345)
N <- 100000
X <- mvrnorm(N,
  mu = c(0, 0),
  Sigma = matrix(c(1, 0, 0, 1), ncol = 2),
  empirical = TRUE
)
head(X)
```

We have generated `r N` random numbers here. The object `X` that we created consists of two variables, as shown. The assumption here is that we are working with two variables that are correlated and follow a standard normal distribution. One could use the `rnorm` function twice to create the two variables but considering that we would derive them as a two-variable set, it leads us to contemplate a multivariate normal distribution. The multivariate normal distribution has a mean and standard deviation (SD) for each individual variable and also possesses covariance for variable combinations. Looking at the arguments for `mvrnorm`, `mu` represents a mean vector and `Sigma` is the variance-covariance matrix. Here, the variance-covariance matrix is a $2 \times 2$ square matrix, where diagonal elements represent variance and off-diagonal elements represent covariance. Covariance is expressed as the product of standard deviation and correlation coefficient.

**Variance**

The above formula calculates the variance, denoted as \( s_x^2 \), for a given set of data. It is calculated by summing the squares of the differences between each data point \( x_i \) and the mean of the data set \( \bar{x} \). This sum is then divided by the number of data points, \( n \), to calculate the variance. This average of squared differences from the mean represents how spread out the data is. As you can see, each difference \( (x_i - \bar{x}) \) is squared before being summed to ensure all differences are treated as positive values.

**Standard Deviation**

This equation represents the standard deviation (s_x), which is derived from the square root of variance (s_x²). The variance is calculated by summing up the squared differences between each observation in a dataset (represented as x_i) and the mean of the dataset (represented as x̄), and then dividing this total by the number of observations in the dataset (represented as n).

**Covariance**

This formula calculates the covariance between two variables. In this equation, "s_xy" represents the covariance of the variables x and y. The summation sign Σ (sum) is used to add together the products of each pair of corresponding elements in the x and y data sets. "x_i" and "y_i" are the individual data points in the variables x and y, while "n" signifies the total number of data points. The terms "\bar{x}" and "\bar{y}" denote the means of the x and y variables respectively. 

This measurement of covariance allows us to understand and quantify the relationship between two variables. A positive covariance indicates that the two variables tend to increase or decrease together, while a negative covariance suggests that one variable tends to increase when the other decreases.

**Correlation Coefficient**

The formula above is for computing the correlation coefficient (r_xy) between two variables, x and y. The correlation coefficient, often denoted as r, measures the strength and direction of a linear relationship between two variables. It's a unit-less measure ranging from -1 to +1, where -1 suggests a perfect negative linear relationship and +1 indicates a perfect positive linear relationship.

Let's break down this formula:

- $x_i$ and $y_i$ are the individual sample points indexed with i.
- $\bar{x}$ and $\bar{y}$ represent the mean (average) of the x and y variables.
- $s_{xy}$, in the numerator, is the covariance of x and y, which is the average of the product of their deviations from their respective means.
- $s_x$ and $s_y$, in the denominator, represent the standard deviations of x and y respectively. The standard deviation is a measure of how spread out the numbers are from their average value, larger values indicating more variability.

In sum, the correlation coefficient equals the covariance of x and y divided by the product of their standard deviations.

Learning and implementing such statistical calculations in R and RStudio will give you a better understanding and control over your data analysis process, helping you tease out meaningful patterns and relationships. These tools allow us to perform complex statistics with ease, alleviating the manual math work seen above. But understanding the underlying math continually lays the foundation for deeper and more insightful analysis.


**Variance-Covariance Matrix**

$$\Sigma = \begin{pmatrix} s_x^2 & s_{xy} \\ s_{yx} & s_y^2 \end{pmatrix}
\end{document}$$

This formula expresses a covariance matrix. In this matrix:

$s_x^2$ represents the variance of X,
$s_y^2$ stands for the variance of Y,
$s_{xy}$ and $s_{yx}$ denote the covariance between X and Y.

Variance is a measure that exhibits how much a set of values spreads out, while covariance provides insight into how much two variables vary together. These values play a crucial role in various statistical operations and analyses.
You didn't provide any Japanese text to translate into English. If you need to translate something, please provide the Japanese text.

The reason for using `Sigma = matrix(c(1,0,0,1),ncol = 2)` is to state that these two variables are uncorrelated and that their standard deviations are each 1. By the way, the option `empirical = TRUE` means it will adjust the generated random numbers to match the correlation coefficient of the variance-covariance matrix that has been set.

Let's visualize this. We will verify that the created random numbers are uncorrelated by using a scatter diagram.

```{r,ggR}
#| fig.height: 5
#| fig.width: 5
#| dev: "ragg_png"
library(tidyverse)
X %>%
  as.data.frame() %>%
  ggplot(aes(x = V1, y = V2)) +
  geom_point()
```

Let's also verify this numerically.

```{r,corr}
cor(X) %>% round(5)
```

We have confirmed that the generated random numbers are uncorrelated. Now, assuming this is the population, let's say we take a sample of `n = 20` from here. What would the correlation be at this time?
Let's do some calculations using R. Decide which rows to extract using the `sample` function, and assign only the corresponding rows to the `s1` object. Then let's try calculating the correlation coefficient.

```{r samp1}
selected_row <- sample(1:N, 20)
print(selected_row)

s1 <- X[selected_row, ]
cor(s1)
```

The correlation coefficient this time turned out to be `r cor(s1)[1,2]`. Even if the population correlation is 0, it's possible for a randomly selected 20 points to have a correlation coefficient (not 0). The question is, to what extent is this possible? In other words, if a researcher obtains a correlation using a sample of $n=20$ and it turns out to be $r = 0.14$, how likely is it that the sample came from a population correlation of $\rho = 0.0$?

## Distribution and Testing of Sample Correlation Coefficients

The sample correlation coefficient is a random variable, so its value changes each time a sample is taken, and the degree to which each value appears can be represented by the sample distribution.
So, what kind of sampling distribution might we follow? Let's try to approximate it by repeating the previous sampling and through the use of random numbers[^7.1].

[^7.1]: You can streamline this process by iteratively generating random numbers of sample size 20 from `mvrnorm`. To have a concrete image of the population, we chose the method of repeatedly sampling from a population with a parent correlation of `0`.

```{r,sampleDist}
iter <- 10000
samples <- c()
for (i in 1:iter) {
  selected_row <- sample(1:N, 20)
  s_i <- X[selected_row, ]
  cor_i <- cor(s_i)[1, 2]
  samples <- c(samples, cor_i)
}
df <- data.frame(R = samples)
# ヒストグラムの描画
g <- df %>%
  ggplot(aes(x = R)) +
  geom_histogram(binwidth = 0.01)
print(g)
```

When we examine the histogram, it can be seen that even in instances where the population correlation coefficient $\rho = 0.0$ and the sample size is 20, sample correlations around $r = 0.3$ or $r = 0.4$ can still occur to a certain extent.

Moreover, it seems that the sample distribution follows some symmetric theoretical distribution. From the findings of mathematical statistics, it is known that for the correlation coefficient, by converting the sample correlation coefficient using the following formula, it follows a t-distribution with degrees of freedom of $n-2$.

In the field of psychology, it's crucial to grasp the basics of statistics, specifically how to conduct a correlation analysis. You may confront this formula while dealing with such analysis:

$$ t = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}} $$

This can look a little daunting at first, but don't worry, let's break it down. This is the formula to calculate the value of 't'. Here, 't' is the t-statistic, 'r' is the correlation coefficient, and 'n' is the number of data pairs. This t-statistic is commonly used in hypothesis testing, specifically when dealing with Pearson's correlation. Remember, understanding this foundational statistical formula is vital to grasp more complex psychological analysis and concepts we will explore further. Using R and RStudio makes such statistical analysis more efficient and accessible.

```{r, tprob}
df %>%
  mutate(T = R * sqrt(18) / sqrt(1 - R^2)) %>%
  ggplot(aes(x = T)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.1) +
  # 自由度18のt分布の確率密度関数を追加
  stat_function(fun = dt, args = list(df = 18), color = "red", linewidth = 2) +
  # Y軸のラベルを変更
  ylab("Density")
```

We will use this to perform a test of correlation coefficient. Below, assuming a sample correlation coefficient of $r=0.5$ with a sample size of 20, we will explain the procedure step by step.

1. The null hypothesis sets the population correlation, $\rho = 0.0$. The alternative hypothesis is $\rho \neq 0.0$.
2. The test statistic is denoted as 't', which is the transformed correlation coefficient 'r'.
3. For our decision criteria, we will set $\alpha = 0.05$. That is, we want to control the probability of mistakenly rejecting the hypothesis that the population correlation is 0 to be less than 5%.
4. Calculate the test statistic. From $n=20, r=0.5$,
$$t = \frac{0.5\times(\sqrt{18})}{\sqrt{1-0.5^2}} = 2.449$$

This equation is saying that we are calculating the value of t (which is a statistic we often use in psychology) by taking 0.5 times the square root of 18 (which could be any given numbers in your actual data), then dividing this by the square root of 1 minus 0.5 squared. After calculating, you will find that the value of t in this case is 2.449.
5. The probability that the absolute value of the sample correlation coefficient exceeds 0.5 can be calculated from the theoretical values ​​of the t-distribution as follows. Alternatively, the critical value, which is 5% at both ends of the t-distribution, can be calculated in the following way.

```{r judge}
(1 - pt(0.5 * sqrt(18) / sqrt(1 - 0.5^2), df = 18)) * 2
qt(0.975, df = 18)
```

The important thing to note here is that the purpose of our current test is to determine "whether we can reject the null hypothesis that the population correlation is zero," with no interest in the sign of the correlation coefficient; we consider it in absolute value terms. The `pt` function represents the cumulative area up to a certain probability point, so by subtracting it from one, we are showing the probability of obtaining a value higher than this probability point. Because the $t$ distribution is a symmetric distribution, twice this value is the probability of occurrence when considered in absolute terms. If this value is less than 5%, we can judge it to be statistically significant. Based on our results, it is indeed fair to say that our findings are statistically significant.

Furthermore, a minor point to note in expression is that this probability refers to the probability of obtaining more extreme values "than or equal to" this observed value. It does not refer to the probability of obtaining this exact observed value. This is because probabilities are considered as areas, and there's no area assigned to a single point.

The function `qt` represents probabilistic points, from which we can determine if the realized value exceeded these points, hence statistically significant. The value calculated from our realized value is $t(18)=2.449$, and since this is greater than the critical value of $2.100$, we can conclude that it is significant.

## Probability of Errors in Two Types of Tests

In any statistical test, there is always a possibility of making wrong conclusions. For instance, when you conduct a hypothesis test, you might incorrectly reject the null hypothesis (an error known as "Type I Error"), or you may incorrectly hold on to the null hypothesis when it's false (an error referred to as "Type II Error").

Understanding these two types of errors and being able to control their probabilities is one of the indispensable skills in statistical analysis. This skill aids in producing more precise and reliable research findings. In the next section, we're going to explore how to do this using R and RStudio. It may be challenging at first, but remember, practice makes perfect.

Although we have carefully examined the calculations above, in practical situations we only have one sample and only one sample statistic is computed. Since this is your own valuable data, it might be difficult to intuitively grasp that it's just a specific case derived from the sample distribution.

When conducting a correlation coefficient test, use R's `cor.test` function as follows. Here, we use the `mvrnorm` function to create hypothetical data with a correlation coefficient of 0.5.

```{r,example}
set.seed(17)
n <- 20
sampleData <- mvrnorm(n,
  mu = c(0, 0),
  Sigma = matrix(c(1, 0.5, 0.5, 1), ncol = 2),
  empirical = TRUE
)
cor.test(sampleData[, 1], sampleData[, 2])
```

We can confirm that the values of t, degrees of freedom, and the $p$ value shown in the results correspond with the examples we showed earlier. Moreover, the confidence interval of the correlation coefficient and the sample correlation coefficient itself are also indicated. From the fact that this confidence interval does not cross zero, we can see that the null hypothesis would be rejected.

We already know that if we take a subset of a data set with a correlation of 0, the correlation coefficient won't necessarily be 0; it might be a number like 0.5. Even if the population correlation is 0, it's possible that the sample correlation might yield a value close to 0. What this suggests is that we shouldn't place too much importance on the values derived from the sample (of course, this assumes we are working with generalizations).
Furthermore, the null hypothesis is that the "population correlation is 0". So, even if it gets **rejected**, it merely means that "we can’t claim that the population correlation is 0". From this, it's inappropriate to argue that the population correlation is likely around $r=0.5$, or that the $p$ value being significantly lower than 5% at 2.4% speaks to the importance of the evidence. This query is based on the hypothetical situation of the population correlation being 0. It does not mean that we are examining what the actual degree of population correlation is. This point is easily misunderstood, so take particular care in understanding this detail.

Hopefully, by now, it has become easier to understand Type 1 and Type 2 errors in a more concrete way. Type 1 error is the probability of making a decision using the statistic calculated from the sample correlation, if the null hypothesis is correct. This is exactly what we saw in the previous steps.

Let's take a look at this from a different angle. `cor.test` can be used to calculate the confidence interval of a sample statistic. Let's analyze the proportion that correctly includes $ \rho = 0 $, which is the null hypothesis here, within this confidence interval.
The object returned by the `cor.test` function includes something named `conf.int`, which by default contains the 95% confidence interval.
Before starting the simulation, we created a two-column data frame to store the results. After the simulation, we used the `ifelse` function to determine whether the population correlation was included.

```{r,alpha}
set.seed(42)
iter <- 10000
intervals <- data.frame(matrix(NA, nrow = iter, ncol = 2))
names(intervals) <- c("Lower", "Upper")
for (i in 1:iter) {
  selected_row <- sample(1:N, 20)
  s_i <- X[selected_row, ]
  cor_i <- cor.test(s_i[, 1], s_i[, 2])
  intervals[i, ] <- cor_i$conf.int[1:2]
}
#
df <- intervals %>%
  mutate(FLG = ifelse(Lower <= 0 & Upper >= 0, 1, 0)) %>%
  summarise(type1error = mean(FLG)) %>%
  print()
```

In this instance, correct judgments were made at a rate of `r df$type1error*100`%. In other words, the rate at which errors occurred was `r (1-df$type1error)*100`%, confirming that the goal of keeping the Type 1 error probability below 5% was successfully achieved.

Similarly, a Type II error is the probability of accepting the null hypothesis when it is not correct. If we were to simulate this, it would go as follows. First, let's create a situation where the population correlation is not zero. For now, let's assume that the population correlation is 0.5 and try to plot the population distribution.


```{r,gg0.5}
#| fig.height: 5
#| fig.width: 5
#| dev: "ragg_png"
set.seed(12345)
N <- 100000
X <- mvrnorm(N,
  mu = c(0, 0),
  Sigma = matrix(c(1, 0.5, 0.5, 1), ncol = 2),
  empirical = TRUE
)

X %>%
  as.data.frame() %>%
  ggplot(aes(x = V1, y = V2)) +
  geom_point()
```

Let's now extract a dataset of sample size 20 and proceed to test it. Based on the test result, we'll create an object that is '1' if it is significant, and '0' if it is not, and then consider the correctness of our decision.


```{r t2error}
iter <- 10000
judges <- c()
for (i in 1:iter) {
  selected_row <- sample(1:N, 20)
  s_i <- X[selected_row, ]
  cor_i <- cor.test(s_i[, 1], s_i[, 2])
  judges <- c(judges, cor_i$p.value)
}
df <- data.frame(p = samples) %>%
  mutate(FLG = ifelse(p <= 0.05, 1, 0)) %>%
  summarise(
    sig = sum(FLG == 1),
    non.sig = sum(FLG == 0),
    type2error = non.sig / iter
  ) %>%
  print()
```

In this case, the correlation coefficient was 0.5, and the null hypothesis should have been rejected, but `r df$type2error*100`% of the cases were incorrectly deemed not significant. In psychological research, it is desirable to have this probability $\beta$ less than 0.2, or inversely, to have a detection rate of over 0.8. Therefore, it can be said that in this particular instance, there wasn't sufficient detection power.

Of course, in reality, we do not know what the parent correlation might be. It could be $0.3$, or it could be $-0.5$. This means that Type II errors are not within the researcher's control, and at most, the researcher can strive to collect samples pertaining to variables where a large correlation is anticipated.

The probability of Type 1 and Type 2 errors is a function of the sample size and effect size (in this case, the scale of the population correlation). As researchers can determine the sample size, they should form an estimate of the effect, decide on the standard for the error probability they want to control, and then reasonably determine the sample size.

## Practice Problems: An Example of Correlation Test

1. Let's try to approximate the sampling distribution, through the use of a histogram of random numbers, when we observe the sample correlation from a population with a correlation coefficient of 0, by taking a sample with a size of 10.
2. Similarly, let's try to approximate the sampling distribution when observing the sample correlation of a sample size of 50, using a histogram of random numbers. How does this differ when compared to sample sizes of 20 or 10?
3. Can we say that a sample correlation of $r=-0.3$ is statistically significant with a sample size of 50? Use `cor.test` to validate, and describe the testing outcomes and your decisions.
4. Let's suppose the sample correlation is $r=-0.3$. Would it be statistically significant if the sample sizes were 10, 20, 50, and 1000, respectively? Perform a test using `cor.test`, and summarize the results. What can we interpret from these findings?
5. Suppose the population correlation is $\rho = -0.3$. How much statistical power can we expect when the sample size is 20? Please approximate this using simulation.
