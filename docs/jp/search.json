[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "心理学統計実習",
    "section": "",
    "text": "はじめに\nこの資料は，授業「心理学統計演習」についてのものです。 演習という授業名にあるように，理論的な解説で「理解して進む」ことよりも，「手を動かして理解する」ことを目的にしています。\nこの資料を活用する人は，理論的な(いわゆる座学の)心理学統計を履修済みであることを前提にしています。また，資料集という位置付けですので，行間の説明が省略されていることが多くあります。その点は講義時間中の講話で補完していくつもりですので，不明な点があれば授業時間中に質問してください。",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "index.html#ライセンス等",
    "href": "index.html#ライセンス等",
    "title": "心理学統計実習",
    "section": "ライセンス等",
    "text": "ライセンス等\nこの資料はCreative Commons BY-SA(CC BY-SA)ライセンスVersion 4.0に基づいて提供されています。 著者に適切なクレジットを与える限り，この本を再利用，再編集，保持，改訂，再頒布(商用利用を含む)をすることができます。 もし再編集したり，このオープンなテキストを変更したい場合，すべてのバージョンにわたってこれと同じライセンス，CC BY-SA を適用しなければなりません。\nThis article is published under a Creative Commons BY-SA license (CC BY-SA) version 4.0. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA.",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "chapter01.html",
    "href": "chapter01.html",
    "title": "1  はじめようR/RStudio",
    "section": "",
    "text": "1.1 環境の準備",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#環境の準備",
    "href": "chapter01.html#環境の準備",
    "title": "1  はじめようR/RStudio",
    "section": "",
    "text": "1.1.1 Rのインストール\nRのインストールに関して，初心者でも利用可能な資料がオンラインで公開されている。\nRはThe Comprehensive R Archive Network，通称CRAN2というネットワークで公開されている。CRANのトップページにはダウンロードリンクが用意されており，自分のプラットフォームにあった最新版をダウンロードしよう3。\n\n\n1.1.2 RStudioのインストール\nRのインストールが終われば，次はRStudioをインストールしよう。 RStudioは総合開発環境(IDE)と呼ばれるものである。Rは単体で，統計の分析や関数の描画など，専門的な利用に耐えうる分析機能を有している。その本質はもちろん計算機能であって，計算を実行する命令文(スクリプト)を与えれば，必要な返答をあたえてくれる。このように分析の本質が計算機能であったとしても，実際の分析活動に際しては，スクリプトの下書きと清書，入出力データや描画ファイルの生成・管理，パッケージ(後述)の管理など，分析にまつわるさまざまな周辺活動が含まれる。喩えるなら料理の本質が包丁・まな板・コンロによる加工であったとしても，実際の調理に際しては，広い調理スペースや使いやすいシンク，ボウルやタッパーなどの補助的な調理器具があった方がスムーズにことが進む。 いわば，R単体で分析をするのは飯盒炊爨のような必要最低限かつワイルドな調理法であり，RStudioは総合的な調理環境を提供してくれるものなのである。\n繰り返しになるが，本質的にはR単体で作業が可能である。なるべく単純な環境を維持したいというのであればR単体での利用を否定するものではないが，RStudioはエディタや文書作成ソフトとしても有用であるので，本授業ではRStudioを使うことを前提とする4。\n\n\n1.1.3 環境の準備に関する導入サイト\n以下に執筆時点(2024年1月)で参照可能な，導入に関するWeb教材を挙げておく。自分に合ったものを適宜参照し，RとRStudioを自身のPC環境に導入してほしい。もちろん自身で「R RStudio インストール」などとして検索しても良いし，chatGPTに相談しても良い。\n\n1.1.3.1 For Windows\n\n東京大学・大学院農学生命科学研究科アグリバイオインフォマティクス教育研究プログラムによるPDF資料\n初心者向けRのインストールガイド\n関西学院大学商学部土方ゼミ資料\n多摩大学情報社会研究所・応用統計学室資料\n奥村 晴彦先生のページ\n\n\n\n1.1.3.2 For Macintosh\n\n東京大学・大学院農学生命科学研究科アグリバイオインフォマティクス教育研究プログラムによるPDF資料\nnoteの記事\nいちばんやさしい，医療統計記事\n\nなお，Macの場合はHomebrewなどのパッケージ管理ソフトを使って導入することもできる(し，そのほうがいい)。その場合は以下の資料を参照。\n\n群馬大学大学院医学系研究科機能形態学の記事\nコアラさばお氏のnote記事\nRyu Takahashi氏のQiita記事\nYuhki Yano氏のQiita記事",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#rstudioの基礎４つのペイン",
    "href": "chapter01.html#rstudioの基礎４つのペイン",
    "title": "1  はじめようR/RStudio",
    "section": "1.2 RStudioの基礎（４つのペイン）",
    "text": "1.2 RStudioの基礎（４つのペイン）\nここまでで，RおよびRStudioを利用する準備が整っているものとする。\nさて，RStudioを起動すると大きくわけて4つの領域に分かれた画面が出てくる。この領域のことをペインと呼ぶ。図中の「領域1」がないように見えるときもあるが，下のペインが最大化され折りたたまれているだけなので，ペイン上部のサイズ変更ボタンを操作することで出てくるだろう。\n\n\n\nRStudioの初期画面\n\n\nこのペインのレイアウトは，メニューのTools &gt; Global Options… &gt; Pane Layoutから変更することもできる。基本的に4分割であることに変わりはないが，自分が利用しやすい位置にレイアウトを変更するとよい。\n\n\n\nレイアウト変更画面。このほかにも背景色などを変えることもできる\n\n\n以下，各ペイン(領域)が何をするところかを簡単に解説する。\n\n1.2.1 領域1；エディタ・ペイン\nエディタ領域。Rのスクリプトはもちろん，レポートの文章など，基本的に入力するときはこのペインに書く。ここで作業するファイルの種類は，File &gt; New Fileから見ると明らかなように，R言語だけでなくC言語，Python言語などのスクリプトや，Rmd，md,Qmd,HTMLなどのマークアップ言語，StanやSQLなど特殊な言語などにも対応している。ペインの右下に現在開かれているファイルの種類が表示されているのを確認しておこう。\nR言語でスクリプトを書く例で解説しよう。Rは命令を逐次実行していくインタプリタ形式であり，ここに記述されたRコードを，右上のRunボタンでコンソールに送って計算を実行するように使う。一回の命令をコマンド，コマンドが積み重ねられた全体をスクリプト，あるいはプログラムと呼ぶ。複数のコマンドを実行したい場合は，エディタ領域で複数行選択してRunボタンを，スクリプトファイル全体を実行したいときはRunボタンのとなりにあるSourceを押す。CTRL+Enter(Macの場合はコマンド+Enter)でRunボタンのショートカットになる。\n\n\n1.2.2 領域2；コンソール・ペイン\nR単体で利用する場合は，ここのペインだけを利用するようなものである。すなわち，ここに示されているのがR本体というか，Rの計算機能そのものである。ここに「＞」の記号が表示されているところをプロンプトといい，プロンプトが表示されているときはRが入力待ちの状態である。\nRは逐次的に計算を行うので，プロンプトのある状態でコマンドを入力すると計算結果が返される。 ここに直接コマンドを書いて行っても良いが，書き間違えたりすることもあるし，コマンドが複数行に渡ることが一般的になってくるので，エディタ領域に清書するつもりで記述していったほうがよい。ごくたまに，一時的に確認したいことがある時だけ，直接コンソールを触るようにすると良い。\nなお，コンソールを綺麗にしたいときは右上の箒ボタンをおすとよい。\n\n\n1.2.3 領域3；環境ペイン\n基本的にこのペインと次の領域4のペインは複数のタブが含まれる。Pane Layoutでどちらにどのタブを含めるかを自分好みにカスタマイズすることもできる。ここでは代表的な2つのタブについてのみ言及する。\nEnvironmentタブは，Rの実行メモリ内に保管されている変数や関数などが表示されている。「変数や関数など」をまとめてオブジェクトというが，ここで内容や構造をGUIで確認することができる。\nHistoryタブは履歴である。これまでコンソールに送られてきたコマンドが順に記録されている。Historyタブからエディタ，コンソールにコマンドを送ることも可能であり，「さっきの命令をもう一度実行したい」といったときに参照すると良い。\n\n\n1.2.4 領域4；ファイルペイン\nここでも代表的なタブについてのみ解説する。\nFilesタブはMacでいうFinder，Windowsでいうエクスプローラーのような，ファイル操作画面である。フォルダの作成，ファイルの削除，リネーム，コピーなどの操作が可能である。\nPlotタブはRコマンドで描画命令が出された時の結果がここに表示される。RStudioの利点の一つは，このPlotから図をファイルにExportすることが可能であり，その際にファイルサイズやファイル形式を指定できるところにある。\nPackagesタブは読み込まれているパッケージ，(読み込まれていないが)保管しているパッケージのリストが表示されている。新しくパッケージを導入するときも，ここのinstallボタンから可能であり，保管しているパッケージのアップデートもボタンひとつで可能である。なお，パッケージについては後ほど言及する。\nHelpタブはRコマンドでヘルプを表示する命令(help関数)が実行された時の結果が表示される領域である。ヘルプを使うことで関数の引数，戻り値，使用例などを参照できる。\n\n\n1.2.5 そのほかのタブ\nそのほか，表示の有無もオプションになっているようないくつかのタブについて，簡単に解説しておく。\nConnectionsタブはRを外部データベースなどに繋げるときに参照する。大規模データをローカルにすべて取り込むことなく，SQLで必要なテーブルだけ取り出すといった操作をする際は必要になってくるだろう。\nGitタブはR，とくにRプロジェクト(後述)のバージョンを管理するときに利用する。Gitとは複数のプログラマによって同時並行的にプログラムを作っていく時の管理システムである。時系列的な差分の記録を得意とするシステムなので，レポートの作成時などに応用すればラボノートの記録としても利用できる。\nBuildタブはRパッケージやWebサイトを構築するときに利用する。なおこの資料もRStudioを利用して作られており，資料を生成(原稿からHTMLやPDFにする)ときにはこのタブを利用している。\nTutorialタブはチュートリアルツアーを楽しむ時のタブである。\nViewerタブはRStudioで作られたHTMLやPDFなどを見るためのタブである。\nPresentationタブはRStudioで作られたプレゼンテーションを見るためのタブである。\nTerminalタブはWindows/MacでいうTerminal，Linuxでいう端末についてのタブであり，Rに限らず，コマンドラインを通じてOSに命令するときに使う。\nBackground Jobsタブはその名の通りバックグラウンドで作業をさせるときに利用する。Rは基本的にシングルコアで計算が実行されるが，このタブを使ってスクリプトファイルをバックグラウンドで実行することで並列的に作業が可能になる。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#rのパッケージ",
    "href": "chapter01.html#rのパッケージ",
    "title": "1  はじめようR/RStudio",
    "section": "1.3 Rのパッケージ",
    "text": "1.3 Rのパッケージ\nRは単体でも線型モデルなどの基本的な分析は可能であるが，より進んだ統計モデルを利用したい場合は専門のパッケージを導入することになる。パッケージとは関数群のことであり，これもCRANやGithubなどインターネットを介して提供されている。ちなみに提供されているパッケージは，CRANで公開されているものだけで344,607件あり5，Github6で公開されているものなど，CRANを介さないパッケージも少なくない。\nパッケージを利用する際は，まずローカルにパッケージファイルをインストールしなければならない。その上で，Rを起動するごとに(セッションごとに)，関数libraryでパッケージを呼び出して利用する。インストールを毎回行う必要はないことに注意。\nインストールはRのコマンドでも可能だが，RStudioのPackagesペインを使って導入するのが簡単だろう。以下に，一部の有名かつ有用なパッケージ名とその簡単な説明を挙げる。本講義の中で使うものもあるので，事前に準備しておくことが望ましい。\n\ntidyverseパッケージ(Wickham et al. 2019)；Rが飛躍的に使いやすくなったのは，このtidyverseパッケージ導入以後のことである。開発者のHadley WickhamはR業界で神と崇められており，R業界に与えたインパクトは大きい。このパッケージは「パッケージ群」「パッケージのパッケージ」であり，tidyverseとはtidyな(整然とした)verse(世界)というような意味合いである。このパッケージは統計分析モデルを提供するものではなく，その前のデータの前処理に関する便利な関数を提供する7。このパッケージをインストールすると，関連する依存パッケージが次々取り込まれるので，少々時間がかかる。\npsychパッケージ(Revelle 2021)；名前の通り，心理学統計に関する統計モデルの多くが含まれている。特に特殊な相関係数や，因子分析モデルなどは非常に便利なので，インストールしておいて間違いない。\nGPArotationパッケージ(Bernaards and Jennrich 2005)；因子分析における因子軸の回転に使うパッケージ。\nstylerパッケージ；スタイルを整えてくれるパッケージ。スクリプトの清書に便利。\nlavaanパッケージ(Rosseel 2012)；潜在変数を含んだモデル(LAtent VAriable ANalysis)の分析，要するに構造方程式モデリング(Structural Equation Modeling;SEM，共分散構造分析ともいう)を実行するパッケージ。\nctvパッケージ(Zeileis 2005); CRAN Task Viewsの略で，膨大に膨れ上がったCRANから必要なパッケージを見つけ出すのは困難であることから，ある程度のジャンルごとに関連しそうなパッケージをまとめて導入してくれるのがこのパッケージ。例えば，このパッケージをインストールした後で，install.views(\"Psychometrics\")とすると，心理統計関係の多くのパッケージを次々導入してくれる。\ncmdstanrパッケージ(Gabry, Češnovar, and Johnson 2023)；複雑な統計モデルで利用される，確率的プログラミング言語stanをRから使うことができるようになるパッケージ。導入にはこのパッケージの他にもstanやコンパイル環境の準備が必要なので，公式の導入サイトも参考にしてほしい。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#rstudioのプロジェクト",
    "href": "chapter01.html#rstudioのプロジェクト",
    "title": "1  はじめようR/RStudio",
    "section": "1.4 RStudioのプロジェクト",
    "text": "1.4 RStudioのプロジェクト\n実際にRを使っていく前に，最後の準備としてRStudioにおけるプロジェクトについて解説しておく。\nみなさんも，PCをつかって文書を作ったり保管したりするときに，フォルダにまとめて入れておくことがあるだろう。フォルダは例えば「文書」&gt;「心理学」&gt;「心理学統計演習」のように階層的に整理することが一般的で，そうしておくことで必要なファイルをすぐに取り出すことができる。\n逆に言えば，こうしたフォルダ管理をしておかなければファイルがPCのなかで散乱してしまい，必要な情報を得るために逐一PCの中身を検索しなければならないだろう。\nR/RStudioをつかった分析実践の場合も同様で，一回のテーマについて複数のファイル(スクリプトファイル，データファイル，画像ファイル，レポートなど文書ファイル等々)があり，シーンに合わせて(例えば「授業」「卒論」など)フォルダで管理することになる。\nさらに，PC環境には作業フォルダ(Working Directory)8という概念がある。たとえばR/RStudioを起動・実行しているときに，Rが「今どこで」実行されているか，どこを管理場所としているか，を表す概念である。例えばこの作業フォルダの中にsample.csvというファイルがあって，それをスクリプト上から読み込みたい，というコマンドを実行するのであれば，そのままファイル名を書けば良い。しかし別の場所にそのファイルが保存されているのなら，作業フォルダから見た相対的な位置を含めて指示してやるか(相対パス)，あるいはPC環境全体からみた絶対的な位置を含めて(絶対パス)指示してやる必要がある。相対・絶対パスの違いは，「ここから二つ目の角を右」のように指示するか，住所で指示するかの違いであると考えれば良い。\nともあれ，この作業フォルダがどこに設定されているかは，実行するときに常に気にしていなければならない。ちなみにこの作業フォルダは，RStudioのファイルペイン・Filesタブでひらいているところとは限らないことに注意してほしい。GUI上でエクスプローラ/Finderで開いたからといって，作業フォルダが自動的に切り替わるようにはなっていない。\nそこでRStudioのプロジェクトである。RStudioには「プロジェクト」という概念があり，作業フォルダや環境の設定などをそこで管理することができる。新しくプロジェクトを始めるときはFile&gt;New Project，すでに一度プロジェクトを作っているときはFile &gt; Open Projectとしてプロジェクトファイル(拡張子が.projのファイル)を開くようにする。そうすると，作業フォルダが当該フォルダに設定される。プロジェクトをGitに連携しておくとバージョン管理などもフォルダ単位で行える。\n以後，本講義で外部ファイルを参照する場合，プロジェクトフォルダの中にそのファイルがあるものとして(パスを必要としない形で)論じるので注意されたし。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#課題",
    "href": "chapter01.html#課題",
    "title": "1  はじめようR/RStudio",
    "section": "1.5 課題",
    "text": "1.5 課題\n\nRの最新版をCRANからダウンロードし，自分のPCにインストールしてください。\nRStudioのDesktop版をPosit社のサイトからダウンロードし，自分のPCにインストールしてください。\nRStidoを起動し，ペインレイアウトをデフォルトではない状態に並べ直してみてください。ソースペインを3列にするのも良いでしょう。\nコンソールペインに書かれている文字を全て消去してみてください。\nファイルペインにあるFilesタブをつかって，色々なフォルダを開けてみたり，不要なファイルを削除したり，ファイル名を変更したりしてみてください。\nファイルペインにあるFilesタブを開き，MoreのところからGo To Working Directoryを選択・実行してください。何か起こったでしょうか。\nこの授業のために，新しいプロジェクトを作成してください。プロジェクトは新しいフォルダでも，既存のフォルダでも構いません。\nプロジェクトが開いた状態のとき，RStudioのウィンドウ・タブのどこかに「プロジェクト名」が表示されているはずです。確認してください。\nまたファイルペインのFilesタブから，色々なファイル操作をした上で，改めてGo To Working Directoryをしてください。プロジェクトフォルダの中に戻ってこれたら成功です。\n新しいRスクリプトファイルを開き，空白のままで結構ですからファイル名をつけて保存してください。\nRStudioを終了あるいは最小化させ，OSのエクスプローラ/Finderから，プロジェクトフォルダに移動してください。先ほど作ったファイルが保存されていることを確認してください。\nプロジェクトフォルダには，プロジェクト名+.projというファイルが存在するはずです。これを開いて，RStudioのプロジェクトを開いてください。\nRStudioのFile &gt; Close Projectからプロジェクトを閉じてください。画面の細部でどこが変わったか，確認してください。\nRStudioを終了し，再びRStudioを起動してください。起動の方法はプロジェクトファイルからでも，アプリケーションの起動でも構いません。起動後に，プロジェクトを開いてください(あるいはプロジェクトが開かれていることを確認してください。)。\n\n\n\n\n\nBernaards, Coen A., and Robert I. Jennrich. 2005. “Gradient Projection Algorithms and Software for Arbitrary Rotation Criteria in Factor Analysis.” Educational and Psychological Measurement 65: 676–96. https://doi.org/10.1177/0013164404272507.\n\n\nGabry, Jonah, Rok Češnovar, and Andrew Johnson. 2023. Cmdstanr: R Interface to ’CmdStan’.\n\n\nRevelle, William. 2021. Psych: Procedures for Psychological, Psychometric, and Personality Research. Evanston, Illinois: Northwestern University. https://CRAN.R-project.org/package=psych.\n\n\nRosseel, Yves. 2012. “lavaan: An R Package for Structural Equation Modeling.” Journal of Statistical Software 48 (2): 1–36. https://doi.org/10.18637/jss.v048.i02.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nZeileis, Achim. 2005. “CRAN Task Views.” R News 5 (1): 39–40. https://CRAN.R-project.org/doc/Rnews/.\n\n\n松村優哉, 湯谷啓明, 紀ノ定保礼, and 前田和寛. 2021. 改訂2版 RユーザのためのRStudio[実践]入門: Tidyverseによるモダンな分析フローの世界. 技術評論社.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#footnotes",
    "href": "chapter01.html#footnotes",
    "title": "1  はじめようR/RStudio",
    "section": "",
    "text": "2024年1月現在で，TokyoだけでなくFukuoka，Sapporo，Yamaguchi，Irumaなどで地方コミュニティがあり，参加者みんなで楽しまれている。↩︎\nCRANは「しーらん」，あるいは「くらん」と発音される。筆者はしーらん派。↩︎\nこの授業のために自身のPCにRをインストールしたとして，次に使うときに半年以上間隔が空いたのなら，改めて最新版をチェックし，バージョンが上がっていたら旧版をアンインストールして最新版をインストールするところから始めた方が良い。Rで利用するパッケージなどが新しい版にしか対応していないことなどもある。Rと畳は新しい方が良い。↩︎\nVSCodeのようなエディタから使うことも可能であるし，Jupyter Notebookの計算エンジンをRにすることも可能。最近では分析ソフトウェアを個々人で準備せず，環境として提供することも一般的になってきており，例えばGoogle ColaboratoryのエンジンをRにすることもできるようになっている。ローカルPCに自前の環境を作るということが，時代遅れになる日も近いかもしれない。↩︎\n2024年01月18日調べ↩︎\nGitはバージョン管理システムであるが，これをインターネット上のサーバ(レポジトリ)で行うものをGithubという。RStudioはGithubとも連携しており，プロジェクトをGithubと紐づけることで簡単にバージョン管理ができる。しかもここで言及しているように，Github上でパッケージを公開することもできるので，最近はCRANの校閲を待たずに公開できるGithubが好まれている側面もある。↩︎\n実は統計データの解析にかかる時間のほとんどが，解析に適切な形にデータを整形する「前処理」に費やされる。前処理，別名データハンドリングをいかに上手く，素早く，直感的にできるかは，その後の分析にも影響するほど重要な手順であるため，tidyverseパッケージの登場はありがたかった。これを使ったデータハンドリングだけの専門書 松村 et al. (2021) が重宝されるほどである。↩︎\nここでは，フォルダとディレクトリは同じ意味であると思ってもらって良い。一般に，CUIではディレクトリ，GUIではフォルダという用語が好まれる。語幹directにあるように，ファイルやアクセス先など具体的な指し示す先を強調しているのがディレクトリであり，それにファイル群などまとまった容れもの，という意味を付加したのがフォルダである。フォルダの方が言葉としてわかりやすいし。↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter02.html",
    "href": "chapter02.html",
    "title": "2  Rの基礎",
    "section": "",
    "text": "2.1 Rで計算\nまずはRを使った計算である。Rスクリプトファイルを開き，最初の行に次の4行を入力してみよう。 各行を実行(Runボタン，あるいはctrl+enter)し，コンソールの結果を確認しよう。\n1 + 2\n\n[1] 3\n\n2 - 3\n\n[1] -1\n\n3 * 4\n\n[1] 12\n\n6 / 3\n\n[1] 2\nそれぞれ加減乗除の計算結果が正しく出ていることを確認してほしい。なお，出力のところに[1]とあるのは，Rがベクトルを演算の基本としているからで，回答ベクトルの第1要素を返していることを意味する。\n四則演算の他に，次のような演算も可能である。\n# 整数の割り算\n8 %/% 3\n\n[1] 2\n\n# 余り\n7 %% 3\n\n[1] 1\n\n# 冪乗\n2^3\n\n[1] 8\nここで，#から始まる行はコメントアウトされたものとして，実際にコンソールに送られても計算されないことに注意しよう。スクリプトが単純なものである場合はコメントをつける必要はないが，複雑な計算になったり，他者と共有するときは「今どのような演算をしているか」を逐一解説するようにすると便利である。\n実践上のテクニックとして，複数行を一括でコメントアウトしたり，アンコメント(コメントアウトを解除する)したりすることがある。スクリプトを複数行選択した上で，CodeメニューからComment/Uncomment Linesを押すとコメント/アンコメントを切り替えられるので試してみよう。また，ショートカットキーも確認し，キーからコメント/アンコメントができるように慣れておくと良い(Ctrl+↑+C/Cmd+↑+C)。\nOne more tips.コメントではなく，大きな段落的な区切り(セクション区切り)が欲しいこともあるかもしれない。Codeメニューの一番上に「Insert Section」があるのでこれを選んでみよう。ショートカットキーから入力しても良い(Ctrl+↑+R/Cmd+↑+R)。セクション名を入力するボックスに適当な命名をすると，スクリプトにセクションが挿入される。次に示すのがセクションの例である。\n# 計算 --------------------------------------------------------------\nこれはもちろん実行に影響を与えないが，ソースが長くなった場合はこのセクション単位で移動したり(スクリプトペインの左下)，アウトラインを確認したり(スクリプトペインの右上にある横三本線)できるので，活用して欲しい。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rの基礎</span>"
    ]
  },
  {
    "objectID": "chapter02.html#オブジェクト",
    "href": "chapter02.html#オブジェクト",
    "title": "2  Rの基礎",
    "section": "2.2 オブジェクト",
    "text": "2.2 オブジェクト\nRでは変数，関数などあらゆるものをオブジェクトとしてあつかう。オブジェクトには任意の名前をつけることができる(数字から始まる名前は不可)。 オブジェクトを作り，そこにある値を代入する例は次の通りである。\n\na &lt;- 1\nb &lt;- 2\nA &lt;- 3\na + b # 1 + 2におなじ\n\n[1] 3\n\nA + b # 3 + 2におなじ\n\n[1] 5\n\n\nここでは数字をオブジェクトに保管し，オブジェクトを使って計算をしている。大文字と小文字が区別されてるため，計算結果が異なることに注意。\n代入に使った記号&lt;-は「小なり」と「ハイフン」であるが，左矢印のイメージである。次のように，=や-&gt;を使うこともできる。\n\nB &lt;- 5\n7 -&gt; A\n\nここで，二行目に7 -&gt; Aを行った。先ほどA &lt;- 3としたが，その後にAには7を代入し直したので，値は上書きされる。\n\nA + b # 7 + 2におなじ\n\n[1] 9\n\n\nこのように，オブジェクトに代入を重ねると，警告などなしに上書きされることに注意して欲しい。似たようなオブジェクト名を使い回していると，本来意図していたものと違う値・状態を保管していることになりかねないからである。\nちなみに，オブジェクトの中身を確認するためには，そのままオブジェクト名を入力すれば良い。より丁寧には，print関数を使う。\n\na\n\n[1] 1\n\nprint(A)\n\n[1] 7\n\n\nあるいは，RStudioのEnvironmentタブをみると，現在Rが保持しているオブジェクトが確認でき，単一の値の場合はValueセクションにオブジェクト名と値を見ることができる。\n注意点として，オブジェクト名として，次の名前は使うことができない。＞ break, else, for, if, in, next, function, repeat, return, while, TRUE, FALSE.\nこれらはRで特別な意味を持つ予約語と呼ぶ。特にTRUEとFALSEは真・偽を表すもので，大文字のT,Fでも代用できるため，この一文字だけをオブジェクト名にするのは避けた方が良い。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rの基礎</span>"
    ]
  },
  {
    "objectID": "chapter02.html#関数",
    "href": "chapter02.html#関数",
    "title": "2  Rの基礎",
    "section": "2.3 関数",
    "text": "2.3 関数\n関数は一般に\\(y=f(x)\\)と表されるが，要するに\\(x\\)を与えると\\(y\\)に形が変わる作用のことを指す。 プログラミング言語では一般に，\\(x\\)を引数(ひきすう,argument)，\\(y\\)を戻り値(もどりち,value)という。以下，関数の使用例を挙げる。\n\nsqrt(16)\n\n[1] 4\n\nhelp(\"sqrt\")\n\n最初の例は平方根square rootを取る関数sqrtであり，引数として数字を与えるとその平方根が返される。第二の例は関数の説明を表示させる関数helpであり，これを実行するとヘルプペインに関数の説明が表示される。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rの基礎</span>"
    ]
  },
  {
    "objectID": "chapter02.html#変数の種類",
    "href": "chapter02.html#変数の種類",
    "title": "2  Rの基礎",
    "section": "2.4 変数の種類",
    "text": "2.4 変数の種類\n先ほどのhelp関数に与えた引数\"sqrt\"は文字列である。文字列であることを明示するためにダブルクォーテーション(\")で囲っている(シングルクォーテーションで囲っても良い)。このように，Rが扱う変数は数字だけではない。変数の種類は数値型(numeric)，文字型(character)，論理値(logical)の3種類がある。\n\nobj1 &lt;- 1.5\nobj2 &lt;- \"Hello\"\nobj3 &lt;- TRUE\n\n数値型は整数(integer)，実数(double)を含み1，そのほか，複素数型(complex)，欠損値を表すNA，非数値を表すNaN(Not a Number)，無限大を表すInfなどがある。\n文字型はすでに説明した通りで，対になるクォーテーションが必要であることに注意してほしい。終わりを表すクォーテーションがなければ，Rは続く数字や文字も含めた「語」として処理する。この場合，enterキーを押しても文字入力が閉じられていないため，コンソールには「+」の表示が出る(この記号は前の行から入力が続いており，プロンプト状態ではないことを表している)。\nまた，文字型は当然のことながら四則演算の対象にならない。ただし，論理型のTRUE/FALSEはそれぞれ1,0に対応しているため，計算結果が表示される。次のコードを実行してこのことを確認しよう。\n\nobj1 + obj2\nobj1 + obj3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rの基礎</span>"
    ]
  },
  {
    "objectID": "chapter02.html#オブジェクトの型",
    "href": "chapter02.html#オブジェクトの型",
    "title": "2  Rの基礎",
    "section": "2.5 オブジェクトの型",
    "text": "2.5 オブジェクトの型\nここまでみてきたように，数値や文字など(まとめてリテラルという)にも種類があるが，これをストックしておくものは全てオブジェクトである。オブジェクトとは変数のこと，と理解しても良いが，関数もオブジェクトに含まれる。\n\n2.5.1 ベクトル\nRのオブジェクトは単一の値しか持たないものではない。むしろ，複数の要素をセットで持つことができるのが特徴である。次に示すのは，ベクトルオブジェクトの例である。\n\nvec1 &lt;- c(2, 4, 5)\nvec2 &lt;- 1:3\nvec3 &lt;- 7:5\nvec4 &lt;- seq(from = 1, to = 7, by = 2)\nvec5 &lt;- c(vec2, vec3)\n\nそれぞれのオブジェクトの中身を確認しよう。 最初のc()は結合combine関数である。また，コロン(:)は連続する数値を与える。 seq関数は複数の引数を取るが，初期値，終了値，その間隔を指定した連続的なベクトルを生成する関数である。\nベクトルの計算は要素ごとに行われる。次のコードを実行し，どのように振る舞うか確認しよう。\n\nvec1 + vec2\n\n[1] 3 6 8\n\nvec3 * 2\n\n[1] 14 12 10\n\nvec1 + vec5\n\n[1]  3  6  8  9 10 10\n\n\n最後の計算でエラーが出なかったことに注目しよう。たとえばvec1 + vec4はエラーになるが，ここでは計算結果が示されている(=エラーにはなっていない)。数学的には，長さの違うベクトルは計算が定義されていないのだが，vec1の長さは3，vec5の長さは6であった。Rはベクトルを再利用するので，長いベクトルが短いベクトルの定数倍になるときは反復して利用される。すなわち，ここでは \\[ (2,4,5,2,4,5) + (1,2,3,7,6,5) = (3,6,8,9,10,10)\\] の計算がなされた。このRの仕様については，意図せぬ挙動にならぬよう注意しよう。\nベクトルの要素にアクセスするときは大括弧([ ])を利用する。 特に第二・第三行目のコードの使い方を確認しておこう。大括弧の中は，要素番号でも良いし，真/偽の判断でも良いのである。この真偽判断による指定の方法は，条件節(if文)をつかって要素を指定できるため，有用である。\n\nvec1[2]\n\n[1] 4\n\nvec2[c(1, 3)]\n\n[1] 1 3\n\nvec2[c(TRUE, FALSE, TRUE)]\n\n[1] 1 3\n\n\nここまで，ベクトルの要素は数値で説明してきたが，文字列などもベクトルとして利用できる。\n\nwords1 &lt;- c(\"Hello!\", \"Mr.\", \"Monkey\", \"Magic\", \"Orchestra\")\nwords1[3]\n\n[1] \"Monkey\"\n\nwords2 &lt;- LETTERS[1:10]\nwords2[8]\n\n[1] \"H\"\n\n\nここでLETTERSはアルファベット26文字が含まれている予約語ベクトルである。\nベクトルを引数に取る関数も多い。たとえば記述統計量である，平均，分散，標準偏差，合計などは，次のようにして計算する。\n\ndat &lt;- c(12, 18, 23, 35, 22)\nmean(dat) # 平均\n\n[1] 22\n\nvar(dat) # 分散\n\n[1] 71.5\n\nsd(dat) # 標準偏差\n\n[1] 8.455767\n\nsum(dat) # 合計\n\n[1] 110\n\n\n他にも最大値maxや最小値min，中央値medianなどの関数が利用可能である。\n\n\n2.5.2 行列\n数学では線形代数でベクトルを扱うが，同時にベクトルが複数並んだ二次元の行列も扱うだろう。 Rでも行列のように配置したオブジェクトを利用できる。\n次のコードで作られる行列\\(A\\),\\(B\\)がどのようなものか確認しよう。\n\nA &lt;- matrix(1:6, ncol = 2)\nB &lt;- matrix(1:6, ncol = 2, byrow = T)\n\n行列を作る関数matrixは，引数として要素，列数(ncol)，行数(nrow)，要素配列を行ごとにするかどうかの指定(byrow)をとる。ここでは要素を1:6としており，1から6までの連続する整数をあたえている。ncolで2列であることを明示しているので，nrowで行数を指定してやる必要はない。byrowの有無でどのように数字が変わっているかは表示させれば一目瞭然であろう。\n与える要素が行数\\(\\times\\)列数に一致しておらず，ベクトルの再利用も不可能な場合はエラーが返ってくる。\nまた，ベクトルの要素指定のように，行列も大括弧を使って要素を指定することができる。行，列の順に指定し，行だけ，列だけの指定も可能である。\n\nA[2, 2]\n\n[1] 5\n\nA[1, ]\n\n[1] 1 4\n\nA[, 2]\n\n[1] 4 5 6\n\n\n\n\n2.5.3 リスト型\n行列はサイズの等しいベクトルのセットであるが，サイズの異なる要素をまとめて一つのオブジェクトとして保管しておきたいときはリスト型をつかう。\n\nObj1 &lt;- list(1:4, matrix(1:6, ncol = 2), 3)\n\nこのオブジェクトの第一要素([[1]])はベクトル，第二要素は行列，第三要素は要素1つのベクトル(スカラー)である。オブジェクトの要素の要素(ex.第二要素の行列の2行3列目の要素)にどのようにアクセスすれば良いか，考えてみよう。\nこのリストは要素へのアクセスの際に[[1]]など数字が必要だが，要素に名前をつけることで利便性が増す。\n\nObj2 &lt;- list(\n  vec1 = 1:5,\n  mat1 = matrix(1:10, nrow = 5),\n  char1 = \"YMO\"\n)\n\nこの名前付きリストの要素にアクセスするときは，$記号を用いることができる。\n\nObj2$vec1\n\n[1] 1 2 3 4 5\n\n\nこれを踏まえて，名前付きリストの要素の要素にアクセスするにはどうすれば良いか，考えてみよう。\nリスト型はこのように，要素のサイズ・長さを問わないため，いろいろなものを保管しておくことができる。統計関数の結果はリスト型で得られることが多く，そのような場合，リストの要素も長くなりがちである。リストがどのような構造を持っているかを見るために，str関数が利用できる。\n\nstr(Obj2)\n\nList of 3\n $ vec1 : int [1:5] 1 2 3 4 5\n $ mat1 : int [1:5, 1:2] 1 2 3 4 5 6 7 8 9 10\n $ char1: chr \"YMO\"\n\n\nstr関数の返す結果と同じものが，RStudioのEnvironmentタブからオブジェクトを見ることでも得られる。 また，リストの要素としてリストを持つ，すなわち階層的になることもある。そのような場合，必要としている要素にどのようにアクセスすれば良いか，確認しておこう。\n\nObj3 &lt;- list(Obj1, Second = Obj2)\nstr(Obj3)\n\nList of 2\n $       :List of 3\n  ..$ : int [1:4] 1 2 3 4\n  ..$ : int [1:3, 1:2] 1 2 3 4 5 6\n  ..$ : num 3\n $ Second:List of 3\n  ..$ vec1 : int [1:5] 1 2 3 4 5\n  ..$ mat1 : int [1:5, 1:2] 1 2 3 4 5 6 7 8 9 10\n  ..$ char1: chr \"YMO\"\n\n\n\n\n2.5.4 データフレーム型\nリスト型は要素のサイズを問わないことはすでに述べた。しかしデータ解析を行うときは得てして，2次元スプレッドシートのような形式である。すなわち一行に1オブザベーション，各列は変数を表すといった具合である。このように矩形かつ，列に変数名を持たせることができる特殊なリスト型をデータフレーム型という。以下はそのようなオブジェクトの例である。\n\ndf &lt;- data.frame(\n  name = c(\"Ishino\", \"Pierre\", \"Marin\"),\n  origin = c(\"Shizuoka\", \"Shizuoka\", \"Hokkaido\"),\n  height = c(170, 180, 160),\n  salary = c(1000, 20, 800)\n)\n# 内容を表示させる\ndf\n\n    name   origin height salary\n1 Ishino Shizuoka    170   1000\n2 Pierre Shizuoka    180     20\n3  Marin Hokkaido    160    800\n\n# 構造を確認する\nstr(df)\n\n'data.frame':   3 obs. of  4 variables:\n $ name  : chr  \"Ishino\" \"Pierre\" \"Marin\"\n $ origin: chr  \"Shizuoka\" \"Shizuoka\" \"Hokkaido\"\n $ height: num  170 180 160\n $ salary: num  1000 20 800\n\n\nところで，心理統計の初歩としてStevensの尺度水準(Stevens 1946)について学んだことと思う。そこでは数値が，その値に許される演算のレベルをもとに，名義，順序，間隔，比率尺度水準という4つの段階に分類される。間隔・比率尺度水準の数値は数学的な計算を施しても良いが，順序尺度水準や名義尺度水準の数字はそのような計算が許されない(ex.2番目に好きな人と3番目に好きな人が一緒になっても，1番好きな人に敵わない。)\nRには，こうした尺度水準に対応した数値型がある。間隔・比率尺度水準は計算可能なのでnumeric型でよいが，名義尺度水準はfactor型(要因型，因子型とも呼ばれる)，順序尺度水準はordered.factor型と呼ばれるものである。\nfactor型の変数の例を挙げる。すでに文字型として入っているものをfactor型として扱うよう変換するためには，as.factor関数が利用できる。\n\ndf$origin &lt;- as.factor(df$origin)\ndf$origin\n\n[1] Shizuoka Shizuoka Hokkaido\nLevels: Hokkaido Shizuoka\n\n\n要素を表示させて見ると明らかなように，値としてはShizuoka,Shizuoka,Hokkaidoの3つあるが，レベル(水準)はShizuoka,Hokkaidoの2つである。このようにfactor型にしておくと，カテゴリとして使えて便利である。\n次に示すのは順序つきfactor型変数の例である。\n\n# 順序付き要因型の例\nratings &lt;- factor(c(\"低い\", \"高い\", \"中程度\", \"高い\", \"低い\"),\n  levels = c(\"低い\", \"中程度\", \"高い\"),\n  ordered = TRUE\n)\n# ratingsの内容と型を確認\nprint(ratings)\n\n[1] 低い   高い   中程度 高い   低い  \nLevels: 低い &lt; 中程度 &lt; 高い\n\n\n集計の際などはfactor型と違わないため，使用例は少ないかもしれない。しかしRは統計モデルを適用する時に，尺度水準に対応した振る舞いをするものがあるので，データの尺度水準を丁寧に設定しておくのも良いだろう。\nデータフレームの要素へのアクセスは，基本的に変数名を介してのものになるだろう。たとえば先ほどのおオブジェクトdf の数値変数に統計処理をしたい場合は，次のようにすると良い。\n\nmean(df$height)\n\n[1] 170\n\nsum(df$salary)\n\n[1] 1820\n\n\nまた，データフレームオブジェクトを一括で要約する関数もある。\n\nsummary(df)\n\n     name                origin      height        salary      \n Length:3           Hokkaido:1   Min.   :160   Min.   :  20.0  \n Class :character   Shizuoka:2   1st Qu.:165   1st Qu.: 410.0  \n Mode  :character                Median :170   Median : 800.0  \n                                 Mean   :170   Mean   : 606.7  \n                                 3rd Qu.:175   3rd Qu.: 900.0  \n                                 Max.   :180   Max.   :1000.0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rの基礎</span>"
    ]
  },
  {
    "objectID": "chapter02.html#外部ファイルの読み込み",
    "href": "chapter02.html#外部ファイルの読み込み",
    "title": "2  Rの基礎",
    "section": "2.6 外部ファイルの読み込み",
    "text": "2.6 外部ファイルの読み込み\n解析の実際では，データセットを手入力することはなく，データベースから取り出してくるか，別ファイルから読み込むことが一般的であろう。\n統計パッケージの多くは独自のファイル形式を持っており，Rにはそれぞれに対応した読み込み関数も用意されているが，ここでは最もプレーンな形でのデータであるCSV形式からの読み込み例を示す。\n提供されたサンプルデータ，Baseball.csvを読み込むことを考える。なおこのデータはUTF-8形式で保存されている2。これを読み込むには，Rがデフォルトで持っている関数read.csvが使える。\n\ndat &lt;- read.csv(\"Baseball.csv\")\nhead(dat)\n\n      Year       Name team salary bloodType height weight UniformNum position\n1 2011年度 永川　勝浩 Carp  12000       O型    188     97         20     投手\n2 2011年度 前田　健太 Carp  12000       A型    182     73         18     投手\n3 2011年度 栗原　健太 Carp  12000       O型    183     95          5   内野手\n4 2011年度 東出　輝裕 Carp  10000       A型    171     73          2   内野手\n5 2011年度   シュルツ Carp   9000      不明    201    100         70     投手\n6 2011年度   大竹　寛 Carp   8000       B型    183     90         17     投手\n  Games AtBats Hit HR Win Lose Save Hold\n1    19     NA  NA NA   1    2    0    0\n2    31     NA  NA NA  10   12    0    0\n3   144    536 157 17  NA   NA   NA   NA\n4   137    543 151  0  NA   NA   NA   NA\n5    19     NA  NA NA   0    0    0    9\n6     6     NA  NA NA   1    1    0    0\n\nstr(dat)\n\n'data.frame':   7944 obs. of  17 variables:\n $ Year      : chr  \"2011年度\" \"2011年度\" \"2011年度\" \"2011年度\" ...\n $ Name      : chr  \"永川　勝浩\" \"前田　健太\" \"栗原　健太\" \"東出　輝裕\" ...\n $ team      : chr  \"Carp\" \"Carp\" \"Carp\" \"Carp\" ...\n $ salary    : int  12000 12000 12000 10000 9000 8000 8000 7500 7000 6600 ...\n $ bloodType : chr  \"O型\" \"A型\" \"O型\" \"A型\" ...\n $ height    : int  188 182 183 171 201 183 177 173 176 188 ...\n $ weight    : int  97 73 95 73 100 90 82 73 80 97 ...\n $ UniformNum: int  20 18 5 2 70 17 31 6 1 43 ...\n $ position  : chr  \"投手\" \"投手\" \"内野手\" \"内野手\" ...\n $ Games     : int  19 31 144 137 19 6 110 52 52 40 ...\n $ AtBats    : int  NA NA 536 543 NA NA 299 192 44 149 ...\n $ Hit       : int  NA NA 157 151 NA NA 60 41 11 35 ...\n $ HR        : int  NA NA 17 0 NA NA 4 2 0 1 ...\n $ Win       : int  1 10 NA NA 0 1 NA NA NA NA ...\n $ Lose      : int  2 12 NA NA 0 1 NA NA NA NA ...\n $ Save      : int  0 0 NA NA 0 0 NA NA NA NA ...\n $ Hold      : int  0 0 NA NA 9 0 NA NA NA NA ...\n\n\nここでhead関数はデータフレームなどオブジェクトの冒頭部分(デフォルトでは6行分)を表示させるものである。また，str関数の結果から明らかなように，読み込んだファイルが自動的にデータフレーム型になっている。\nちなみに，サンプルデータにおいて欠損値に該当する箇所にはNAの文字が入っていた。read.csv関数では，欠損値はデフォルトで文字列”NA”としている。しかし，実際は別の文字(ex.ピリオド)や，特定の値(ex.9999)の場合もあるだろう。その際は，オプションna.stringsで「欠損値として扱う値」を指示すれば良い。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rの基礎</span>"
    ]
  },
  {
    "objectID": "chapter02.html#おまけスクリプトの清書",
    "href": "chapter02.html#おまけスクリプトの清書",
    "title": "2  Rの基礎",
    "section": "2.7 おまけ；スクリプトの清書",
    "text": "2.7 おまけ；スクリプトの清書\nさて，ここまでスクリプトを書いてきたことで，そこそこ長いスクリプトファイルができたことと思う。 スクリプトの記述については，もちろん「動けばいい」という考え方もあるが，美しくかけていたほうがなお良いだろう。「美しい」をどのように定義するかは異論あるだろうが，一般に「コード規約」と呼ばれる清書方法がある。ここでは細部まで言及しないが，RStudioのCodeメニューからReformat Codeを実行してみよう。スクリプトファイルが綺麗に整ったように見えないだろうか?\n美しいコードはデバッグにも役立つ。時折Reformatすることを心がけよう。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rの基礎</span>"
    ]
  },
  {
    "objectID": "chapter02.html#課題",
    "href": "chapter02.html#課題",
    "title": "2  Rの基礎",
    "section": "2.8 課題",
    "text": "2.8 課題\n\nRを起動し，新しいスクリプトファイルを作成してください。そのファイル内で，2つの整数を宣言し，足し算を行い，結果をコンソールに表示してください。\nスクリプトに次の計算を書き，実行してください。\n\n\\(\\frac{5}{6} + \\frac{1}{3}\\)\n\\(9.6 \\div 4\\)\n\\(2.3 + \\frac{1}{2}\\)\n\\(3\\times (2.2 + \\frac{4}{5})\\)\n\\((-2)^4\\)\n\\(2\\sqrt{2} \\times \\sqrt{3}\\)\n\\(2\\log_e 25\\)\n\nRのスクリプトファイル内で，ベクトルを作成してください。ベクトルには1から10までの整数を格納してください。その後，ベクトルの要素の合計と平均を計算してください。ベクトルを合計する関数はsum，平均はmeanです。\n次の表をリスト型オブジェクトTblにしてください。\n\n\n\n\nName\nPop\nArea\nDensity\n\n\n\n\nTokyo\n1,403\n2,194\n6,397\n\n\nBeijing\n2,170\n16,410\n1,323\n\n\nSeoul\n949\n605\n15,688\n\n\n\n\n先ほど作ったTblオブジェクトの，東京(Tokyo)の面積(Area)の値を表示させてください(リスト要素へのアクセス)\nTblオブジェクトの人口(Pop)変数の平均を計算してください。\nTblオブジェクトをデータフレーム型オブジェクトdf2に変換してください。新たに作り直しても良いですし，as.data.frame関数を使っても良い。\nRのスクリプトを使用して，Baseball2022.csv ファイルを読み込み，データフレームdatに格納してください。ただし，このファイルの欠損値は\\(999\\)という数値になっています。\n読み込んだdatの冒頭の10行を表示してみてください。\n読み込んだdatにsummary関数を適用してください。\nこのデータセットの変数teamは名義尺度水準です。Factor型にしてください。他にもFactor型にすべき変数が2つありますので，それらも同様に型を変換してください。\nこのデータセットの変数の中で，数値データに対して平均，分散，標準偏差，最大値，最小値，中央値を それぞれ算出してください。\n課題を記述したスクリプトファイルに対して，Reformatなどで整形してください。\n\n\n\n\n\nStevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rの基礎</span>"
    ]
  },
  {
    "objectID": "chapter02.html#footnotes",
    "href": "chapter02.html#footnotes",
    "title": "2  Rの基礎",
    "section": "",
    "text": "実数はreal numberじゃないのか，という指摘もあろうかとおもう。ここでは電子計算機上の数値の分類である，倍精度浮動小数点数(double-precision floating-point number)の意味である。倍精度とは単精度の倍を意味しており，単精度は32ビットを，倍精度は64ビットを単位として一つの数字を表す仕組みのことである。↩︎\nUTF-8というのは文字コードの一種で，0と1からなる機械のデータを人間語に翻訳するためのコードであり，世界的にもっとも一般的な文字コードである。しかしWindowsOSはいまだにデフォルトでShift-JISというローカルな文字コードにしているため，このファイルを一度Windows機のExcelなどで開くと文字化けし，以下の手続が正常に作用しなくなることがよくある。本講義で使う場合は，ダウンロード後にExcelなどで開くことなく，直接Rから読み込むようにされたし。↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rの基礎</span>"
    ]
  },
  {
    "objectID": "chapter03.html",
    "href": "chapter03.html",
    "title": "3  Rによるデータハンドリング",
    "section": "",
    "text": "3.1 tidyverseの導入\n本講義ではtidyverseをつかったデータハンドリングを扱う。tidyverseは，データに対する統一的な設計方針を表す概念でもあり，具体的にはそれを実装したパッケージ名でもある。まずはtidyverseパッケージをインストール(ダウンロード)し，次のコードでRに読み込んでおく。\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nAttaching core tidyverse packages,と表示され，複数のパッケージ名にチェックマークが入っていたものが表示されただろう。tidyverseパッケージはこれらの下位パッケージを含むパッケージ群である。これに含まれるdplyr,tidyrパッケージはデータの整形に，readrはファイルの読み込みに，forecatsはFactor型変数の操作に，stringrは文字型変数の操作に，lubridateは日付型変数の操作に，tibbleはデータフレーム型オブジェクトの操作に，purrrはデータに適用する関数に，ggplot2は可視化に特化したパッケージである。\n続いてConflictsについての言及がある。tidyverseパッケージに限らず，パッケージを読み込むと表示されることのあるこの警告は，「関数名の衝突」を意味している。ここまで，Rを起動するだけで，sqrt,meanなどの関数が利用できた。これはRの基本関数であるが，具体的にはbaseパッケージに含まれた関数である。Rは起動時にbaseなどいくつかのパッケージを自動的に読み込んでいるのである。これに別途パッケージを読み込むとき，あとで読み込まれたパッケージが同名の関数を使っていることがある。このとき，関数名は後から読み込んだもので上書きされる。そのことについての警告が表示されているのである。具体的にみると，dplyr::filter() masks stats::filter()とあるのは，最初に読み込んでいたstatsパッケージのfilter関数は，(tidyverseパッケージに含まれる)dplyrパッケージのもつ同名の関数で上書きされ，今後はこちらが優先的に利用されるよ，ということを示している。\nこのような同音異字関数は，関数を特定するときに混乱を招くかもしれない。あるパッケージの関数であることを明示したい場合は，この警告文にあるように，パッケージ名::関数名，という書き方にすると良い。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter03.html#パイプ演算子",
    "href": "chapter03.html#パイプ演算子",
    "title": "3  Rによるデータハンドリング",
    "section": "3.2 パイプ演算子",
    "text": "3.2 パイプ演算子\n続いてパイプ演算子について解説する。パイプ演算子はtidyverseパッケージに含まれていたmagrittrパッケージで導入されたもので，これによってデータハンドリングの利便性が一気に向上した。そこでRもver 4.2からこの演算子を導入し，特段パッケージのインストールを必要としなくとも使えるようになった。このR本体のパイプ演算子のことを，tidyverseのそれと区別して，ナイーブパイプと呼ぶこともある。\nともあれこのパイプ演算子がいかに優れたものであるかを解説しよう。次のスクリプトは，あるデータセットの標準偏差を計算するものである1。数式で表現すると次の通り。ここで\\(\\bar{x}\\)はデータベクトル\\(x\\)の算術平均。 \\[v = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2}\\]\n\ndat &lt;- c(10, 13, 15, 12, 14) # データ\nM &lt;- mean(dat) # 平均\ndev &lt;- dat - M # 平均偏差\npow &lt;- dev^2 # 平均偏差の2乗\nvariance &lt;- mean(pow) # 平均偏差の2乗の平均が分散\nstandardDev &lt;- sqrt(variance) # 分散の正の平方根が標準偏差\n\nここでは，標準偏差オブジェクトstandardDevを作るまでに平均オブジェクトM，平均偏差ベクトルdev，その2乗したものpow，分散varianceと4つものオブジェクトを作って答えに到達している。また，作られるオブジェクトが左側にあり，その右側にどのような演算をしているかが記述されているため，頭の中では「オブジェクトを作る，次の計算で」と読んでいったことだろう。\nパイプ演算子はこの思考の流れをそのまま具現化する。パイプ演算子は%&gt;%と書き，左側の演算結果をパイプ演算子の右側に来る関数の第一引数として右側に渡す役目をする。これを踏まえて上のスクリプトを書き直してみよう。ちなみにパイプ演算子はショートカットCtrl(Cmd)+Shift+Mで入力できる。\n\ndat &lt;- c(10, 13, 15, 12, 14)\nstandardDev &lt;- dat %&gt;%\n  {\n    . - mean(.)\n  } %&gt;%\n  {\n    .^2\n  } %&gt;%\n  mean() %&gt;%\n  sqrt()\n\nここでピリオド(.)は，前の関数から引き継いだもの(プレイスホルダー)であり，二行目は{dat - mean(dat)}，すなわち平均偏差の計算を意味している。それを次のパイプで二乗し，平均し，平方根を取っている。平均や平方根を取るときにプレイスホルダーが明示されていないのは，引き受けた引数がどこに入るかが明らかなので省略しているからである。\nこの例に見るように，パイプ演算子を使うと，データ\\(\\to\\)平均偏差$\\(2乗\\)\\(平均\\)$平方根，という計算の流れと，スクリプトの流れが一致しているため，理解しやすくなったのではないだろうか。\nまた，ここでの計算は，次のように書くこともできる。\n\nstandardDev &lt;- sqrt(mean((dat - mean(dat))^2))\n\nこの書き方は，関数の中に関数がある入れ子状態になっており，\\(y = h(g(f(x)))\\)のような形式である。これも対応するカッコの内側から読み解いていく必要があり，思考の流れと逆転しているため理解が難しい。パイプ演算子を使うと，x %&gt;% f() %&gt;% g() %&gt;% h() -&gt; yのように記述できるため，苦労せずに読むことができる。\n以下はこのパイプ演算子を使った記述で進めていくので，この表記法(およびショートカット)に慣れていこう。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter03.html#課題1.パイプ演算子",
    "href": "chapter03.html#課題1.パイプ演算子",
    "title": "3  Rによるデータハンドリング",
    "section": "3.3 課題1.パイプ演算子",
    "text": "3.3 課題1.パイプ演算子\n\nsqrt,mean関数がbaseパッケージに含まれることをヘルプで確認してみましょう。どこを見れば良いでしょうか。filter,lag関数はどうでしょうか。\ntidyverseパッケージを読み込んだことで，filter関数はdplyrパッケージのものが優先されることになりました。dplyrパッケージのfilter関数をヘルプで見てみましょう。\n上書きされる前のstatsパッケージのfilter関数に関するヘルプを見てみましょう。\n先ほどのデータを使って，平均値絶対偏差(MeanAD)および中央絶対偏差(MAD)をパイプ演算子を使って算出してみましょう。なお平均値絶対偏差，中央値絶対偏差は次のように定義されるものです。また絶対値を計算するR関数はabsです。\n\n\\[MeanAD = \\frac{1}{n}\\sum_{i=1}^n|x_i - \\bar{x}|\\] \\[MAD = median(|x_1-median(x)|,\\cdots,|x_n-median(x)|)\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter03.html#列選択と行選択",
    "href": "chapter03.html#列選択と行選択",
    "title": "3  Rによるデータハンドリング",
    "section": "3.4 列選択と行選択",
    "text": "3.4 列選択と行選択\nここからはtidyverseを使ったより具体的なデータハンドリングについて言及する。 まずは特定の列および行だけを抜き出すことを考える。データの一部にのみ処理を加えたい場合に重宝する。\n\n3.4.1 列選択\n列選択はselect関数である。これはtidyverseパッケージ内のdplyrパッケージに含まれている。 select関数はMASSパッケージなど，他のパッケージに同名の関数が含まれることが多いので注意が必要である。\n例示のために，Rがデフォルトで持つサンプルデータ，irisを用いる。なお，irisデータは150行あるので，以下ではデータセットの冒頭を表示するhead関数を用いているが，演習の際にはheadを用いなくても良い。\n\n# irisデータの確認\niris %&gt;% head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# 一部の変数を抜き出す\niris %&gt;%\n  select(Sepal.Length, Species) %&gt;%\n  head()\n\n  Sepal.Length Species\n1          5.1  setosa\n2          4.9  setosa\n3          4.7  setosa\n4          4.6  setosa\n5          5.0  setosa\n6          5.4  setosa\n\n\n逆に，一部の変数を除外したい場合はマイナスをつける。\n\niris %&gt;%\n  select(-Species) %&gt;%\n  head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\n# 複数変数の除外\niris %&gt;%\n  select(-c(Petal.Length, Petal.Width)) %&gt;%\n  head()\n\n  Sepal.Length Sepal.Width Species\n1          5.1         3.5  setosa\n2          4.9         3.0  setosa\n3          4.7         3.2  setosa\n4          4.6         3.1  setosa\n5          5.0         3.6  setosa\n6          5.4         3.9  setosa\n\n\nこれだけでも便利だが，select関数は適用時に抜き出す条件を指定してやればよく，そのために便利な以下のような関数がある。\n\nstarts_with()\nends_with()\ncontains()\nmatches()\n\n使用例を以下に挙げる。\n\n# starts_withで特定の文字から始まる変数を抜き出す\niris %&gt;%\n  select(starts_with(\"Petal\")) %&gt;%\n  head()\n\n  Petal.Length Petal.Width\n1          1.4         0.2\n2          1.4         0.2\n3          1.3         0.2\n4          1.5         0.2\n5          1.4         0.2\n6          1.7         0.4\n\n# ends_withで特定の文字で終わる変数を抜き出す\niris %&gt;%\n  select(ends_with(\"Length\")) %&gt;%\n  head()\n\n  Sepal.Length Petal.Length\n1          5.1          1.4\n2          4.9          1.4\n3          4.7          1.3\n4          4.6          1.5\n5          5.0          1.4\n6          5.4          1.7\n\n# containsで部分一致する変数を取り出す\niris %&gt;%\n  select(contains(\"etal\")) %&gt;%\n  head()\n\n  Petal.Length Petal.Width\n1          1.4         0.2\n2          1.4         0.2\n3          1.3         0.2\n4          1.5         0.2\n5          1.4         0.2\n6          1.7         0.4\n\n# matchesで正規表現による選択をする\niris %&gt;%\n  select(matches(\".t.\")) %&gt;%\n  head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\n\nここで触れた正規表現とは，文字列を特定するためのパターンを指定する表記ルールであり，R言語に限らずプログラミング言語一般で用いられるものである。書誌検索などでも用いられることがあり，任意の文字列や先頭・末尾の語などを記号(メタ文字)を使って表現するものである。詳しくは正規表現で検索すると良い(たとえばこちらのサイトなどがわかりやすい。)\n\n\n3.4.2 行選択\n一般にデータフレームは列に変数が並んでいるので，select関数による列選択とは変数選択とも言える。 これに対し，行方向にはオブザベーションが並んでいるので，行選択とはオブザベーション(ケース，個体)の選択である。行選択にはdplyrのfilter関数を使う。\n\n# Sepal.Length変数が6以上のケースを抜き出す\niris %&gt;%\n  filter(Sepal.Length &gt; 6) %&gt;%\n  head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          7.0         3.2          4.7         1.4 versicolor\n2          6.4         3.2          4.5         1.5 versicolor\n3          6.9         3.1          4.9         1.5 versicolor\n4          6.5         2.8          4.6         1.5 versicolor\n5          6.3         3.3          4.7         1.6 versicolor\n6          6.6         2.9          4.6         1.3 versicolor\n\n# 特定の種別だけ抜き出す\niris %&gt;%\n  filter(Species == \"versicolor\") %&gt;%\n  head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          7.0         3.2          4.7         1.4 versicolor\n2          6.4         3.2          4.5         1.5 versicolor\n3          6.9         3.1          4.9         1.5 versicolor\n4          5.5         2.3          4.0         1.3 versicolor\n5          6.5         2.8          4.6         1.5 versicolor\n6          5.7         2.8          4.5         1.3 versicolor\n\n# 複数指定の例\niris %&gt;%\n  filter(Species != \"versicolor\", Sepal.Length &gt; 6) %&gt;%\n  head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1          6.3         3.3          6.0         2.5 virginica\n2          7.1         3.0          5.9         2.1 virginica\n3          6.3         2.9          5.6         1.8 virginica\n4          6.5         3.0          5.8         2.2 virginica\n5          7.6         3.0          6.6         2.1 virginica\n6          7.3         2.9          6.3         1.8 virginica\n\n\nここで==とあるのは一致しているかどうかの判別をするための演算子である。=ひとつだと「オブジェクトへの代入」と同じになるので，判別条件の時には重ねて表記する。同様に，!=とあるのはnot equal，つまり不一致のとき真になる演算子である。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter03.html#変数を作る再割り当てする",
    "href": "chapter03.html#変数を作る再割り当てする",
    "title": "3  Rによるデータハンドリング",
    "section": "3.5 変数を作る・再割り当てする",
    "text": "3.5 変数を作る・再割り当てする\n既存の変数から別の変数を作る，あるいは値の再割り当ては，データハンドリング時に最もよく行う操作のひとつである。たとえば連続変数をある値を境に「高群・低群」というカテゴリカルな変数に作り変えたり，単位を変換するために線形変換したりすることがあるだろう。このように，変数を操作するときに「既存の変数を加工して特徴量を作りだす」というときの操作は，基本的にdplyrのmutate関数を用いる。次の例をみてみよう。\n\nmutate(iris, Twice = Sepal.Length * 2) %&gt;% head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species Twice\n1          5.1         3.5          1.4         0.2  setosa  10.2\n2          4.9         3.0          1.4         0.2  setosa   9.8\n3          4.7         3.2          1.3         0.2  setosa   9.4\n4          4.6         3.1          1.5         0.2  setosa   9.2\n5          5.0         3.6          1.4         0.2  setosa  10.0\n6          5.4         3.9          1.7         0.4  setosa  10.8\n\n\n新しくTwice変数ができたのが確認できるだろう。この関数はパイプ演算子の中で使うことができる(というかその方が主な使い方である)。次の例は，Sepal.Length変数を高群と低群の2群に分けるものである。\n\niris %&gt;%\n  select(Sepal.Length) %&gt;%\n  mutate(Sepal.HL = ifelse(Sepal.Length &gt; mean(Sepal.Length), 1, 2)) %&gt;%\n  mutate(Sepal.HL = factor(Sepal.HL, label = c(\"High\", \"Low\"))) %&gt;%\n  head()\n\n  Sepal.Length Sepal.HL\n1          5.1      Low\n2          4.9      Low\n3          4.7      Low\n4          4.6      Low\n5          5.0      Low\n6          5.4      Low\n\n\nここでもちいたifelse関数は，if(条件判断,真のときの処理,偽のときの処理)という形でもちいる条件分岐関数であり，ここでは平均より大きければ1，そうでなければ2を返すようになっている。mutate関数でこの結果をSepal.HL変数に代入(生成)し，次のmutate関数では今作ったSepal.HL変数をFactor型に変換して，その結果をまたSepal.HL変数に代入(上書き)している。このように，変数の生成先を生成元と同じにしておくと上書きされるため，たとえば変数の型変換(文字型から数値型へ，数値型からFactor型へ，など)にも用いることができる。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter03.html#課題2.select-filter-mutate",
    "href": "chapter03.html#課題2.select-filter-mutate",
    "title": "3  Rによるデータハンドリング",
    "section": "3.6 課題2.select, filter, mutate",
    "text": "3.6 課題2.select, filter, mutate\n\nBaseball.csvを読み込んで、データフレームdfに代入しましょう。\ndfには複数の変数が含まれています。変数名の一覧はnames関数で確認できます。dfオブジェクトに含まれる変数名を確認しましょう。\ndfには多くの変数がありますが、必要なのは年度(Year)、選手名(Name)、所属球団(team)、身長(height)、体重(weight)、年俸(salary)、守備位置(position)だけです。これらの変数だけを選択して、df2オブジェクトを作成しましょう。\ndf2には数年分のデータが含まれています。2020年度のデータだけを分析したいので、選別してみましょう。\n同じく、2020年度の阪神タイガースに関するデータだけを選別してみましょう。\n同じく、2020年度の阪神タイガース以外のデータセットはどのようにして選別できるでしょうか。\n選手の身体的特徴を表すBMI変数を作成しましょう。なお、BMIは体重(kg)を身長(m)の二乗で除したものです。変数heightの単位がcmであることに注意しましょう。\n投手と野手を区別する新しい変数position2を作成しましょう。これはFactor型にします。なお、野手は投手でないもの、すなわち内野手、外野手、捕手のいずれかです。\n日本プロ野球界は大きく分けてセリーグ(Central League)とパリーグ(Pacific League)に分かれています。セリーグに所属する球団はGiants, Carp, Tigers, Swallows, Dragons, DeNAであり、パ・リーグはそれ以外です。df2を加工して、所属するリーグの変数Leagueを作成しましょう。この変数もFactor型にしておきましょう。\n変数Yearは語尾に「年度」という文字が入っているため文字列型になっています。実際に使うときは不便なので、「年度」という文字を除外し、数値型変数に変換しましょう。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter03.html#sec-Long_and_Wide",
    "href": "chapter03.html#sec-Long_and_Wide",
    "title": "3  Rによるデータハンドリング",
    "section": "3.7 ロング型とワイド型",
    "text": "3.7 ロング型とワイド型\nここまでみてきたデータは行列の2次元に，ケース\\(\\times\\)変数の形で格納されていた。この形式は，人間が見て管理するときにわかりやすい形式をしているが，計算機にとっては必ずしもそうではない。たとえば「神エクセル」と揶揄されることがあるように，稀に表計算ソフトを方眼紙ソフトあるいは原稿用紙ソフトと勘違いしたかのような使い方がなされる場合がある。人間にとってはわかりやすい(見て把握しやすい)かもしれないが，計算機にとって構造が把握できないため，データ解析に不向きである。巷には，こうした分析しにくい電子データがまだまだたくさん存在する。\nこれをうけて2020年12月，総務省により機械判読可能なデータの表記方法の統一ルールが策定された(総務省 2020)。それには次のようなチェック項目が含まれている。\n\nファイル形式はExcelかCSVとなっているか\n1セル1データとなっているか\n数値データは数値属性とし，文字列を含まないこと\nセルの結合をしていないか\nスペースや改行等で体裁を整えていないか\n項目名を省略していないか\n数式を使用している場合は，数値データに修正しているか\nオブジェクトを使用していないか\nデータの単位を記載しているか\n機種依存文字を使用していないか\nデータが分断されていないか\n1シートに複数の表が掲載されていないか\n\nデータの入力の基本は，1行に1ケースの情報が入っている，過不足のない1つのデータセットを作ることといえるだろう。\n同様に，計算機にとって分析しやすいデータの形について，Hadley (2014) が提唱したのが整然データ(Tidy Data)という考え方である。整然データとは，次の4つの特徴を持ったデータ形式のことを指す。\n\n個々の変数(variable)が1つの列(column)をなす。\n個々の観測(observation)が1つの行(row)をなす。\n個々の観測の構成単位の類型(type of observational unit)が1つの表(table)をなす。\n個々の値(value)が1つのセル(cell)をなす。\n\nこの形式のデータであれば，計算機が変数と値の対応構造を把握しやすく，分析しやすいデータになる。データハンドリングの目的は，混乱している雑多なデータを，利用しやすい整然データの形に整えることであると言っても過言ではない。 さて，ここでよく考えてみると，変数名も一つの変数だと考えることに気づく。一般に，行列型のデータは次のような書式になっている。\n\nワイド型データ\n\n\n\n午前\n午後\n夕方\n深夜\n\n\n\n\n東京\n晴\n晴\n雨\n雨\n\n\n大阪\n晴\n曇\n晴\n晴\n\n\n福岡\n晴\n曇\n曇\n雨\n\n\n\nここで，たとえば大阪の夕方の天気を見ようとすると「晴れ」であることは明らかだが，この時の視線の動きは大阪行の，夕方列，という参照の仕方である。言い方を変えると，大阪・夕方の「晴れ」を参照するときに，行と列の両方のラベルを参照する必要がある。\nここで同じデータを次のように並べ替えてみよう。\n\nロング型データ\n\n\n地域\n時間帯\n天候\n\n\n\n\n東京\n午前\n晴\n\n\n東京\n午後\n晴\n\n\n東京\n夕方\n雨\n\n\n東京\n深夜\n雨\n\n\n大阪\n午前\n晴\n\n\n大阪\n午後\n曇\n\n\n大阪\n夕方\n晴\n\n\n大阪\n深夜\n晴\n\n\n福岡\n午前\n晴\n\n\n福岡\n午後\n曇\n\n\n福岡\n夕方\n曇\n\n\n福岡\n深夜\n雨\n\n\n\nこのデータが表す情報は同じだが，大阪・夕方の条件を絞り込むことは行選択だけでよく，計算機にとって使いやすい。この形式をロング型データ，あるいは「縦持ち」データという。これに対して前者の形式をワイド型データ，あるいは「横持ち」データという。\nロング型データにする利点のひとつは，欠損値の扱いである。ワイド型データで欠損値が含まれる場合，その行あるいは列全体を削除するのは無駄が多く，かと言って行・列両方を特定するのは技術的にも面倒である。これに対しロング型データの場合は，当該行を絞り込んで削除するだけで良い。\ntidyverseには(正確にはtidyrには)，このようなロング型データ，ワイド型データの変換関数が用意されている。 実例とともに見てみよう。まずはワイド型データをロング型に変換するpivot_longerである。\n\niris %&gt;% pivot_longer(-Species)\n\n# A tibble: 600 × 3\n   Species name         value\n   &lt;fct&gt;   &lt;chr&gt;        &lt;dbl&gt;\n 1 setosa  Sepal.Length   5.1\n 2 setosa  Sepal.Width    3.5\n 3 setosa  Petal.Length   1.4\n 4 setosa  Petal.Width    0.2\n 5 setosa  Sepal.Length   4.9\n 6 setosa  Sepal.Width    3  \n 7 setosa  Petal.Length   1.4\n 8 setosa  Petal.Width    0.2\n 9 setosa  Sepal.Length   4.7\n10 setosa  Sepal.Width    3.2\n# ℹ 590 more rows\n\n\nここでは元のirisデータについて，Speciesセルを軸として，それ以外の変数名と値をname,valueに割り当てて縦持ちにしている。\n逆に，ロング型のデータをワイド型に持ち替えるには，pivot_widerを使う。 実例は以下の通りである。\n\niris %&gt;%\n  select(-Species) %&gt;%\n  rowid_to_column(\"ID\") %&gt;%\n  pivot_longer(-ID) %&gt;%\n  pivot_wider(id_cols = ID, names_from = name, values_from = value)\n\n# A tibble: 150 × 5\n      ID Sepal.Length Sepal.Width Petal.Length Petal.Width\n   &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1     1          5.1         3.5          1.4         0.2\n 2     2          4.9         3            1.4         0.2\n 3     3          4.7         3.2          1.3         0.2\n 4     4          4.6         3.1          1.5         0.2\n 5     5          5           3.6          1.4         0.2\n 6     6          5.4         3.9          1.7         0.4\n 7     7          4.6         3.4          1.4         0.3\n 8     8          5           3.4          1.5         0.2\n 9     9          4.4         2.9          1.4         0.2\n10    10          4.9         3.1          1.5         0.1\n# ℹ 140 more rows\n\n\n今回はSpecies変数を除外し，別途ID変数として行番号を変数に付与した。この行番号をキーに，変数名はnames列から，その値はvalue列から持ってくることでロング型をワイド型に変えている2。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter03.html#グループ化と要約統計量",
    "href": "chapter03.html#グループ化と要約統計量",
    "title": "3  Rによるデータハンドリング",
    "section": "3.8 グループ化と要約統計量",
    "text": "3.8 グループ化と要約統計量\nデータをロング型にすることで，変数やケースの絞り込みが容易になる。その上で，ある群ごとに要約した統計量を算出したい場合は，group_by変数によるグループ化と，summariseあるいはreframeがある。実例を通して確認しよう。\n\niris %&gt;% group_by(Species)\n\n# A tibble: 150 × 5\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\n上のコードでは，一見したところ表示されたデータに違いがないように見えるが，出力時にSpecies[3]と表示されていることがわかる。ここで，Species変数の3水準で群分けされていることが示されている。これを踏まえて，summariseしてみよう。\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    n = n(),\n    Mean = mean(Sepal.Length),\n    Max = max(Sepal.Length),\n    IQR = IQR(Sepal.Length)\n  )\n\n# A tibble: 3 × 5\n  Species        n  Mean   Max   IQR\n  &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 setosa        50  5.01   5.8 0.400\n2 versicolor    50  5.94   7   0.7  \n3 virginica     50  6.59   7.9 0.675\n\n\nここではケース数(n)，平均(mean)，最大値(max)，四分位範囲(IQR)3を算出した。\nまた，ここではSepal.Lengthについてのみ算出したが，他の数値型変数に対しても同様の計算がしたい場合は，across関数を使うことができる。\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(across(\n    c(Sepal.Length, Sepal.Width, Petal.Length),\n    ~ mean(.x)\n  ))\n\n# A tibble: 3 × 4\n  Species    Sepal.Length Sepal.Width Petal.Length\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46\n2 versicolor         5.94        2.77         4.26\n3 virginica          6.59        2.97         5.55\n\n\nここで，~mean(.x)の書き方について言及しておく。チルダ(tilda,~)で始まるこの式を，Rでは特にラムダ関数とかラムダ式と呼ぶ。これはこの場で使う即席関数の作り方である。別の方法として，正式に関数を作る関数functionを使って次のように書くこともできる。\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(across(\n    c(Sepal.Length, Sepal.Width, Petal.Length),\n    function(x) {\n      mean(x)\n    }\n  ))\n\n# A tibble: 3 × 4\n  Species    Sepal.Length Sepal.Width Petal.Length\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46\n2 versicolor         5.94        2.77         4.26\n3 virginica          6.59        2.97         5.55\n\n\nラムダ関数や自作関数の作り方については，後ほどあらためて触れるとして，ここでは複数の変数に関数をあてがう方法を確認して置いて欲しい。across関数で変数を選ぶ際は，select関数の時に紹介したstarts_withなども利用できる。次に示す例は，複数の変数を選択し，かつ，複数の関数を適用する例である。複数の関数を適用するために，ラムダ関数をリストで与えることができる。\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(across(starts_with(\"Sepal\"),\n    .fns = list(\n      M = ~ mean(.x),\n      Q1 = ~ quantile(.x, 0.25),\n      Q3 = ~ quantile(.x, 0.75)\n    )\n  ))\n\n# A tibble: 3 × 7\n  Species    Sepal.Length_M Sepal.Length_Q1 Sepal.Length_Q3 Sepal.Width_M\n  &lt;fct&gt;               &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1 setosa               5.01            4.8              5.2          3.43\n2 versicolor           5.94            5.6              6.3          2.77\n3 virginica            6.59            6.22             6.9          2.97\n# ℹ 2 more variables: Sepal.Width_Q1 &lt;dbl&gt;, Sepal.Width_Q3 &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter03.html#課題3.データの整形",
    "href": "chapter03.html#課題3.データの整形",
    "title": "3  Rによるデータハンドリング",
    "section": "3.9 課題3.データの整形",
    "text": "3.9 課題3.データの整形\n\n上で作ったdf2オブジェクトを利用します。環境にdf2オブジェクトが残っていない場合は、もう一度上の課題に戻って作り直しておきましょう。\n年度(Year)でグルーピングし、年度ごとの登録選手数(データの数)、平均年俸を見てみましょう。\n年度(Year)とチーム(team)でグルーピングし、同じく年度ごとの登録選手数(データの数)、平均年俸を見てみましょう。\n続いて、一行に1年度分、列に各チームと変数の組み合わせが入った、ワイド型データを作りたいと思います。pivot_widerを使って上のオブジェクトをワイド型にしてみましょう。\nワイド型になったデータを、Year変数をキーにしてpivot_longerでロング型データに変えてみましょう。\n\n\n\n\n\nHadley, Wickham. 2014. “Tidy Data.” Journal of Statistical Software 59: 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n総務省. 2020. “統計表における機械判別可能なデータ作成に関する表記方法.” 統計企画会議申し合わせ. https://www.soumu.go.jp/main_content/000723697.pdf.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter03.html#footnotes",
    "href": "chapter03.html#footnotes",
    "title": "3  Rによるデータハンドリング",
    "section": "",
    "text": "もちろんsd(dat) の一行で済む話だが，ここでは説明のために各ステップを書き下している。もっとも，sd関数で計算されるのは\\(n-1\\)で割った不偏分散の平方根であり，標本標準偏差とは異なるものである。↩︎\nSpecies変数を除外したのは，これをキーにしたロング型をワイド型に変えることができない(Speciesは3水準しかない)からで，個体を識別するIDが別途必要だったからである。Species情報が欠落することになったが，これはロング型データのvalue列がchar型とdouble型の両方を同時に持てないからである。この問題を回避するためには，Factor型のデータをas.numeric()関数で数値化することなどが考えられる。↩︎\n四分位範囲(Inter Quantaile Range)とは，データを値の順に4分割した時の上位1/4の値から，上位3/4の値を引いた範囲である。↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rによるデータハンドリング</span>"
    ]
  },
  {
    "objectID": "chapter04.html",
    "href": "chapter04.html",
    "title": "4  Rによるレポートの作成",
    "section": "",
    "text": "4.1 Rmd/Quartoの使い方",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rによるレポートの作成</span>"
    ]
  },
  {
    "objectID": "chapter04.html#rmdquartoの使い方",
    "href": "chapter04.html#rmdquartoの使い方",
    "title": "4  Rによるレポートの作成",
    "section": "",
    "text": "4.1.1 概略\n今回はRStudioを使った文書作成法について解説する。皆さんは、これまで作成と言えば、基本的にMicrosoft Wordのような文書作成ソフトを使ってきたものと思う。また，統計解析といえばR(やその他のソフト)，図表の作成はExcelといったように，用途ごとに異なるアプリケーションを活用するのが一般的であろう。\nこのやり方は，統計解析の数値結果を表計算ソフトに，そこで作った図表を文書作成ソフトにコピー＆ペーストする，という転記作業が何度も発生する。ここで転記ミス・貼り付け間違いが生じると，当然ながら出来上がる文書は間違ったものになる。こうした転記ミスのことを「コピペ汚染」と呼ぶこともある。\n問題はこの環境をまたぐ作業にあるわけで，計算・作図・文書が一つの環境で済めばこうした問題が起こらない。これを解決するのがR markdonやQuartoという書式・ソフトウェアなのである。\nRmarkdownのmarkdownとは書式の一種である。マークアップ言語と呼ばれる書き方の一種で，なかでもRとの連携に特化したのがRmarkdownである。マークアップ言語とは，言語の中に専門の記号を埋め込み，その書式に対応した読み込みアプリで，表示の際に書式を整える方式のことを指す。有名なマークアップ言語としては，数式に特化した LaTeX や，インターネットウェブサイトで用いられている HTML などがある。\nRmarkdownはmarkdownの書式を踏襲しつつ，Rでの実行結果を文中に埋め込むコマンドを有している。Rのコマンドで計算したり図表を作成しつつ，その結果を埋め込む場所をマークアップ言語で指定する。最終的に文書を閲覧する場合は，マークアップ言語を出力ファイルに変換する(コンパイルする，ニットするという)必要があり，その時Rでの計算が実行される。コンパイルのたびに計算されるので，同じコードでも乱数を使ったコードを書いていたり，一部読み込みファイルを変更するだけで，出力される結果は変わる。しかしコピペ汚染のように，間違った値・図表が含まれるものではないので，研究の再現性にも一役買うことになる。再現可能な文書の作成について、詳しくは 高橋 (2018) を参考にすると良い。\nQuartoはRmarkdownをさらに拡張させたもので，RStudioを提供しているPosit社が今もっとも注力しているソフトのひとつである。RmarkdownはRとmarkdownの連携であったが，QuartoはRだけでなく，PythonやJuliaといった他の言語にも対応しているし，これら複数の計算言語の混在も許す。すなわち，一部はRで計算し，その結果をPythonで検算してJuliaで描画する，といったことを一枚のファイルで書き込むことも可能である。\nなおこの授業資料もQuartoで作成している。このようにQuartoはプレゼンテーション資料やウェブサイトも作成できるし，出力形式もウェブサイトだけでなくPDFやePUB(電子書籍の形式)にすることが可能である。なおこの授業の資料もウェブサイトと同時にPDF形式と，ePUB形式で出力されている。Quartoについて専門の解説書はまだないが，インターネットに充実したドキュメントがあるので検索するといいだろう。まだ新しい技術なので，公式を第一に参照すると良い。\n\n\n4.1.2 ファイルの作成とknit\nRmarkdownはRStudioとの相性がよく，RStudioのFile &gt; New FileからR Markdownを選ぶとRmarkdownファイルがサンプルとともに作成される。作成時に文書のタイトル，著者名，作成日時や出力フォーマットが指定できるサブウィンドゥが開き，作成するとサンプルコードが含まれたR markdownファイルが表示されるだろう。\nQuartoも同様に，RStudioのFile &gt; New FileからQuarto Documentを選ぶことで新しいファイル画面が開く。なおRmarkdownファイルの拡張子はRmdとすることが一般的であり，QuartoはQmdとすることが一般的である。もっとも，QuartoはRStudio以外のエディタから利用することも考えられていて，例えばVS Codeなどの一般的なエディタで作成し，コマンドライン経由でコンパイルすることも可能である。\n\n\n\nRmarkdownファイルのサンプル\n\n\n\n\n\nQuartoファイルのサンプル\n\n\nRmarkdown，Quartoともに，ファイルの冒頭に4つのハイフンで囲まれた領域が見て取れるだろう。これはYAMLヘッダ(YAMLはYet Another Markup Languageの略。ここはまだマークアップ領域じゃないよということ)と呼ばれる，文書全体に対する設定をする領域のことである。\nこの領域を一瞥すると，タイトルや著者名，出力形式などが記載されていることが見て取れる。YAMLヘッダはインデントに敏感で，また正しくない記述が含まれているとエラーになって出力ファイルが作られないことが少なくないため，ここを手動で書き換えるときは注意が必要である。とはいえ，ここを自在に書き換えることができるようになると，様々な応用が効くので興味があるものは調べて色々トライしてもらいたい。\nさて，Rmd/Qmdのファイル上部にKnitあるいはRenderと書かれたボタンがあるだろう。これをクリックすると，表示用ファイルへの変換が実行される。1 Rmarkdownの場合は，すでにサンプルコードが含まれているので，数値および図表のはいったHTMLドキュメントが表示されるだろう。以下はこのサンプルコードを例に説明するため，Rmarkdownとそのコンパイル(knit))を一度試してもらいたい。その上で，元のRmdファイルと出来上がったファイルとの対応関係を確認してみよう。\n\n\n\nRmdファイルと出力結果の対応\n\n\nおおよそ，何がどのように変換されているかの対応が推察できるだろう。 出力ファイルの冒頭には，YAMLで設定したタイトル，著者名，日付などが表示されているし，#の印がついていた一行は見出しとして強調されている。\n特に注目したいのが，元のファイルで3つのクォーテーションで囲まれた灰色の領域である。この領域のことを特にチャンクといい，ここに書かれたRスクリプトが変換時に実行され，結果として出力される。出力ファイルをみると，summary(cars) というチャンクで指定された命令文があり，その結果(carsというデータセットの要約)が出力されているのが見て取れる。繰り返しになるが，ポイントは原稿ファイルには計算を指示するスクリプトが書かれているだけで，出力結果を書いていないことにある。原稿は指示だけなのである。こうすることで，コピー＆ペーストのミスがなくなるし，同じRmd/Qmd原稿とデータを持っていれば，ことなるPC上でも同じ出力が得られる。環境を統合することで，ミスの防止と再現可能性に貢献しているのがわかるだろう。\n今回はcarsというRがデフォルトでもっているサンプルデータの例なので，どの環境でも同じ結果が出力されている。しかしもちろん，個別のデータファイルであっても，同じファイルで同じ読み込み方，同じ加工をしていれば，環境が違っても追跡可能である。注意して欲しいのは，コンパイルするときは新しい環境から行われるという点ある。すなわち，原稿ファイルにないオブジェクトの利用はできないのである。これは再現性を担保するという意味では当然のことで，「事前に別途処理しておいたデータ」から分析を始められても，その事前処理が適切だったかどうかがチェックできないからである。Rmd/Qmdファイルと，CSVファイルなどの素データが共有されていれば再現できる，という利点を活かすため，データハンドリングを含めた前処理も全てチャンクに書き込み，新しい環境で最初からトレースできるようにする必要がある。不便に感じるところもあるかもしれないが，科学的営みとして重要な手続きであることを理解してもらいたい。2\nRStudiでは，ビジュアルモードやアウトライン表示，チャンク挿入ボタンやチャンクごとの実行・設定などRmd/Qmdファイルの編集に便利な機能も複数用意されているので， 高橋 (2018) などを参考にいろいろ試してみるといいだろう。\n\n\n4.1.3 マークダウンの記法\n以下では，マークダウン記法について基本的な利用法を解説する。\n\n4.1.3.1 見出しと強調\nすでに見たように，マークダウンでは#記号で見出しを作ることができる。#の数が見出しレベルに対応し，#はトップレベル，本でいうところの「章,chapter」，HTMLでいうところのH1に相当する。#記号の後ろに半角スペースが必要なことに注意されたし。以下，##で「節,section」あるいはH2，###で小節(subsection,H3)，####で小小節(subsubsection,H4)…と続く。\n心理学を始め，科学論文の書き方としての「パラグラフライティング」を既にみしっていることだろう。文章をセクション，サブセクション，パラグラフ，センテンスのように階層的に分割し，それぞれの区分が4つの下位区分を含むような文章構造である。心理学の場合は特に「問題，方法，結果，考察」の4セクションで一論文が構成されるのが基本である。こうしたアウトラインを意識した書き方は読み手にも優しく，マークダウンの記法ではそれが自然と実装できるようになっている。\nこれとは別に，一部を太字や斜体で強調したいこともあるだろう。そのような場合はアスタリスクを1つ，あるいは2つつけて強調したり強調したりできる。\n\n\n4.1.3.2 図表とリンク\n文中に図表を挿入したいこともあるだろう。表の挿入は，マークダウン独自の記法があり，縦棒|やハイフン-を駆使して以下のように表記する。\n| Header 1 | Header 2 | Header 3 |\n| -------- | -------- | -------- |\n| Row 1    | Data 1   | Data 2   |\n| Row 2    | Data 3   | Data 4   |\nRのコードの中には分析結果をマークダウン形式で出力してくれる関数もあるし，表計算ソフトなどでできた表があるなら，chatGPTなど生成AIを利用するとすぐに書式変換してくれるので，そういったツールを活用すると良い。\n図の挿入は，マークダウンでは図のファイルへのリンクと考えると良い。次のように，大括弧で括った文字がキャプション，つづく小括弧で括ったものが図へのリンクとなる。実際に表示されるときは図が示される。\n![図のキャプション](図へのリンク)\n同様に，ウェブサイトへのリンクなども，[表示名](リンク先)の書式で対応できる。\n\n\n4.1.3.3 リスト\n並列的に箇条書きを示したい場合は，プラスあるいはマイナスでリストアップする。注意すべきは，リストの前後に改行を入れておくべきことである。\nここまで前の文\n\n+  list item 1\n+  list item 2\n+  list item 3\n    - sub item 1\n    - sub item 2\n\nここから次の文\n\n\n4.1.3.4 チャンク\n既に述べたように，チャンク(chunk)と呼ばれる領域は実行されるコードを記載するところである。チャンクはまず，バックスラッシュ3つ繋げることでコードブロックであることを示し，次に rと書くことで計算エンジンがRであることを明示する。ここにJuiliaやPythonなど他の計算エンジンを指定することも可能である。\n可能であれば，チャンク名をつけておくと良い。次の例は，チャンク名として「chunksample」を与えたものである。チャンク名をつけておくと，RStudioでは見出しジャンプをつかって移動することもできるので，編集時に便利である。\n```{r chunksample,echo = FALSE} summary(cars) ```\nさらに，echo = FALSEのようにチャンクオプションを指定することができる。echo=FALSEは入力したスクリプトを表示せず，結果だけにするオプションである。そのほか「計算結果を含めない」「表示せずに計算は実行する」等様々な指定が可能である。\nなおQuartoではこのチャンクオプションを次のように書くこともできる。\n```{r} #| echo: FALSE #| include: FALSE summary(cars) ```",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rによるレポートの作成</span>"
    ]
  },
  {
    "objectID": "chapter04.html#プロットによる基本的な描画",
    "href": "chapter04.html#プロットによる基本的な描画",
    "title": "4  Rによるレポートの作成",
    "section": "4.2 プロットによる基本的な描画",
    "text": "4.2 プロットによる基本的な描画\n再現可能な文書という観点から，図表もスクリプトによる記述で表現することは重要である。\nデータはまず可視化するものと心がけよ。可視化は，数値の羅列あるいはまとめられた統計量では把握しきれない多くの情報を提供し，潜在的な関係性を直観的に見つけ出せる可能性がある。なので，取得したあらゆるデータはまず可視化するもの，と思っておいて間違いはない。大事なことなので二度言いました。可視化の重要性については心理学の知見にも触れている キーラン・ヒーリー ([2018] 2021) も参考にしてほしい。\nさて，Rには基本的な作図環境も整っており，plotという関数に引数として，x軸，y軸に相当する変数を与えるだけで，簡単に散布図を書いてくれる。\n\nplot(iris$Sepal.Length, iris$Sepal.Width,\n  main = \"Example of Scatter Plot\",\n  xlab = \"Sepal.Length\",\n  ylab = \"Sepal.Width\"\n)\n\n\n\n\n\n\n\n\nこの関数のオプションとして，タイトルを与えたり，軸に名前を与えたりできる。またプロットされるピンの形，描画色，背景色など様々な操作が可能である。特段のパッケージを必要とせずとも，基本的な描画機能は備えていると言えるだろう。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rによるレポートの作成</span>"
    ]
  },
  {
    "objectID": "chapter04.html#ggplotによる描画",
    "href": "chapter04.html#ggplotによる描画",
    "title": "4  Rによるレポートの作成",
    "section": "4.3 ggplotによる描画",
    "text": "4.3 ggplotによる描画\nここでは，tidyverseに含まれる描画専用のパッケージである，ggplot2 パッケージを用いた描画を学ぶ。Rの基本描画関数でもかなりのことができるのだが，このggplot2パッケージをもちいた図の方が美しく，直観的に操作できる。というのもggplotのggとはThe Grammar of Graphics(描画の文法)のことであり，このことが示すようにロジカルに図版を制御できるからである。ggplot2の形で記述された図版のスクリプトは可読性が高く，視覚的にも美しいため，多くの文献で利用されている。\nggplot2 パッケージの提供する描画環境の特徴は，レイア(Layer)の概念である。図版は複数のレイアの積み重ねとして表現される。まず土台となるキャンバスがあり，そこにデータセット，幾何学的オブジェクト(点，線，バーなど)，エステティックマッピング(色，形，サイズなど)，凡例やキャプションを重ねていく，という発想である。そして図版全体を通したテーマを手強することで，カラーパレットの統一などの仕上げをすれば，すぐにでも論文投稿可能なレベルの図版を描くことができる。\n以下にggplot2における描画のサンプルを示す。サンプルデータmtcarsを用いた。\n\nlibrary(ggplot2)\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  labs(title = \"車の重量と燃費の関係\", x = \"重量\", y = \"燃費\")\n\n\n\n\n\n\n\n\nまずは出来上がる図版の美しさと，コードのイメージを把握してもらいたい。 最初のlibrary(ggplot2)はパッケージを読み込んでいるところである。今回は明示的にggplot2を読み込んでいるが，tidyverseパッケージを読み込むと同時に読み込まれているので，Rのスクリプトの冒頭にlibrary(tidyverse)と書く癖をつけておけば必要ない。\n続いてggplotの関数が4行にわたって書いてあるが，それぞれが+の記号で繋がれていることがわかるだろう。これがレイアを重ねるという作業に相当する。まずは，図を書くためのキャンバスを用意し，その上にいろいろ重ねていくのである。\n次のコードは，キャンバスだけを描画した例である。\n\ng &lt;- ggplot()\nprint(g)\n\n\n\n\n\n\n\n\nここではg というオブジェクトをggplot関数でつくり，それを表示させた。最初はこのようにプレーンなキャンバスだが，ここに次々と上書きしていくことになる。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rによるレポートの作成</span>"
    ]
  },
  {
    "objectID": "chapter04.html#幾何学的オブジェクトgeom",
    "href": "chapter04.html#幾何学的オブジェクトgeom",
    "title": "4  Rによるレポートの作成",
    "section": "4.4 幾何学的オブジェクトgeom",
    "text": "4.4 幾何学的オブジェクトgeom\n幾何学的オブジェクト(geometric object) とは，データの表現方法の指定であり，ggplotには様々なパターンが用意されている。以下に一例を挙げる。\n\ngeom_point(): 散布図で使用され、データ点を個々の点としてプロットする。\ngeom_line(): 折れ線グラフで使用され、データ点を線で結んでプロットする。時系列データなどによく使われる。\ngeom_bar(): 棒グラフで使用され、カテゴリごとの量を棒で表示する。データの集計（カウントや合計など）に適している。\ngeom_histogram(): ヒストグラムで使用され、連続データの分布を棒で表示する。データの分布を理解するのに役立つ。\ngeom_boxplot(): 箱ひげ図で使用され、データの分布（中央値、四分位数、外れ値など）を要約して表示する。\ngeom_smooth(): 平滑化曲線を追加し、データのトレンドやパターンを可視化する。線形回帰やローパスフィルタなどの方法が使われる。\n\nこれらの幾何学的オブジェクトに，データおよび軸との対応を指定するなどして描画する。次に挙げるのはgeom_point による点描画，つまり散布図である。\n\nggplot() +\n  geom_point(data = mtcars, mapping = aes(x = disp, y = wt))\n\n\n\n\n\n\n\n\n一行目でキャンバスを用意し，そこにgeom_pointで点を打つようにしている。 このとき，データはmtcarsであり，x軸に変数dispを，y軸に変数wtをマッピングしている。マッピング関数のaesはaesthetic mappingsの意味で，データによって変わる値(x座標，y座標，色，サイズ，透明度など)を指定することができる。\nレイアは次々と重ねることができる。以下の例を見てみよう。\n\ng &lt;- ggplot()\ng1 &lt;- g + geom_point(data = mtcars, mapping = aes(x = disp, y = wt))\ng2 &lt;- g1 + geom_line(data = mtcars, mapping = aes(x = disp, y = wt))\nprint(g2)\n\n\n\n\n\n\n\n\n重ねることを強調するために，g オブジェクトを次々作るようにしたが，もちろん1つのオブジェクトでまとめて書いてもいいし，gオブジェクトとして保管せずとも，最初の例のように直接出力することもできる。また，ここでは点描画オブジェクトに線描画オブジェクトを重ねているが，データやマッピングは全く同じである。異なるデータを一枚のキャンバスに書く場合は，このように幾何学オブジェクトごとの指定が可能であるが，図版は得てして一枚のキャンバスに一種類のデータになりがちである。そのような場合は，以下に示すようにキャンバスの段階から基本となるデータセットとマッピングを与えることが可能である。\n\nggplot(data = mtcars, mapping = aes(x = disp, y = wt)) +\n  geom_point() +\n  geom_line()\n\nまた，この用例の場合ggplot関数の第一引数はデータセットなので，パイプ演算子で渡すことができる。\n\nmtcars %&gt;%\n  ggplot(mapping = aes(x = disp, y = wt)) +\n  geom_point() +\n  geom_line()\n\nパイプ演算子を使うことで，素データをハンドリングし，必要な形に整えて可視化する，という流れがスクリプト上で読むように表現できるようになる。慣れてくると，データセットから可視化したい要素を特定し，最終的にどのように成形すればggplotに渡しやすくなるかを想像して加工していくようになる。そのためには到達目標となる図版のイメージを頭に描き，その図のx軸，y軸は何で，どのような幾何学オブジェクトが上に乗っているのか，といったように図版のリバースエンジニアリング，あるいは図版の作成手順の書き下しができる必要がある。たとえるなら，食べたい料理に必要な材料を集め，大まかな手順(下ごしらえからの調理)を組み立てられるかどうか，である。実際にレシピに書き起こす際は生成AIの力を借りると良いが，その際も最終的な目標と，全体的な設計方針から指示し，微調整を追加していくように指示すると効率的である。\n以下に，データハンドリングと描画の一例を示す。各ステップにコメントをつけたので，文章を読むように加工と描画の流れを確認し，出力結果と照らし合わせてみよう。\n\n# mtcarsデータセットを使用\nmtcars %&gt;%\n  # 変数選択\n  select(mpg, cyl, wt, am) %&gt;%\n  mutate(\n    # 変数am,cylをFactor型に変換\n    am = factor(am, labels = c(\"automatic\", \"manual\")),\n    cyl = factor(cyl)\n  ) %&gt;%\n  # 水準ごとにグループ化\n  group_by(am, cyl) %&gt;%\n  summarise(\n    M = mean(mpg), # 各グループの平均燃費（M）を計算\n    SD = sd(mpg), # 各グループの燃費の標準偏差（SD）を計算\n    .groups = \"drop\" # summarise後の自動的なグルーピングを解除\n  ) %&gt;%\n  # x軸にトランスミッションの種類、y軸に平均燃費，塗りつぶしの色はcyl\n  ggplot(aes(x = am, y = M, fill = cyl)) +\n  # 横並びの棒グラフ\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  # ±1SDのエラーバーを追加\n  geom_errorbar(\n    # エラーバーのマッピング\n    aes(ymin = M - SD, ymax = M + SD),\n    # エラーバーの位置を棒グラフに合わせる\n    position = position_dodge(width = 0.9),\n    width = 0.25 # エラーバーの幅を設定\n  )\n\n\n\n\n\n\n\n\n繰り返しになるが，このコードは慣れてくるまでいきなり書けるものではない。重要なのは「出力結果をイメージ」することと，それを「要素に分解」，「手順に沿って並べる」ことができるかどうかである。3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rによるレポートの作成</span>"
    ]
  },
  {
    "objectID": "chapter04.html#描画tips",
    "href": "chapter04.html#描画tips",
    "title": "4  Rによるレポートの作成",
    "section": "4.5 描画tips",
    "text": "4.5 描画tips\n最後に，いくつかの描画テクニックを述べておく。これらについては，必要な時に随時ウェブ上で検索したり，生成AIに尋ねることでも良いが，このような方法がある，という基礎知識を持っておくことも重要だろう。なお描画について詳しくは 松村 et al. (2021) の4章を参照すると良い。\n\n4.5.1 ggplotオブジェクトを並べる\n複数のプロットを一枚のパネルに配置したい，ということがあるかもしれない。先ほどのmtcarsデータの例でいえば，am変数にオートマチック車かマニュアル車かの2水準があるが，このようなサブグループごとに図を分割したいという場合である。\nこのような時には，facet_wrapやfacet_gridという関数が便利である。前者はある変数について，後者は2つの変数について図を分割する。\n\nmtcars %&gt;%\n  # 重さwtと燃費mpgの散布図\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point() +\n  # シリンダ数cylで分割\n  facet_wrap(~cyl, nrow = 2) +\n  # タイトルをつける\n  labs(caption = \"facet_wrapの例\")\n\n\n\n\n\n\n\n\n\nmtcars %&gt;%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point() +\n  # シリンダ数cylとギア数gearで分割\n  facet_grid(cyl ~ gear) +\n  # キャプションをつける\n  labs(caption = \"facet_gridの例\")\n\n\n\n\n\n\n\n\n一枚の図をサブグループに分けるのではなく，異なる図を一枚の図として赤痛いこともあるかもしれない。そのような場合は，patchworkパッケージを使うと便利である。\n\nlibrary(patchwork)\n\n# 散布図の作成\ng1 &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  # 散布図のタイトルとサブタイトル\n  ggtitle(\"Scatter Plot\", \"MPG vs Weight\")\n\n# 棒グラフの作成\ng2 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_bar(stat = \"identity\") +\n  # 棒グラフのタイトルとサブタイトル\n  ggtitle(\"Bar Chart\", \"Average MPG by Cylinder\")\n\n# patchworkを使用して2つのグラフを組み合わせる\ncombined_plot &lt;- g1 + g2 +\n  plot_annotation(\n    title = \"Combined Plots\",\n    subtitle = \"Scatter and Bar Charts\"\n  )\n\n# プロットを表示\nprint(combined_plot)\n\n\n\n\n\n\n\n\n\n\n4.5.2 ggplotオブジェクトの保存\nRmdやQuartoで文書を作るときは，図が自動的に生成されるので問題ないが，図だけ別のファイルとして利用したい，保存したいということがあるかもしれない。その時はggsave関数でggplotオブジェクトを保存するとよい。\n\n# 散布図を作成\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\nggsave(\n  filename = \"my_plot.png\", # 保存するファイル名。\n  plot = p, # 保存するプロットオブジェクト。\n  device = \"png\", # 保存するファイル形式。\n  path = \"path/to/directory\", # ファイルを保存するディレクトリのパス\n  scale = 1, # グラフィックスの拡大縮小比率\n  width = 5, # 保存するプロットの幅（インチ）\n  height = 5, # 保存するプロットの高さ（インチ）\n  dpi = 300, # 解像度（DPI: dots per inch）\n)\n\n\n\n4.5.3 テーマの変更（レポートに合わせる）\nレポートや論文などの提出次の条件として，図版をモノクロで表現しなければならないことがあるかもしれない。ggplotでは自動的に配色されるが，その背後ではデフォルトの絵の具セット(パレットという) が選択されているからである。このセットを変更すると，同じプロットでも異なる配色で出力される。モノクロ(グレイスケール)で出力したい時のパレットはGraysである。\n\n# グレースケールのプロット\np1 &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  scale_fill_brewer(palette = \"Greys\") +\n  ggtitle(\"Gray Palette\")\n\n# カラーパレットが多く含まれているパッケージの利用\nlibrary(RColorBrewer)\n# 色覚特性を考慮したカラーパレット\np2 &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  scale_color_brewer(palette = \"Set2\") + # 色覚特性を考慮したカラーパレット\n  ggtitle(\"Palette for Color Blind\")\n\n# 両方のプロットを並べて表示\ncombined_plot &lt;- p1 + p2 + plot_layout(ncol = 2)\nprint(combined_plot)\n\n\n\n\n\n\n\n\nまた，ggplot2のデフォルト設定では，背景色が灰色になっている。これは全体のテーマとしてtheme_gray()が設定されているからである。しかし日本心理学会の執筆・投稿の手引きに記載されているグラフの例を見ると，背景は白色とされている。このような設定に変更するためには，theme_classic()やtheme_bw()を用いる。\n\np2 + theme_classic()\n\n\n\n\n\n\n\n\nこのほかにも，様々な描画上の工夫は考えられる。目標となる図版のレシピを書き起こせるように，要素に分解ができれば，殆どのケースにおいて問題を解決することができるだろう。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rによるレポートの作成</span>"
    ]
  },
  {
    "objectID": "chapter04.html#課題",
    "href": "chapter04.html#課題",
    "title": "4  Rによるレポートの作成",
    "section": "4.6 課題",
    "text": "4.6 課題\n以下にデスマス調に整えた文章を示します。\n\n今日の課題はRmarkdownで記述してください。著者名に学籍番号と名前を含め、適宜見出しをつくり、平文で以下に挙げる課題を記載することでどの課題に対する回答のコード（チャンク）であるかわかるようにしてください。\n\n\nBaseball.csvを読み込み、2020年度のデータセットに限定し、以下の操作に必要であれば変数の変換を済ませたデータセット、dat.tbを用意してください。\ndat.tbの身長変数を使って、ヒストグラムを描いてください。この時、テーマをtheme_classicにしてください。\ndat.tbの身長変数と体重変数を使って、散布図を描いてください。この時、テーマをtheme_bwにしてください。\n(承前) 散布図の各点を血液型で塗り分けてください。この時、カラーパレットをSet3に変えてください。\n(承前) 散布図の点の形を血液型で変えてください。\ndat.tbの身長と体重についての散布図を、チームごとに分割してください。\n(承前) geom_smooth()でスムーズな線を引いてください。特にmethodを指定する必要はありません。\n(承前) geom_smooth()で直線関数を引いてください。method=\"lm\"と指定するといいでしょう。\nx軸は身長、y軸は体重の平均値をプロットしてください。方法はいろいろありますが、要約統計量を計算した別のデータセットdat.tb2を作るか、幾何学オブジェクトの中で、次のように関数を適用することもできます。ヒント：geom_point(stat=\"summary\", fun=mean)。\n課題2, 4および体重のヒストグラムを使って下の図を描き、ggsave関数を使って保存するコードを書いてください。ファイル名やその他オプションは任意です。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nキーラン・ヒーリー. (2018) 2021. データ分析のためのデータ可視化入門. Translated by 瓜生真也, 江口哲史, and 三村喬生. 講談社.\n\n\n松村優哉, 湯谷啓明, 紀ノ定保礼, and 前田和寛. 2021. 改訂2版 RユーザのためのRStudio[実践]入門: Tidyverseによるモダンな分析フローの世界. 技術評論社.\n\n\n高橋康介. 2018. 再現可能性のすゝめ. Edited by 石田基広. Vol. 3. Wonderful r. 共立出版.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rによるレポートの作成</span>"
    ]
  },
  {
    "objectID": "chapter04.html#footnotes",
    "href": "chapter04.html#footnotes",
    "title": "4  Rによるレポートの作成",
    "section": "",
    "text": "もし新しく開いているファイルに名前がつけられていないのなら(Untitledのままになっているようであれば),ファイル名の指定画面が開く。また環境によっては，初回実行時にコンパイルに必要な関連パッケージのダウンロードが求められることがある。↩︎\nもっとも，Rのバージョンやパッケージのバージョンによっては同じ計算結果が出ない可能性がある。より本質的な計算過程に違いがあるかもしれないのである。そのため，R本体やパッケージのバージョンごとパッキングして共有する工夫も考えられている。Dockerと呼ばれるシステムは，解析環境ごと保全し共有するシステムの例である。↩︎\n実際コードはchatGPTver4に指示して生成した。いきなり全体像を描くのではなく，徐々に追記していくと効果的である。↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rによるレポートの作成</span>"
    ]
  },
  {
    "objectID": "chapter05.html",
    "href": "chapter05.html",
    "title": "5  Rでプログラミング",
    "section": "",
    "text": "5.1 代入\n代入は，言い換えればオブジェクト(メモリ)に保管することを指す。これについては既に Chapter 2 で触れた通りであり，ここでは言及しない。オブジェクトや変数の型，常に上書きされる性質に注意しておけば十分だろう。\n一点だけ追加で説明しておくと，次のような表現がなされることがある。\na &lt;- 0\na &lt;- a + 1\nprint(a)\n\n[1] 1\nここではあえて，代入記号として=を使った。2行目にa = a + 1 とあるが，これを見て数式のように解釈しようとすると混乱する。数学的には明らかにおかしな表現だが，これは上書きと代入というプログラミング言語の特徴を使ったもので，「(いま保持している)aの値に1を加えたものを，(新しく同じ名前のオブジェクト)aに代入する(=上書きする)」という意味である。この方法で，a をカウンタ変数として用いることがある。誤読の可能性を下げるため，この授業においては代入記号を&lt;-としている。\nこのオブジェクトを上書きするという特徴は多くの言語に共通したものであり，間違いを避けるためには，オブジェクトを作る時に初期値を設定することが望ましい。先の例では，代入の直上でa &lt;- 0としており，オブジェクトa に0 を初期値として与えている。この変数の初期化作業がないと，以前に使っていた値を引き継いでしまう可能性があるので，今から新しく使う変数を作りたいというときは，このように明示しておくといいだろう。\nなお，変数をメモリから明示的に削除する場合は，remove関数を使う。\nremove(a)\nこれを実行すると，RStudioのEnvironmentタブからオブジェクトaが消えたことがわかるだろう。 メモリの一斉除去は，同じくRStudioのEnvironmentタブにある箒マークをクリックするか，remove(list=ls())とすると良い2。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rでプログラミング</span>"
    ]
  },
  {
    "objectID": "chapter05.html#反復",
    "href": "chapter05.html#反復",
    "title": "5  Rでプログラミング",
    "section": "5.2 反復",
    "text": "5.2 反復\n\n5.2.1 for文\n電子計算機の特徴は，電源等のハードウェア的問題がなければ疲労することなく計算を続けられるところにある。人間は反復によって疲労が溜まったり，集中力が欠如するなどして単純ミスを生成するが，電子計算機にそういったところはない。\nこのように反復計算は電子計算機の中心的特徴であり，細々した計算作業を指示した期間反復させ続けることができる。反復の代表的なコマンドはforであり，forループなどと呼ばれる。forループはプログラミングの基本的な制御構造であり，R言語のforループの基本的な構文は次のようになる：\nfor (value in sequence) {\n    # 実行するコード\n}\nここのvalueは各反復でsequenceの次の要素を取る反復インデックス変数である。。sequenceは一般にベクトルやリストなどの配列型のデータであり，「#実行するコード」はループ体内で実行される一連の命令になる。\n以下はfor文の例である。\n\nfor (i in 1:5) {\n  cat(\"現在の値は\", i, \"です。\\n\")\n}\n\n現在の値は 1 です。\n現在の値は 2 です。\n現在の値は 3 です。\n現在の値は 4 です。\n現在の値は 5 です。\n\n\nfor 文は続く小括弧のなかである変数を宣言し(ここではi)，それがどのように変化するか(ここでは1:5，すなわち1,2,3,4,5)を指定する。続く中括弧の中で，反復したい操作を記入する。今回はcat 文によるコンソールへの文字力の出力を行っている。ここでのコマンドは複数あってもよく，中括弧が閉じられるまで各行のコマンドが実行される。\n次に示すは，sequenceにあるベクトルが指定されているので，反復インデックス変数が連続的に変化しない例である。\n\nfor (i in c(2, 4, 12, 3, -6)) {\n  cat(\"現在の値は\", i, \"です。\\n\")\n}\n\n現在の値は 2 です。\n現在の値は 4 です。\n現在の値は 12 です。\n現在の値は 3 です。\n現在の値は -6 です。\n\n\nまた，反復はネスト(入れ子)になることもできる。次の例を見てみよう。\n\n# 2次元の行列を定義\nA &lt;- matrix(1:9, nrow = 3)\n\n# 行ごとにループ\nfor (i in 1:nrow(A)) {\n  # 列ごとにループ\n  for (j in 1:ncol(A)) {\n    cat(\"要素 [\", i, \", \", j, \"]は \", A[i, j], \"\\n\")\n  }\n}\n\n要素 [ 1 ,  1 ]は  1 \n要素 [ 1 ,  2 ]は  4 \n要素 [ 1 ,  3 ]は  7 \n要素 [ 2 ,  1 ]は  2 \n要素 [ 2 ,  2 ]は  5 \n要素 [ 2 ,  3 ]は  8 \n要素 [ 3 ,  1 ]は  3 \n要素 [ 3 ,  2 ]は  6 \n要素 [ 3 ,  3 ]は  9 \n\n\nここで，反復インデックス変数がiとjというように異なる名称になっていることに注意しよう。例えば今回，ここで両者をiにしてしまうと，行変数なのか列変数なのかわからなくなってしまう。また少し専門的になるが，R言語はfor文で宣言されるたびに，内部で反復インデックス変数を新しく生成している(異なるメモリを割り当てる)ためにエラーにならないが，他言語の場合は同じ名前のオブジェクトと判断されることが一般的であり，その際は値が終了値に到達せず計算が終わらないといったバグを引き起こす。反復に使う汎用的な変数名としてi,j,kがよく用いられるため，自身のスクリプトの中でオブジェクト名として単純な一文字にすることは避けた方がいいだろう。\n\n\n5.2.2 while文\nwhileループはプログラミングの基本構造であり，特定の条件が真（True）である間，繰り返し一連の命令を実行する。「“while”（～する間）」という名前から直感的に理解できるだろう。\nR言語のwhileループの基本的な構文は次のようになる：\nwhile (condition) {\n    # 実行するコード\n}\nここで，「condition」はループが終了するための条件である。「# 実行するコード」はループ体内で実行される一連の指示である。たとえば，1から10までの値を出力するwhileループは以下のように書くことができる：\n\ni &lt;- 1\nwhile (i &lt;= 5) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nこのコードでは，「i」が5以下である限りループが続く。「print(i)」で「i」の値が表示され，「i &lt;- i + 1」で「i」の値が1ずつ増加する。これにより，「i」の値が10を超えると条件が偽(False)となり，ループが終了する。\nwhileループを使用する際の一般的な注意点は，無限ループ（終わらないループ）を避けることである。これは，conditionが常に真(True)である場合に発生する。そのような状況を避けるためには，ループ内部で何らかの形でconditionが最終的に偽(False)となるようにコードを記述することが必要である。\nまた，R言語は他の多くのプログラミング言語と異なり，ベクトル化された計算を効率的に行う設計がされている。したがって，可能な限りforループやwhileループを使わずに，ベクトル化した表現を利用すれば計算速度を上げることができる。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rでプログラミング</span>"
    ]
  },
  {
    "objectID": "chapter05.html#条件分岐",
    "href": "chapter05.html#条件分岐",
    "title": "5  Rでプログラミング",
    "section": "5.3 条件分岐",
    "text": "5.3 条件分岐\n条件分岐はプログラム内で特定の条件を指定し，その条件が満たされるかどうかによって異なる処理を行うための制御構造である。R言語では if-else を用いて条件分岐を表現する。\n\n5.3.1 if 文の基本的な構文\n以下が if 文の基本的な構文になる：\nif (条件) {\n    # 条件が真である場合に実行するコード\n}\nif の後の小括弧内に条件を指定する。この条件が真(TRU)であれば，その後の 中括弧{} 内のコードが実行される。さらに，else を使用して，条件が偽（FALSE）の場合の処理を追加することもできる：\nif (条件) {\n    # 条件が真である場合に実行するコード\n} else {\n    # 条件が偽である場合に実行するコード\n}\n以下に具体的な使用例を示そう：\n\nx &lt;- 10\n\nif (x &gt; 0) {\n  print(\"x is positive\")\n} else {\n  print(\"x is not positive\")\n}\n\n[1] \"x is positive\"\n\n\nこのコードでは，変数 x が正の場合とそうでない場合で異なるメッセージを出力する。\n条件は論理式（例：x &gt; 0, y == 1）や論理値（TRUE/FALSE）を返す関数・操作（例：is.numeric(x)）などで指定する。また，複数の条件を組み合わせる際には論理演算子(&&, ||)を使用する。\nこの例では，xが正とyが負の場合に特定のメッセージを出力する。それ以外の場合は，「Other case」と出力される。xやyの値を色々変えて，試してみて欲しい。\n\nx &lt;- 10\ny &lt;- -3\n\nif (x &gt; 0 && y &lt; 0) {\n  print(\"x is positive and y is negative\")\n} else {\n  print(\"Other case\")\n}\n\n[1] \"x is positive and y is negative\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rでプログラミング</span>"
    ]
  },
  {
    "objectID": "chapter05.html#反復と条件分岐に関する練習問題",
    "href": "chapter05.html#反復と条件分岐に関する練習問題",
    "title": "5  Rでプログラミング",
    "section": "5.4 反復と条件分岐に関する練習問題",
    "text": "5.4 反復と条件分岐に関する練習問題\n\n1から20までの数字で，偶数だけをプリントするプログラムを書いてください。\n1から40までの数値をプリントするプログラムを書いてください。ただしその数値に3がつく(1か10の位の値が3である)か，3の倍数の時だけ，数字の後ろに「サァン！」という文字列をつけて出力してください。\nベクトル c(1, -2, 3, -4, 5) の各要素について，正なら “positive”，負なら “negative” をプリントするプログラムを書いてください。\n次の行列\\(A\\)と\\(B\\)の掛け算を計算するプログラムを書いてください。なお，Rで行列の積は%*%という演算子を使いますが，ここではfor文を使ったプログラムにしてください。出来上がる行列の\\(i\\)行\\(j\\)列目の要素\\(c_{ij}\\)は，行列\\(A\\)の第\\(i\\)行の各要素と，行列\\(B\\)の第\\(j\\)列目の各要素の積和，すなわち\\[c_{ij}=\\sum_{k} a_{ik}b_{kj}\\]になります。検算用のコードを下に示します。\n\n\nA &lt;- matrix(1:6, nrow = 3)\nB &lt;- matrix(3:10, nrow = 2)\n## 課題になる行列\nprint(A)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\nprint(B)\n\n     [,1] [,2] [,3] [,4]\n[1,]    3    5    7    9\n[2,]    4    6    8   10\n\n## 求めるべき答え\nC &lt;- A %*% B\nprint(C)\n\n     [,1] [,2] [,3] [,4]\n[1,]   19   29   39   49\n[2,]   26   40   54   68\n[3,]   33   51   69   87",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rでプログラミング</span>"
    ]
  },
  {
    "objectID": "chapter05.html#関数を作る",
    "href": "chapter05.html#関数を作る",
    "title": "5  Rでプログラミング",
    "section": "5.5 関数を作る",
    "text": "5.5 関数を作る\n複雑なプログラムも，ここまでの代入、反復，条件分岐の組み合わせからなる。回帰分析や因子分析のような統計モデルを実行するときに，統計パッケージのユーザとしては，統計モデルを実現してくれる関数にデータを与えて答えを受け取るだけであるが，そのアルゴリズムはこれらプログラミングのピースを紡いでつくられているのである。\nここでは関数を自分で作ることを考える。といっても身構える必要はない。表計算ソフトウェアで同じような操作を繰り返すときにマクロに記録するように，R上で同じようなコードを何度も書く機会があるならば，それを関数という名のパッケージにしておこう，ということである。関数化しておくことで手続きをまとめることができ，小単位に分割できるため並列して開発したり，バグを見つけやすくなるという利点がある。\n\n5.5.1 基本的な関数の作り方\n関数が受け取る値のことを引数(ひきすう，argument)といい，また関数が返す値のことを戻り値(もどりち,value)という。\\(y=f(x)\\)という式は，引数がxで戻り値がyな関数\\(f\\)，と言い換えることができるだろう。\nRの関数を書く基本的な構文は以下のようになる。\nfunction_name &lt;- function(argument) {\n   # function body\n   return(value)\n}\nここでfunction bodyとあるのは計算本体である。例えば与えられた数字に3を足して返す関数，add3を作ってみよう。プログラムは以下のようになる。\n\nadd3 &lt;- function(x) {\n  x &lt;- x + 3\n  return(x)\n}\n# 実行例\nadd3(5)\n\n[1] 8\n\n\nまた，2つの値を足し合わせる関数は次のようになる。\n\nadd_numbers &lt;- function(a, b) {\n  sum &lt;- a + b\n  return(sum)\n}\n# 実行例\nadd_numbers(2, 5)\n\n[1] 7\n\n\nここで示したように，引数は複数取ることも可能である。また，既定値default valueを設定することも可能である。次の例を見てみよう。\n\nadd_numbers2 &lt;- function(a, b = 1) {\n  sum &lt;- a + b\n  return(sum)\n}\n# 実行例\nadd_numbers2(2, 5)\n\n[1] 7\n\nadd_numbers2(4)\n\n[1] 5\n\n\n関数を作るときに，(a,b=1)としているのは，bに既定値として1を与えていて，特に指定がなければこの値を使うよう指示しているということである。実行例において，引数が2つ与えられている場合はそれらを使った計算をし(2+5)，1つしか与えられていない場合は第一引数aに与えられた値を，第二引数bは既定値を使った計算をする(4+1)，という挙動になる。\nここから推察できるように，われわれユーザが使う統計パッケージの関数にも実は多くの引数があり，既定値が与えられているということだ。これらは選択的に，あるいは能動的に与えることができるものであるが，これらの引数は選択的に指定することができるのだが，通常は一般的に使われる値や計算の細かな設定に関するものであり，開発者がユーザの手間を省くために提供しているものである。関数のヘルプを見ると指定可能な引数の一覧が表示されるので，ぜひ興味を持って見てもらいたい。\n\n\n5.5.2 複数の戻り値\nRでの戻り値は1つのオブジェクトでなければならない。しかし，複数の値を返したいということがあるだろう。そのような場合は，返すオブジェクトをlistなどでひとまとめにして作成すると良い。以下に簡単な例を示す。\n\ncalculate_values &lt;- function(a, b) {\n  sum &lt;- a + b\n  diff &lt;- a - b\n  # 戻り値として名前付きリストを作成\n  result &lt;- list(\"sum\" = sum, \"diff\" = diff)\n  return(result)\n}\n# 実行例\nresult &lt;- calculate_values(10, 5)\n# 結果を表示\nprint(result)\n\n$sum\n[1] 15\n\n$diff\n[1] 5",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rでプログラミング</span>"
    ]
  },
  {
    "objectID": "chapter05.html#課題",
    "href": "chapter05.html#課題",
    "title": "5  Rでプログラミング",
    "section": "5.6 課題",
    "text": "5.6 課題\n\nある値を与えたとき，正の値なら”positive”，負の値なら”negative”，0のときは”Zero”と表示する関数を書いてください。\nある2組の数字を与えた時，和，差，積，商を返す関数を書いてください。\nあるベクトルを与えた時，算術平均，中央値，最大値，最小値，範囲を返す関数を書いてください。\nあるベクトルを与えた時，標本分散を返す関数を書いてください。なおRの分散を返す関数varは不偏分散\\(\\hat{\\sigma}\\)を返しており，標本分散vとは計算式が異なります。念のため，計算式を以下に示します。 \\[\\hat{\\sigma} = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2 \\] \\[v= \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2 \\]\n\n\n\n\n\nシ. 2016. 計算機言語のまとめノート. 暗黒通信団.\n\n\nランダー，J.P. (2017) 2018. みんなのr 第2版. Translated by 高柳慎一, 津田真樹, 牧山幸史, 松村杏子, and 簑田高志. マイナビ出版.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.\n\n\n株式会社ホクソエム, trans. (2016) 2017. Rプログラミング本格入門: 達人データサイエンティストへの道. 単行本. 共立出版.\n\n\n石田基広, 市川太祐, 高柳慎一, and 福島真太朗, trans. (2015) 2016. R言語徹底解説. 共立出版.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rでプログラミング</span>"
    ]
  },
  {
    "objectID": "chapter05.html#footnotes",
    "href": "chapter05.html#footnotes",
    "title": "5  Rでプログラミング",
    "section": "",
    "text": "シ (2016) には117種もの計算機言語が紹介されている。↩︎\nls()関数はlist objectsの意味で，メモリにあるオブジェクトのリストを作る関数↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rでプログラミング</span>"
    ]
  },
  {
    "objectID": "chapter06.html",
    "href": "chapter06.html",
    "title": "6  確率とシミュレーション",
    "section": "",
    "text": "6.1 確率の考え方と使い所\n統計と確率は密接な関係がある。 まずデータをたくさん集めると，個々のケースでは見られない全体的な傾向が見られるようになり，それを表現するのに確率の考え方を使う，というのがひとつ。 次にデータがそれほどたくさんなくとも，大きな全体の中から一部を取り出した標本Sampleと考えられるとき，標本は全体の性質をどのように反映しているかを考えることになる。ここで全体の傾向から一部を取り出した偶然性を表現するときに確率の考え方を使うことになる。 最後に，理論的・原理的に挙動がわかっている機械のようなものでも，現実的・実践的には系統だったズレが生じたり，偶然としか考えられない誤差が紛れ込むことがある。前者は機械の調整で対応できるが，後者は偶然が従う確率を考える必要がある。\n心理学は人間を対象に研究を行うが，あらゆる人間を一度に調べるわけにはいかないので，サンプルを取り出して調査したり実験したりする(第2のケース)。データサイエンスでは何万レコードというおおきなデータセットになるが，心理学の場合は数件から数十件しかないことも多い。また，心理学的傾向を理論立ててモデル化できたとしても，実際の行動には誤差が含まれている可能性が高い(第3のケース)。このことから，心理学で得られるデータは確率変数として考えられ，小標本から母集団の性質を推測する推測統計と共に利用される。\n厳密に数学的な意味での確率は，集合，積分，測度といった緻密な概念の積み重ねから定義される1。ここではその詳細に分け入らず，単に「特定の結果が生じる可能性について，0から1の間の実数でその大小を表現したもの」とだけ理解しておいて欲しい。この定義からは，「全ての可能な組み合わせのうち当該事象の成立する割合」という解釈も成り立つし，「主観的に重みづけた真実味の強さに関する信念の度合い」という解釈も成り立つ。2 これまで学んできた確率は順列・組み合わせを全て書き出す退屈なもの，と思っていたかもしれないが，「十中八九まちがいないね(80-90%ほど確からしいと考えている)」という数字も確率の一種として扱えるので，非常に身近で適用範囲の広い概念である。理解を進めるポイントの1つとして，確率を面積として考えると良いかもしれない。ありうる状況の全体の空間に対して，事象の成立する程度がどの程度の面積がどの程度の割合であるかを表現したのが確率という量である，と考えるのである( 平岡 and 堀 (2009) は書籍の中で一貫して面積で説明している。この説明だと，条件付き確率などの理解がしやすい。)。\nただし注意して区別しておいて欲しいのが，確率変数とその実現値の違いである。データセットやスプレッドシートに含まれる値は，あくまでも確率変数の実現値というのであって，確率変数はその不確実な状態を有した変数そのものを指す言葉である。サイコロは確率変数だが，サイコロの出目は確率変数の実現値である。心理変数は確率変数だが，手に入れたデータはその実現値である。実現値を通じて変数の特徴を知り，全体を推測するという流れである。\n目の前のデータを超えて，抽象的な実体で議論を進めることが難しく感じられるかもしれない。実は誰しもそうなのであって，確率の正確な理解は非常に難易度が高い。しかしRなど計算機言語に実装されている関数を通じて，より具体的に，操作しながら理解することで徐々に理解していこう。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter06.html#確率分布の関数",
    "href": "chapter06.html#確率分布の関数",
    "title": "6  確率とシミュレーション",
    "section": "6.2 確率分布の関数",
    "text": "6.2 確率分布の関数\n確率変数の実現値は，確率分布に従う。確率分布とは，その実現値がどの程度生じやすいかを全て表した総覧であり，一般的に関数で表現される。実現値が連続的か離散的かによって名称が異なるが，連続的な確率分布関数は確率密度関数(Probability Density Function)，離散的な確率分布関数は確率質量関数(Probability Mass Function)という。\nRには最初から確率に関する関数がいくつか準備されている。最も有名な確率分布である正規分布について，次のような関数がある。\n\n# 標準のプロット関数，curve\ncurve(dnorm(x), from = -4, to = 4)\n\n\n\n\n\n\n\n\n\n# ggplot2を使ってカッコよく\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata.frame(x = seq(-4, 4, by = 0.01)) %&gt;%\n  mutate(y = dnorm(x)) %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  theme_classic()\n\n\n\n\n\n\n\n\nここでdnormという関数を使っているが，dはDensity(確率密度)の頭文字であり，normはNormal Distribution(正規分布)の一部である。このように，Rでは確率分布の名前を表す名称(ここではnorm)と，それに接頭文字ひとつ(d)で関数を構成する。この接頭文字は他にp,q,rがあり，dpois(ポアソン分布poisson distributionの確率密度関数)，pnorm(正規分布normal distributionの累積分布関数),rbinom(二項分布binomial distributionからの乱数生成)のように使う。\nここでは正規分布を例に説明を続けよう。正規分布は平均\\(\\mu\\)と標準偏差\\(\\sigma\\)でその形状が特徴づけられる。これらの確率分布の特徴を表す数字のことを母数 parameterという。たとえば，次の3つの曲線はパラメータが異なる正規分布である。\n\ndata.frame(x = seq(-4, 4, by = 0.01)) %&gt;%\n  mutate(\n    y1 = dnorm(x, mean = 0, sd = 1),\n    y2 = dnorm(x, mean = 1, sd = 0.5),\n    y3 = dnorm(x, mean = -1, sd = 2)\n  ) %&gt;%\n  pivot_longer(-x) %&gt;%\n  ggplot(aes(x = x, y = value, color = name)) +\n  geom_line()\n\n\n\n\n\n\n\n\n平均は位置母数，標準偏差はスケール母数とも呼ばれ，分布の位置と幅を変えていることがわかる。言い換えると，データになるべく当てはまるように正規分布の母数を定めることもできるわけで，左右対称で単峰の分布という特徴があれば，正規分布でかなり様々なパターンを表せる。\nさて，上の例で用いた関数はいずれもdを頭に持つdnormであり，確率分布の密度の高さを表現していた。ではpやqが表すのは何であろうか。数値と図の例を示すので，その対応関係を確認してもらいたい。\n\n# 累積分布関数\npnorm(1.96, mean = 0, sd = 1)\n\n[1] 0.9750021\n\n# 累積分布の逆関数\nqnorm(0.975, mean = 0, sd = 1)\n\n[1] 1.959964\n\n\n数値で直感的にわかりにくい場合，次の図を見て確認しよう。pnorm関数はx座標の値を与えると，そこまでの面積(以下のコードで描かれる色付きの領域)すなわち確率を返す。qnorm関数は確率(=面積)を与えると，確率密度関数のカーブの下領域を積分してその値になるときのx座標の値を返す。\n\n# 描画\nprob &lt;- 0.9\n## 全体の正規分布カーブ\ndf1 &lt;- data.frame(x = seq(from = -4, 4, by = 0.01)) %&gt;%\n  mutate(y = dnorm(x, mean = 0, sd = 1))\n## qnorm(0.975)までのデータ\ndf2 &lt;- data.frame(x = seq(from = -4, qnorm(prob), by = 0.01)) %&gt;%\n  mutate(y = dnorm(x, mean = 0, sd = 1))\n## データセットの違いに注意\nggplot() +\n  geom_line(data = df1, aes(x = x, y = y)) +\n  geom_ribbon(data = df2, aes(x = x, y = y, ymin = 0, ymax = y), fill = \"blue\", alpha = 0.3) +\n  ## 以下装飾\n  geom_segment(\n    aes(x = qnorm(prob), y = dnorm(qnorm(prob)), xend = qnorm(prob), yend = 0),\n    arrow = arrow(length = unit(0.2, \"cm\")), color = \"red\"\n  )\n\n\n\n\n\n\n\n\nd,p,q,rといった頭の文字は，他の確率分布関数にも付く。では次にrについて説明しよう。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter06.html#乱数",
    "href": "chapter06.html#乱数",
    "title": "6  確率とシミュレーション",
    "section": "6.3 乱数",
    "text": "6.3 乱数\n乱数とは何であるかを説明するのは，「ランダムである(確率変数である)とは如何なることか」を説明するのと同じように難しい。 カンタンに説明するなら，規則性のない数列という意味である。 しかし計算機はアルゴリズムに沿って正しく数値を計算するものだから，ランダムに，規則性がない数字を示すということは厳密にはあり得ない。 計算機が出す乱数は，乱数生成アルゴリズムに沿って出される数字であり，ランダムに見えて実は規則性があるので，疑似乱数というのが正しい。\nとはいえ，人間が適当な数字を思いつきで誦じていく3よりは，よほど規則性がない数列を出すので，疑似的とはいえ十分に役に立つ。 たとえばアプリなどで「ガチャ」を引くというのは，内部で乱数によって数値を出し，それに基づいてあたり・ハズレ等の判定をしている。他にも，RPGなどで攻撃する時に一定の確率で失敗するとか，一定の確率で「会心の一撃」を出すというのも同様である。ここで大事なのは，そうしたゲームへの実装において規則性のない数字に基づくプログラムにしたとしても，その統計的な性質，すなわち実現値の出現確率はある程度制御したいのである。\nそこで，ある確率分布に基づく乱数を生成したい，ということになる。幸いにして，一様乱数(全ての実現値が等しい確率で生じる)を関数で変換することで，正規分布ほか様々な確率分布に従う乱数を作ることができる。Rにはその基本関数として幾つかの確率分布に従う乱数が実装されている。たとえば次のコードは，平均50，SD10の正規分布に従う乱数を10個出現させるものである。\n\nrnorm(n = 10, mean = 50, sd = 10)\n\n [1] 72.34636 44.57886 60.97939 45.58851 46.00126 55.37714 48.76385 29.24080\n [9] 56.24563 64.28728\n\n\nたとえば諸君が心理統計の練習問題を作ろうとして，適当な数列が欲しければこのようにすれば良いかもしれない。しかし，同じ問題をもう一度作ろうとすると，乱数なのでまた違う数字が出てしまう。\n\nrnorm(n = 10, mean = 50, sd = 10)\n\n [1] 42.20988 44.87213 53.02336 47.11919 39.72257 40.28535 43.01383 55.40406\n [9] 37.37935 69.34737\n\n\n疑似乱数に過ぎないのだから，再現性のある乱数を生じさせたいと思うかもしれない。そのような場合は，set.seed関数を使う。疑似乱数は内部の乱数生成の種(seed)から計算して作られているため，その数字を固定してやると同じ乱数が再現できる。\n\n# seedを指定\nset.seed(12345)\nrnorm(n = 3)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\n# 同じseedを再設定\nset.seed(12345)\nrnorm(n = 3)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\n\n\n6.3.1 乱数のつかいかた\n乱数の使い方のひとつは，先に述べたように，プログラムが偶然による振る舞いをしているように仕掛けたいとき，ということだろう。\n実は他にも使い道がある。それは確率分布を具体的に知りたいときである。次に示すのは，標準正規分布から\\(n = 10,100,1000,10000\\)とした時のヒストグラムである。\n\nrN10 &lt;- rnorm(10)\nrN100 &lt;- rnorm(100)\nrN1000 &lt;- rnorm(1000)\nrN10000 &lt;- rnorm(10000)\n\ndata.frame(\n  N = c(\n    rep(1, 10), rep(2, 100),\n    rep(3, 1000), rep(4, 10000)\n  ),\n  X = c(rN10, rN100, rN1000, rN10000)\n) %&gt;%\n  mutate(N = as.factor(N)) %&gt;%\n  ggplot(aes(x = X, fill = N)) +\n  # 縦軸を相対頻度に\n  geom_histogram(aes(y = ..density..)) +\n  facet_wrap(~N)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nこれを見ると，最初の10個程度のヒストグラムは不規則な分布に見えるが，100,1000と増えるに従って徐々に正規分布の理論的形状に近似していくところがみて取れる。\nRにはポアソン分布や二項分布などに加え，統計に馴染みの深いt分布やF分布，\\(\\chi^2\\)分布などの確率分布関数も実装されている。これらの分布はパラメタの値を聞いてもイメージしにくいところがあるかもしれないが，そのような時はパラメタを指定した上で乱数を大量に生成し，そのヒストグラムを描けば確率分布関数の形が眼に見えてくるため，より具体的に理解できるだろう。\n実際，ベイズ統計学が昨今隆盛している一つの理由は，計算機科学の貢献によるところが大きい。マルコフ連鎖モンテカルロ法(MCMC法)と呼ばれる乱数発生技術は，明確な名前を持たないモデルによって作られる事後分布からでも，乱数を生成できる技術である。この分布は解析的に示すことは困難であるが，そこから乱数を生成し，そのヒストグラムを見ることで，形状を可視化できるのである。\nまた，この乱数利用法の利点は可視化だけではない。標準正規分布において，ある範囲の面積(=確率)が知りたいとする。たとえば，確率点-1.5から+1.5までの範囲の面積を求めたいとしよう。正規分布の数式はわかっているので，次のようにすればその面積は求められる。 \\[ p = \\int_{-1.5}^{+1.5} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}} dx \\]\nもちろん我々はpnorm関数を知っているので，次のようにして数値解を得ることができる。\n\npnorm(+1.5, mean = 0, sd = 1) - pnorm(-1.5, mean = 0, sd = 1)\n\n[1] 0.8663856\n\n\n同様のことは乱数を使って，次のように近似解を得ることができる。\n\nx &lt;- rnorm(100000, mean = 0, sd = 1)\ndf &lt;- data.frame(X = x) %&gt;%\n  # 該当する範囲かどうかを判定する変数を作る\n  mutate(FLG = ifelse(X &gt; -1.5 & X &lt; 1.5, 1, 2)) %&gt;%\n  mutate(FLG = factor(FLG, labels = c(\"in\", \"out\")))\n## 計算\ndf %&gt;%\n  group_by(FLG) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(prob = n / 100000)\n\n# A tibble: 2 × 3\n  FLG       n  prob\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 in    86642 0.866\n2 out   13358 0.134\n\n\nここでは乱数を10,000個生成し，指定の範囲内に入るかどうか(入れば1,入らなければ2)を示すfactor型変数FLGを作った。この変数ごとに群分けして数を数え，総数で割ることで相対度数にする。確率は全体の中に占める相対的な面積の割合であり，今回当該領域の値が0.866とpnorm関数で算出した解とほぼ同等の値変えられている。\nなお，次のようにすれば範囲の可視化も容易い。\n\n## 可視化\ndf %&gt;%\n  ggplot(aes(x = X, fill = FLG)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\n\n\n\n\n繰り返すが，確率分布の形がイメージできなかったり，解析的にその式を書き表すことが困難であった場合でも，具体的な数値にすることでヒストグラムで可視化でき，また近似的に確率計算ができている。\nあくまでも近似に過ぎないのでその精度が信用できない，というひとは生成する乱数の数を10倍，100倍にすれば良い。昨今の計算機の計算能力において，その程度の増加はさほど計算料の負担にならない。複雑な積分計算が記述統計量(数え上げ)の問題になる点で，具体的に理解できるという利点は大きい。\nさらに思いを馳せてほしいのだが，心理学者は心理学実験や調査によって，データを得る。しかしそれらは個人差や誤差を考え，確率変数だとされている。目の前の数件から数十件のデータであっても，正規分布に従うと仮定して統計的処理をおこなう。これは「乱数によって生成したデータ」に対して行うとしても本質的には同じである。すなわち，調査実験を行う前に，乱数によってシミュレーションしておくことができるのである。調査実験の本番一発勝負をする前に，自分の取ろうとしているデータがどのような性質を持ちうるかを具体的に確かめておくことは重要な試みであろう。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter06.html#練習問題乱数を用いて",
    "href": "chapter06.html#練習問題乱数を用いて",
    "title": "6  確率とシミュレーション",
    "section": "6.4 練習問題；乱数を用いて",
    "text": "6.4 練習問題；乱数を用いて\n正規乱数を用いて，次の値を近似計算してみよう。なお設定や解析的に算出した「真の値」と少数以下2位までの精度が得られるように工夫しよう。\n\n平均100,標準偏差8の正規分布の期待値。なお連続確率変数の期待値は次の式で表されます。\\[E[X] = \\int_{-\\infty}^{\\infty} x f(x) \\, dx\\] ここで\\(x\\)は確率変数を表し，\\(f(x)\\)は確率密度関数であり，確率密度関数の全定義域を積分することで得られます。正規分布の期待値は，平均パラメータに一致しますので，今回の真値は設定した\\(100\\)になります。\n平均100,標準偏差3の正規分布の分散を計算してみよう。なお連続確率変数の分散は次の式で表されます。\\[\\sigma^2 = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\, dx\\] ここで\\(\\mu\\)は確率変数の期待値であり，正規分布の分散は，標準偏差パラメータの二乗に一致しますので，今回の真値は\\(3^2 = 9\\)です。\n平均65，標準偏差10の正規分布に従う確率変数\\(X\\)の，\\(90 &lt; X &lt; 110\\)の面積。解析的に計算した結果は次の通りです。\n\n\npnorm(110, mean = 65, sd = 10) - pnorm(90, mean = 65, sd = 10)\n\n[1] 0.006206268\n\n\n\n平均10，標準偏差10の正規分布において，実現値が7以上になる確率。解析的に計算した結果は次の通りです。\n\n\n1 - pnorm(7, mean = 10, sd = 10)\n\n[1] 0.6179114\n\n\n\n確率変数\\(X,Y\\)があります。\\(X\\)は平均10,SD10の正規分布，\\(Y\\)は平均5，SD8の正規分布に従うものとします。ここで，\\(X\\)と\\(Y\\)が独立であるとしたとき，和\\(Z=X+Y\\)の平均と分散が，もとの\\(X,Y\\)の平均の和，分散の和になっていることを，乱数を使って確認してください。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter06.html#母集団と標本",
    "href": "chapter06.html#母集団と標本",
    "title": "6  確率とシミュレーション",
    "section": "6.5 母集団と標本",
    "text": "6.5 母集団と標本\nここまで確率分布の性質を見るために乱数を利用する方法を見てきた。ここからは，推測統計学における確率分布の利用を考える。推測統計では，知りたい集団全体のことを母集団population，そこから得られた一部のデータを標本sampleと呼ぶのであった。標本の統計量を使って，母集団の性質を推論するのが推測統計/統計的推測である。母集団の特徴を表す統計量は母数parameterと呼ばれ，母平均，母分散など「母」の字をつけて母集団の情報であることを示す。同様に，標本の平均や分散も計算できるが，この時は標本平均，標本分散など「標本」をつけて明示的に違いを強調することもある。\n乱数を使って具体的な例で見てみよう。ここに100人から構成される村があったとする。この村の人々の身長を測ってデータにしたとしよう。100個の適当な数字を考えるのは面倒なので，乱数で生成してこれに代える。\n\nset.seed(12345)\n# 100人分の身長データをつくる。小数点以下2桁を丸めた\nPo &lt;- rnorm(100, mean = 150, sd = 10) %&gt;% round(2)\nprint(Po)\n\n  [1] 155.86 157.09 148.91 145.47 156.06 131.82 156.30 147.24 147.16 140.81\n [11] 148.84 168.17 153.71 155.20 142.49 158.17 141.14 146.68 161.21 152.99\n [21] 157.80 164.56 143.56 134.47 134.02 168.05 145.18 156.20 156.12 148.38\n [31] 158.12 171.97 170.49 166.32 152.54 154.91 146.76 133.38 167.68 150.26\n [41] 161.29 126.20 139.40 159.37 158.54 164.61 135.87 155.67 155.83 136.93\n [51] 144.60 169.48 150.54 153.52 143.29 152.78 156.91 158.24 171.45 126.53\n [61] 151.50 136.57 155.53 165.90 144.13 131.68 158.88 165.93 155.17 137.04\n [71] 150.55 142.15 139.51 173.31 164.03 159.43 158.26 141.88 154.76 160.21\n [81] 156.45 160.43 146.96 174.77 159.71 168.67 156.72 146.92 155.37 158.25\n [91] 140.36 141.45 168.87 146.08 140.19 156.87 144.95 171.58 144.00 143.05\n\n\nこの100人の村が母集団なので，母平均や母分散は次のようにして計算できる。\n\nM &lt;- mean(Po)\nV &lt;- mean((Po - M)^2)\n# 母平均\nprint(M)\n\n[1] 152.4521\n\n# 母分散\nprint(V)\n\n[1] 123.0206\n\n\nさて，この村からランダムに10人の標本を得たとしよう。ベクトルの前から10人でも良いが，Rにはサンプリングをする関数sampleがあるのでこれを活用する。\n\ns1 &lt;- sample(Po, size = 10)\ns1\n\n [1] 164.61 155.86 136.93 143.29 160.43 168.87 151.50 155.17 153.71 135.87\n\n\nこのs1が手元のデータである。心理学の実験でデータを得る，というのはこのように全体に対してごく一部だけ取り出したものになる。このサンプルの平均や分散は標本平均，標本分散である。\n\nm1 &lt;- mean(s1)\nv1 &lt;- mean((s1 - mean(s1))^2)\n# 標本平均\nprint(m1)\n\n[1] 152.624\n\n# 標本分散\nprint(v1)\n\n[1] 110.2049\n\n\n今回，母平均は152.4521で標本平均は152.624である。実際に知りうる値は標本の値だけなので，標本平均152.624を得たら，母平均も152.624に近い値だろうな，と推測するのはおかしなことではないだろう。しかし標本平均は，標本の取り方によって毎回変わるものである。試しにもう一つ，標本をとったとしよう。\n\ns2 &lt;- sample(Po, size = 10)\ns2\n\n [1] 154.76 135.87 143.05 171.45 136.57 170.49 156.87 158.25 155.17 155.20\n\nm2 &lt;- mean(s2)\nv2 &lt;- mean((s2 - mean(s2))^2)\n# 標本平均その2\nprint(m2)\n\n[1] 153.768\n\n\n今回の標本平均は153.768になった。このデータが得られたら，諸君は母平均が「153.768に近い値だろうな」と推測するに違いない。標本1の152.624と標本2の153.768を比べると，前者の方が正解152.4521に近い(その差はそれぞれ-0.1719と-1.3159である)。つまり，標本の取り方によっては当たり外れがあるということである。データをとって研究していても，仮説を支持する結果なのかそうでないのかは，こうした確率的揺らぎの下にある。\nつまり，標本は確率変数であり，標本統計量も確率的に変わりうるものである。標本統計量でもって母数を推定するときは，標本統計量の性質や標本統計量が従う確率分布を知っておく必要がある。以下では母数の推定に望ましい性質を持つ推定量の望ましい性質をみていこう。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter06.html#一致性",
    "href": "chapter06.html#一致性",
    "title": "6  確率とシミュレーション",
    "section": "6.6 一致性",
    "text": "6.6 一致性\n最も単純には，標本統計量が母数に近ければ近いほど，できれば一致してくれれば喜ばしい。先ほどの例では100人の村から10人しか取り出さなかったが，もし20人，30人とサンプルサイズが大きくなると母数に近づいていくことが予想できる。この性質のことを一致性consistencyといい，推定量が持っていてほしい性質のひとつである。幸い，標本平均は母平均に対して一致性を持っている。\nこのことを確認してみよう。サンプルサイズを様々に変えて計算してみれば良い。例として，平均50,SD10の正規分布からサンプルサイズを2から1000まで増やしていくことにしよう。サンプルを取り出すことを，乱数生成に置き換えてその平均を計算していくこととする。\n\nset.seed(12345)\nsample_size &lt;- seq(from = 2, to = 1000, by = 10)\n# 平均値を格納するオブジェクトを初期化\nsample_mean &lt;- rep(0, length(sample_size))\n# 反復\nfor (i in 1:length(sample_size)) {\n  sample_mean[i] &lt;- rnorm(sample_size[i], mean = 50, sd = 10) %&gt;%\n    mean()\n}\n\n# 可視化\ndata.frame(size = sample_size, M = sample_mean) %&gt;%\n  ggplot(aes(x = size, y = M)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = 50, color = \"red\")\n\n\n\n\n\n\n\n\nこのようにサンプルサイズが増えていくにつれて，真値の50に近づいていくことが見て取れる。母集団分布の形状やパラメータ，サンプルサイズなどを変えて確認してみよう。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter06.html#不偏性",
    "href": "chapter06.html#不偏性",
    "title": "6  確率とシミュレーション",
    "section": "6.7 不偏性",
    "text": "6.7 不偏性\n推定量は確率変数であり，確率分布でその性質を記述することができる。標本統計量の従う確率分布のことを標本分布と呼ぶが，標本分布の確率密度関数がわかっているなら，その期待値や分散も計算できるだろう。推定量の期待値(平均)が母数に一致することも，推定量の望ましい性質の一つであり，この性質のことを不偏性unbiasednessという。\n心理統計を学ぶ時に初学者を苛立たせるステップの一つとして，分散の計算の時にサンプルサイズ\\(n\\)ではなく\\(n-1\\)で割る，という操作がある。これは不偏分散といって標本分散とは違うのだが，前者が不偏性を持っているのに対し，後者がそうでないからである。これを乱数を使って確認してみよう。\n平均50，SD10(母分散\\(10^2=100\\))の母集団から，サンプルサイズ\\(n=20\\)の標本を繰り返し得る。これはサイズ20の乱数生成で行う。各標本に対して標本分散と不偏分散を計算し，その平均(標本統計量の期待値)を計算してみよう。\n\niter &lt;- 5000\nvars &lt;- rep(0, iter)\nunbiased_vars &lt;- rep(0, iter)\n\n## 乱数の生成と計算\nset.seed(12345)\nfor (i in 1:iter) {\n  sample &lt;- rnorm(n = 20, mean = 50, sd = 10)\n  vars[i] &lt;- mean((sample - mean(sample))^2)\n  unbiased_vars[i] &lt;- var(sample)\n}\n\n## 期待値\nmean(vars)\n\n[1] 95.08531\n\nmean(unbiased_vars)\n\n[1] 100.0898\n\n\n標本分散を計算したオブジェクトvarsの平均すなわち期待値は95.0853144であり，設定した値(真値)の100からは幾分はなれている。これに対して，Rの埋め込み関数であるvarをつかった不偏分散の平均すなわち期待値は100.0898047であり，母分散の推定量としてはこちらの方が好ましいことがわかる。このように標本分散にはバイアスが生じることがわかっているので，あらかじめバイアスを補正するために元の計算式を修正していたのである。この説明で，苛立ちを感じていた人の溜飲が下がればよいのだが。\n他にも推定量の望ましい性質として有効性efficacyがあるが，詳細は 小杉, 紀ノ定, and 清水 (2023) を参照してほしい。この本には正規分布以外の例や，相関係数など他の標本統計量の例なども載っているが，いずれも乱数生成による近似で理解を進めるものである。諸君も数理統計的な説明に疲れたなら，ぜひ参考にしてもらいたい。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter06.html#信頼区間",
    "href": "chapter06.html#信頼区間",
    "title": "6  確率とシミュレーション",
    "section": "6.8 信頼区間",
    "text": "6.8 信頼区間\n標本統計量は確率変数であり，標本を取るたびに変わる。標本を取るときに入る確率的ゆらぎによるからで，標本平均は一致性，不偏性という望ましい性質を持ってはいるが，標本平均\\(=\\)母平均とはならない。\n標本平均という確率変数の実現値一点でもって，母平均を推測することは，母平均を推測する上ではほぼ確実に外れるギャンブルである。そこで母数に対してある幅でもって推定することを考えよう。\nたとえば平均50，標準偏差10の標準正規分布を母集団分布とし，サンプルサイズ10の標本をとり，その標本平均を母平均の推定値としよう(点推定)。同時に，その推定値に少し幅を持たせ，たとえば標本平均\\(\\pm 5\\)の区間推定をしたとする。この時，真値\\(0\\)を正しく推測できる確率を，反復乱数生成のシミュレーションで確かめてみよう。\n\niter &lt;- 10000\nn &lt;- 10\nmu &lt;- 50\nSD &lt;- 10\n\n# 平均値を格納しておくオブジェクト\nm &lt;- rep(0, iter)\n\nset.seed(12345)\nfor (i in 1:iter) {\n  # サンプリングし，標本統計量を保存\n  sample &lt;- rnorm(n, mean = mu, sd = SD)\n  m[i] &lt;- mean(sample)\n}\n\nresult.df &lt;- data.frame(m = m) %&gt;%\n  # 推定が一致するとTRUE,外れるとFALSEになる変数を作る\n  mutate(\n    point_estimation = ifelse(m == mu, TRUE, FALSE),\n    interval_estimation = ifelse(m - 5 &lt;= mu & mu &lt;= m + 5, TRUE, FALSE)\n  ) %&gt;%\n  summarise(\n    n1 = sum(point_estimation),\n    n2 = sum(interval_estimation),\n    prob1 = mean(point_estimation),\n    prob2 = mean(interval_estimation)\n  ) %&gt;%\n  print()\n\n  n1   n2 prob1 prob2\n1  0 8880     0 0.888\n\n\n結果からわかるように，点推定値は一度も正しく母数を当てていない。これは当然で，実数でやる以上小数点以下どこかでズレてしまうことがあるからで，精度を無視すると一致することはあり得ないのである。これに対して幅を持った予測の場合は，10^{4}回の試行のうち8880回はその区間内に真値を含んでおり，その正答率は88.8%である。\n区間推定において正答率を100%にするためには，その区間を無限に広げなければならない(母平均の推定の場合)。これは実質的に何も推定していないことに等しいので，5%程度の失敗を認めよう，95% の正答率で区間推定しようというのが習わしになっている。この区間のことを95%の信頼区間confidence intervalという。\n\n6.8.1 正規母集団分布の母分散が明らかな場合の信頼区間\n上のシミュレーションを応用して，区間推定が正当する確率が95%になるまで区間を調整して行ってもよいが，さすがにそれは面倒なので，推測統計学によって明らかになっている性質を紹介しよう。\n母集団が正規分布に従い，その母平均が\\(\\mu\\)，母分散が\\(\\sigma^2\\)であることがわかっている場合，標本平均の従う分布は平均\\(\\mu\\), 分散\\(\\frac{\\sigma^2}{n}\\)(標準偏差\\(\\frac{\\sigma}{\\sqrt{n}})\\)の正規分布であることがわかっている。\n標準正規分布の95%区間は，次の通り約\\(\\pm 1.96\\)である。\n\n# 両端から2.5%ずつ取り除くと\nqnorm(0.025)\n\n[1] -1.959964\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nこれらを合わせると，標本平均が\\(\\bar{X}\\)であったとき，95%信頼区間は標準偏差を1.96倍して，次のようになる。\n\\[ \\bar{X} - 1.96 \\frac{\\sigma}{\\sqrt{n}} \\le \\mu \\le \\bar{X} + 1.96 \\frac{\\sigma}{\\sqrt{n}} \\]\n先ほどの数値例を応用して，これを確かめてみよう。95％ちかい割合で，区間内に真値が含んでいることがわかる。\n\ninterval &lt;- 1.96 * SD / sqrt(n)\nresult.df2 &lt;- data.frame(m = m) %&gt;%\n  # 推定が一致するとTRUE,外れるとFALSEになる変数を作る\n  mutate(\n    interval_estimation = ifelse(m - interval &lt;= mu & mu &lt;= m + interval, TRUE, FALSE)\n  ) %&gt;%\n  summarise(\n    prob = mean(interval_estimation)\n  ) %&gt;%\n  print()\n\n    prob\n1 0.9498\n\n\n\n\n6.8.2 正規母集団分布の母分散が不明な場合の信頼区間\n先ほどの例では母分散がわかっている場合の例であったが，母平均や母分散がわかっていれば推測する必要はないわけで，実践的には母分散がわからない場合の推定が必要になってくる。幸いにしてそのような場合，すなわち母分散を不偏分散(標本統計量)で置き換えた場合は，標本平均が自由度\\(n-1\\)のt分布に従うことがわかっている。(詳細は 小杉, 紀ノ定, and 清水 (2023) を参照) ただその場合，標準正規分布のように95%区間が\\(\\pm 1.96\\)に限らず，サンプルサイズに応じてt分布の形が変わるから，それを考慮して以下の式で信頼区間を算出する。 \\[ \\bar{X} + T_{0.025}\\frac{U}{\\sqrt{n}} \\le \\mu \\le \\bar{X} + T_{0.975}\\frac{U}{\\sqrt{n}} \\]\nここで\\(T_{0.025}\\)はt分布の2.5パーセンタイル，\\(T_{0.975}\\)は97.5パーセンタイルを指す。t分布は(平均が0であれば)左右対称なので，\\(T_{0.025}=-T_{0.975}\\)と考えても良い。また\\(U^2\\)は不偏分散である(\\(U\\)はその平方根)。\nこれも乱数による近似計算で確認しておこう。同じく95％ちかい割合で，区間内に真値が含まれていることがわかる。\n\n# シミュレーションの設定\niter &lt;- 10000\nn &lt;- 10\nmu &lt;- 50\nSD &lt;- 10\n\n# 平均値を格納しておくオブジェクト\nm &lt;- rep(0, iter)\ninterval &lt;- rep(0, iter)\n\nset.seed(12345)\nfor (i in 1:iter) {\n  # サンプリングし，標本統計量を保存\n  sample &lt;- rnorm(n, mean = mu, sd = SD)\n  m[i] &lt;- mean(sample)\n  U &lt;- sqrt(var(sample)) # sd(sample)でも同じ\n  interval[i] &lt;- qt(p = 0.975, df = n - 1) * U / sqrt(n)\n}\n\nresult.df &lt;- data.frame(m = m, interval = interval) %&gt;%\n  # 推定が一致するとTRUE,外れるとFALSEになる変数を作る\n  mutate(\n    interval_estimation = ifelse(m - interval &lt;= mu & mu &lt;= m + interval, TRUE, FALSE)\n  ) %&gt;%\n  summarise(\n    prob = mean(interval_estimation)\n  ) %&gt;%\n  print()\n\n    prob\n1 0.9482",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter06.html#課題",
    "href": "chapter06.html#課題",
    "title": "6  確率とシミュレーション",
    "section": "6.9 課題",
    "text": "6.9 課題\n\n算術平均\\(M = \\frac{1}{n}\\sum x_i\\)が一致推定量であることが示されましたが，調和平均\\(HM = \\frac{n}{\\sum \\frac{1}{x_i}}\\)や幾何平均\\(GM = (\\prod x_i)^{\\frac{1}{n}} = \\exp(\\frac{1}{n}\\sum \\log(x_i)))\\)はどうでしょうか。シミュレーションで確かめてみましょう。\nサンプルサイズ\\(n\\)が大きくなるほど，標本平均が母平均に近づくという性質は正規分布以外でも成立するでしょうか。自由度\\(\\nu = 3\\)のt分布を使って，シミュレーションで確認してみましょう。なおt分布の乱数はrt()で生成でき，非心度パラメータncpを指定しなければその平均は0です。\nt分布の自由度\\(\\nu\\)が極めて大きい時は，標準正規分布に一致することがわかっています。rt()関数を使って自由度が10,50,100のときの乱数を1000個生成し，ヒストグラムを書いてその形状を確認しましょう。また乱数の平均と標本標準偏差を計算し，標準正規分布に近づくことを確認しましょう。\n平均が50，標準偏差が10の正規分布からサンプルサイズ20の乱数を10000個生成し，quantile関数をつかって95％信頼区間をシミュレートしてください。理論値と比較して確認してください。\n平均が100，標準偏差が15の正規分布から抽出された標本について，サンプルサイズを10，100，1000と変えたときの標本平均の95%信頼区間の幅を比較してください。\n\n\n\n\n\n佐藤坦. 1994. はじめての確率論: 測度から確率へ. 共立出版.\n\n\n吉田伸生. 2021. 確率の基礎から統計へ. 新装版. 日本評論社.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.\n\n\n平岡和幸, and 堀玄. 2009. プログラミングのための確率統計. オーム社. http://amazon.co.jp/o/ASIN/4274067750/.\n\n\n河野敬雄. 1999. 確率概論. 京都大学学術出版会.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter06.html#footnotes",
    "href": "chapter06.html#footnotes",
    "title": "6  確率とシミュレーション",
    "section": "",
    "text": "詳しくは 吉田 (2021) , 河野 (1999), 佐藤 (1994) などを参照のこと。↩︎\n前者の解釈は高校までの数学で学ぶ確率であり，頻度主義的確率と呼ばれることがある。一方後者の解釈は，降水確率X%のように日常でも使うものであり，主観確率と呼ばれることがある。こうした解釈の違いを，主義主張の対立であって数学的ではない，と批判する向きもあるが，実際コルモゴロフの公理はどちらの立場でも成立するように整えられており，筆者個人的にはユーザが理解しやすく計算できればどちらでも良いと考えている。↩︎\n厳密なエビデンスは示せないが，俗に「嘘のゴサンパチ」というように人間が適当に数字を述べると5,3,8が使われる率がチャンスレベルより高いと言われている。↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>確率とシミュレーション</span>"
    ]
  },
  {
    "objectID": "chapter07.html",
    "href": "chapter07.html",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "",
    "text": "7.1 帰無仮説検定の理屈と手続き",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#帰無仮説検定の理屈と手続き",
    "href": "chapter07.html#帰無仮説検定の理屈と手続き",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "",
    "text": "7.1.1 帰無仮説検定の目的\n帰無仮説検定は，実験や調査で得たデータから得られた知見が意味のあるものかどうか，母集団の性質として一般化可能かどうかを判定するための枠組みである。手法と判断基準が明確なゲームの一種だと考えたよう。というのも，帰無仮説検定は有意水準という基準を設けて，帰無仮説と対立仮説という2つの考え方(モデル)を対決させ，勝敗を決するものだからである。勝敗を決するとしたのは，帰無仮説と対立仮説は排他的な関係にあるからであり，どちらも正しいとかどちらも間違っているという結末にはならないからである。ただし，あくまでも推測統計的なロジックに基づく判定であるから，判定結果にも確率的な要素が含まれる。本当は帰無仮説が正しい時に，間違って「対立仮説が正しい」と判定してしまう確率はゼロではない。逆に帰無仮説が正しくない時に，間違って「対立仮説が正しくない(帰無仮説が正しい)」と判定してしまう可能性もある。前者をタイプ1エラー，後者をタイプ2エラーという。どちらの確率もゼロであってほしいが，そうはならないので，前者を\\(\\alpha\\)，後者を\\(\\beta\\)としたときに，それぞれを一定の水準以下に抑えたい。この目的のために手順を整え，一般化したのが帰無仮説検定である。なお，先に述べた有意水準は，この\\(\\alpha\\)の許容される水準であり，心理学では一般に5%に設定する。\nこのように帰無仮説検定という考え方は，エラーの統制が本来の狙いであるから，「有意になるように工夫する」という発想は根本的に間違っている。また，統計的推定という数学的手続きに，人間が納得しやすい判定を下すという人為的手続きが組み合わさったものであるから，帰無仮説検定の結果に過剰な意味を持たせたり一喜一憂したりすることがないように注意しよう。\n\n\n7.1.2 帰無仮説検定の手続き\n帰無仮説検定の手続きを一般化すれば，次のようになる。\n\n帰無仮説と対立仮説を設定する。\n検定統計量を選択する。\n判定基準を決定する。\n検定統計量を計算する。\n判定する。\n\n帰無仮説検定は，群間の平均値に差があるかどうか，相関係数に統計的な意味があるかどうかといった事例に対して適用される。 当然のことながら，これは標本から母集団を推定するという文脈における話で，物理学的な真偽を理論的に判断するとか，全数調査のように母集団全体の情報が手に入る場合といった場合の話ではない。また，標本のサンプルサイズが小さく，標本統計量の信頼区間が大きいことから，枠組みなしには判定できないという背景があることも再確認しておこう。\n母集団の状態がわからないので，仮説を設定する。帰無仮説Null Hypothesisは空っぽの仮説という意味で，母平均差がない(差がゼロ，\\(\\mu_1 - \\mu_2 = 0\\))とか，母相関がゼロ(\\(\\rho = 0\\))である，とされる。対立仮説Alternative Hypothesisは帰無仮説と排他的な関係にある仮説としてつくられるから，「差が無くはない(\\(\\mu_1 - \\mu_2 \\neq 0\\))」「相関がゼロではない(\\(\\rho \\neq 0\\))」という表現になる。なぜ帰無仮説がゼロであることから始められるかといえば，ふたつの排他的な仮説を考えた時にゼロでない状態というのは無数にあり得るので，仮説として特定できないからである(差が1のとき，1.1のとき，1.11のとき・・・と延々と検定し続けるわけにもいくまい)。\n検定統計量の選択は，二群の平均値差のときは\\(t\\)，三群以上の時は\\(F\\)，相関係数の検定も\\(t\\)，と天下り的に示されることが一般的である。もちろんこれらの統計量が選ばれるのは，数理統計的な論拠に基づいている。判定基準は5%水準とすることが一般的だし，検定統計量の計算はアルゴリズムに沿って機械的に可能である。判定は客観的な指標に基づいて行われるから，「どの状況でどのような帰無仮説をおくか」が類型化できれば，この手続き全体が自動的に進められる。\nしかしここでは改めて，丁寧に手順を追いながらみてみよう。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#相関係数の検定",
    "href": "chapter07.html#相関係数の検定",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "7.2 相関係数の検定",
    "text": "7.2 相関係数の検定\nここでは相関係数の検定を例に取り上げる。俗に「無相関検定」と呼ばれるように，相関がどれほど大きいとかどれほど意味があるということをチェックするのでは無く，無相関ではない，ということをチェックする。もちろん標本相関は計算してゼロでなければ，それは無相関ではない。ここで考えたいのは，母相関がゼロではないということである。言い換えると，母相関がゼロの状態であっても，標本相関がゼロでないことは，小標本のサンプリングという背景のもとでは当然のことである。\n確認してみよう。まず，無相関なデータセットを作ることを考える。RのMASSパッケージを使い，多変量正規分布の確率分布関数から乱数を生成しよう。\n\nlibrary(MASS)\nset.seed(12345)\nN &lt;- 100000\nX &lt;- mvrnorm(N,\n  mu = c(0, 0),\n  Sigma = matrix(c(1, 0, 0, 1), ncol = 2),\n  empirical = TRUE\n)\nhead(X)\n\n           [,1]        [,2]\n[1,] -0.4070308 -0.72271139\n[2,] -0.5774631 -0.57075167\n[3,]  0.2312929 -0.42458994\n[4,]  0.6242499 -0.55522146\n[5,] -0.7791585  0.55004824\n[6,]  1.8995860 -0.04899946\n\n\nここでは10^{5}個の乱数を生成した。つくられたオブジェクトXは表示されているように，2変数からなる。ここでは相関のある2変数を想定しており，各変数がそれぞれ標準正規分布に従っているという設定である。rnorm関数を2つ使って2変数をつくっても良いのだが，2変数セットで取り出すことを考えると多変量正規分布をかんがえることになる。多変量正規分布は，ひとつひとつの変数については正規分布として平均とSDをもち，かつ，変数の組み合わせとして共分散をもつものである。mvrnormの引数をみると，muは平均ベクトルであり，Sigmaが分散共分散行列である。分散共分散行列とは，ここでは\\(2\\times 2\\)の正方行列であり，対角項に分散を，非対角項に共分散をもつ行列である。共分散は標準偏差と相関係数の積で表される。\n分散\n\\[ s_x^2 = \\frac{1}{n}\\sum (x_i - \\bar{x})^2 =  \\frac{1}{n}\\sum (x_i - \\bar{x})(x_i - \\bar{x})\\]\n標準偏差\n\\[ s_x = \\sqrt{s_x^2} = \\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\]\n共分散\n\\[ s_{xy} = \\frac{1}{n}\\sum (x_i - \\bar{x})(y_i - \\bar{y})\\]\n相関係数\n\\[r_{xy} = \\frac{s_{xy}}{s_xs_y} = \\frac{\\frac{1}{n}\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\sqrt{\\frac{1}{n}\\sum (y_i - \\bar{y})^2}}\\]\n分散共分散行列\n\\[\\Sigma = \\begin{pmatrix} s_x^2 & s_{xy} \\\\ s_{yx} & s_y^2 \\end{pmatrix}\n= \\begin{pmatrix} s_x^2 & r_{xy}s_xs_y \\\\ r_{xy}s_xs_y & s_y^2 \\end{pmatrix}\\]\n今回Sigma = matrix(c(1,0,0,1),ncol = 2)としたのは，この2変数が無相関であること(SDはそれぞれ1であること)を指定している。ちなみにempirical = TRUEのオプションは，生成された乱数が設定した分散共分散行列のもつ相関係数と一致するように補正することを意味している。\n可視化しておこう。つくられた乱数が無相関であることを，散布図を使って確認する。\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nX %&gt;%\n  as.data.frame() %&gt;%\n  ggplot(aes(x = V1, y = V2)) +\n  geom_point()\n\n\n\n\n\n\n\n\n数値的にも確認しておこう。\n\ncor(X) %&gt;% round(5)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nつくられた乱数が無相関であることが確認できた。さてこれが母集団であったとして，ここからたとえばn = 20のサンプルをとったとする。この時の相関はどうなるだろうか。 Rで計算してみよう。sample関数をつかって抜き出す行を決めて，該当する行だけs1オブジェクトに代入する。その上で相関係数を計算してみよう。\n\nselected_row &lt;- sample(1:N, 20)\nprint(selected_row)\n\n [1]  9647 80702 57543 93179 99032 82624 32672 53670 69698 42383 23801 69303\n[13]  9816 61803 69464 23107 76958 44447    10 27292\n\ns1 &lt;- X[selected_row, ]\ncor(s1)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.1431698\n[2,] 0.1431698 1.0000000\n\n\n今回の相関係数は0.1431698となった。母集団の相関係数が0であっても，適当に抜き出した20点が相関係数を持ってしまう(0でない)ことはあり得ることなのである。問題は，これがどの程度あり得ることなのか，である。いいかえると，研究者が\\(n=20\\)のサンプルをとって相関を得た時，それが\\(r = 0.14\\)であったとしても，母相関\\(\\rho = 0.0\\)からのサンプルである可能性がどれぐらいあるか，ということである。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#標本相関係数の分布と検定",
    "href": "chapter07.html#標本相関係数の分布と検定",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "7.3 標本相関係数の分布と検定",
    "text": "7.3 標本相関係数の分布と検定\n標本相関係数は確率変数なので，毎回標本を取る度に値が変わるし，どの実現値がどの程度出現するかは標本分布で表現できる。 ではどのような標本分布に従うのだろうか。先ほどのサンプリングを繰り返して，乱数によって近似してみよう1。\n\niter &lt;- 10000\nsamples &lt;- c()\nfor (i in 1:iter) {\n  selected_row &lt;- sample(1:N, 20)\n  s_i &lt;- X[selected_row, ]\n  cor_i &lt;- cor(s_i)[1, 2]\n  samples &lt;- c(samples, cor_i)\n}\ndf &lt;- data.frame(R = samples)\n# ヒストグラムの描画\ng &lt;- df %&gt;%\n  ggplot(aes(x = R)) +\n  geom_histogram(binwidth = 0.01)\nprint(g)\n\n\n\n\n\n\n\n\nヒストグラムを見ると，サンプルサイズが20の場合，母相関係数\\(\\rho = 0.0\\)であっても\\(r = 0.3\\)や\\(r=0.4\\)程度の標本相関が出現することはある程度みられることである。\nまた，標本分布は左右対称の何らかの理論分布に従っていそうだ。数理統計学の知見から，相関係数の場合，標本相関係数を次の式によって変換することで，自由度が\\(n-2\\)の\\(t\\)分布に従うことが知られている。\n\\[ t = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} \\]\n\ndf %&gt;%\n  mutate(T = R * sqrt(18) / sqrt(1 - R^2)) %&gt;%\n  ggplot(aes(x = T)) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 0.1) +\n  # 自由度18のt分布の確率密度関数を追加\n  stat_function(fun = dt, args = list(df = 18), color = \"red\", linewidth = 2) +\n  # Y軸のラベルを変更\n  ylab(\"Density\")\n\n\n\n\n\n\n\n\nこれを利用して相関係数の検定が行われる。以下，サンプルサイズ20で標本相関係数が\\(r=0.5\\)だったとして，手順に沿って解説する。\n\n帰無仮説は母相関\\(\\rho = 0.0\\)とする。対立仮説は\\(\\rho \\neq 0.0\\)である。\n検定統計量は相関係数\\(r\\)を変換した\\(t\\)とする。\n判定基準として，\\(\\alpha = 0.05\\)とする。すなわち，母相関が0であるという仮説を棄却して間違える確率を5%以下に制御したい。\n検定統計量を計算する。\\(n=20,r=0.5\\) より， \\[t = \\frac{0.5\\times(\\sqrt{18})}{\\sqrt{1-0.5^2}} = 2.449\\]\n標本相関係数の絶対値が0.5を超える確率は，\\(t\\)分布の理論値から，次のように計算できる。あるいは，\\(t\\)分布の両端5%を切り出す臨界値 を次のように計算できる。\n\n\n(1 - pt(0.5 * sqrt(18) / sqrt(1 - 0.5^2), df = 18)) * 2\n\n[1] 0.02476956\n\nqt(0.975, df = 18)\n\n[1] 2.100922\n\n\nここで注意してほしい点は，今回の検定の目的が「母相関が0であるという帰無仮説を棄却できるかどうか」であり，相関係数の符号については関心がなく絶対値で考える点である。pt関数は，ある確率点までの累積面積であるから，1から引くことでその確率点以上の値がでる確率が示される。\\(t\\)分布は左右対称の分布なので，これを2倍した値が絶対値で考えた時の出現確率である。これが5%よりも小さければ，有意であると判断できる。今回は，統計的に有意であるといって良い。\nなお，表現上の細かい注意点になるが，この確率は今回の実現値「以上」のより極端な値が出る確率であり，この実現値が出る確率という言い方はしない。確率は面積であり，点に対する面積はないからである。\nqt関数で示されるのは確率点なので，これ以上の値を今回の実現値が出していたら，統計的に有意であると判断できる。今回の実現値から算出した値は\\(t(18)=2.449\\)であり，臨界値の\\(2.100\\)よりも大きな値なので，有意であると判断できる。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#種類の検定のエラー確率",
    "href": "chapter07.html#種類の検定のエラー確率",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "7.4 2種類の検定のエラー確率",
    "text": "7.4 2種類の検定のエラー確率\n上では丁寧に計算過程をみてきたが，実践場面ではサンプルはひとつであり，標本統計量もひとつ算出されるだけである。自分の大切なデータであるから，標本分布から得られた特定のケースにすぎないことが直感的にわかりにくいかもしれない。\n相関係数の検定をするときは，Rの関数cor.testを使って次のように行う。ここではmvrnorm関数を使って，相関係数0.5の仮想データを作っている。\n\nset.seed(17)\nn &lt;- 20\nsampleData &lt;- mvrnorm(n,\n  mu = c(0, 0),\n  Sigma = matrix(c(1, 0.5, 0.5, 1), ncol = 2),\n  empirical = TRUE\n)\ncor.test(sampleData[, 1], sampleData[, 2])\n\n\n    Pearson's product-moment correlation\n\ndata:  sampleData[, 1] and sampleData[, 2]\nt = 2.4495, df = 18, p-value = 0.02477\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.07381057 0.77176071\nsample estimates:\ncor \n0.5 \n\n\n結果として示されている，tの値や自由度，\\(p\\)値が先ほど示した例と対応していることを確認できる。さらに，相関係数の信頼区間や標本相関係数そのものも示されている。この信頼区間が0を跨いでいないことからも，帰無仮説が棄却されることが見て取れるだろう。\nわれわれは既に，母相関が0のデータセットの一部を取り出すと，その相関係数が0ではなく0.5のような数字になることも知っている。もちろん母相関が0であれば標本相関も0近い値が出やすいとしても，である。つまり標本から得られた値をあまり大事に考えすぎない方が良い(もちろん一般化を念頭においている時は，である)。 また，帰無仮説は「母相関が0である」なので，これが棄却されたとしても「母相関が0であるとは言えない」のに過ぎない。ここから，母相関も\\(r=0.5\\)付近にあるはずだとか，\\(p\\)値が2.4%なので5%よりもずいぶん低いのは証拠の重要さを物語っているのだ，と論じるのは適切ではない。母相関が0という仮想的な状況のもとでの話であって，母相関が実際にどの程度なのかを検討しているわけではない。この点が誤解されやすいので特に注意してほしい。\nここに来るとタイプ1エラー，タイプ2エラーがより具体的に理解できるようになってきたではないだろうか。タイプ1エラーはこの帰無仮説が正しい時に，標本相関から計算した統計量で判断する確率であるから，上の手続きで見たことそのものである。\n別の角度で見てみよう。cor.testをつかうと標本統計量の信頼区間が算出できる。この信頼区間が母相関–ここでは帰無仮説である\\(\\rho =0\\)を「正しく」含んでいる割合を見てみよう。 cor.test関数が返すオブジェクトには，conf.intという名前のものがあり，デフォルトではここで95%の信頼区間が含まれている。 シミュレーションに先立って，結果を格納する2列のデータフレームを作っておき，シミュレーション後にifelse関数で母相関が含まれているかどうかの判定をした。\n\nset.seed(42)\niter &lt;- 10000\nintervals &lt;- data.frame(matrix(NA, nrow = iter, ncol = 2))\nnames(intervals) &lt;- c(\"Lower\", \"Upper\")\nfor (i in 1:iter) {\n  selected_row &lt;- sample(1:N, 20)\n  s_i &lt;- X[selected_row, ]\n  cor_i &lt;- cor.test(s_i[, 1], s_i[, 2])\n  intervals[i, ] &lt;- cor_i$conf.int[1:2]\n}\n#\ndf &lt;- intervals %&gt;%\n  mutate(FLG = ifelse(Lower &lt;= 0 & Upper &gt;= 0, 1, 0)) %&gt;%\n  summarise(type1error = mean(FLG)) %&gt;%\n  print()\n\n  type1error\n1       0.95\n\n\n今回の例では，95%の割合で正しく判断できていた。言い換えると，エラーが生じる割合は5%だったので，タイプ1エラー確率を5%以下にするという目的はしっかり達成できていたことが確認できた。\n同様に，タイプ2エラーは，帰無仮説が正しくないときに帰無仮説を採択する確率だから，シミュレーションするなら次のようになる。まず母相関が0でない状況を作り出そう。今回は母相関が0.5であるとして，母集団分布を描いてみよう。\n\nset.seed(12345)\nN &lt;- 100000\nX &lt;- mvrnorm(N,\n  mu = c(0, 0),\n  Sigma = matrix(c(1, 0.5, 0.5, 1), ncol = 2),\n  empirical = TRUE\n)\n\nX %&gt;%\n  as.data.frame() %&gt;%\n  ggplot(aes(x = V1, y = V2)) +\n  geom_point()\n\n\n\n\n\n\n\n\n今度は，ここからサンプルサイズ20のデータセットを取り出し，検定することにしよう。検定の結果，有意になれば1，ならなければ0というオブジェクトを作って，判定の正しさを考えてみることにする。\n\niter &lt;- 10000\njudges &lt;- c()\nfor (i in 1:iter) {\n  selected_row &lt;- sample(1:N, 20)\n  s_i &lt;- X[selected_row, ]\n  cor_i &lt;- cor.test(s_i[, 1], s_i[, 2])\n  judges &lt;- c(judges, cor_i$p.value)\n}\ndf &lt;- data.frame(p = judges) %&gt;%\n  mutate(FLG = ifelse(p &lt;= 0.05, 1, 0)) %&gt;%\n  summarise(\n    sig = sum(FLG == 1),\n    non.sig = sum(FLG == 0),\n    type2error = non.sig / iter\n  ) %&gt;%\n  print()\n\n   sig non.sig type2error\n1 6442    3558     0.3558\n\n\n今回は母相関が0.5であり，帰無仮説は棄却されて然るべきなのだが，有意でないと判断された割合が35.58%あったことになる。心理学の研究などでは，この確率\\(\\beta\\)が0.2未満，逆にいうと検出が0.8以上あることが望ましいとされているので，今回のこの事例では十分な件出力がなかった，と言えるだろう。\nもちろん実際には，母相関がどれぐらいなのかわからない。\\(0.3\\)なのかもしれないし，\\(-0.5\\)であるかもしれない。つまりタイプ2エラーは研究者が制御できるところではなく，せいぜい大きな相関が見込めそうな変数について標本を取ろうと心がけるだけである。\nタイプ1,2エラーの確率は，サンプルサイズや効果量(ここでは母相関の大きさ)の関数である。サンプルサイズは研究者が決定することができるので，効果を見積もり，制御したいエラー確率の基準を決めて，合理的にサンプルサイズを決めるべきである。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#課題",
    "href": "chapter07.html#課題",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "7.5 課題",
    "text": "7.5 課題\n\n母相関が0の母集団から，サンプルサイズ10の標本を取り出して標本相関を見た時の標本分布を，乱数のヒストグラムで近似してみましょう。\n同じく，サンプルサイズ50の標本を取り出して標本相関を見た時の標本分布を，乱数のヒストグラムで近似してみましょう。サンプルサイズが20や10の時と比べてどういう違いがあるでしょうか。\nサンプルサイズ50の標本相関が\\(r=-0.3\\)のとき，統計的に有意と言えるでしょうか。cor.test をつかって検定し，検定結果と判断結果を記述してください。\n標本相関が\\(r=-0.3\\)だとします。サンプルサイズが10,20,50,1000のとき，統計的に有意と言えるでしょうか。cor.testを使って検定し，検定結果を一覧にしてみましょう。ここから何がわかるでしょうか。\n母相関が\\(\\rho = -0.3\\)だったとします。サンプルサイズ20のとき，どの程度の検出力があると見込めるでしょうか。シミュレーションで近似してください。\n\n\n\n\n\n池田功毅, and 平石界. 2016. “心理学における再現可能性危機：問題の構造と解決策.” 心理学評論 59 (1): 3–14. https://doi.org/10.24602/sjpr.59.1_3.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#footnotes",
    "href": "chapter07.html#footnotes",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "",
    "text": "このような二度手間を取らず，mvrnormからサンプルサイズ20の乱数を反復生成しても良い。母集団を具体的なものとしてイメージするために，母相関が0の母集団からサンプリングを繰り返す方法をとった。↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter08.html",
    "href": "chapter08.html",
    "title": "8  平均値差の検定",
    "section": "",
    "text": "8.1 一標本検定\nまず配置標本検定の例から始める。母平均がわかっている，あるいは理論的に仮定される特定の値に対して，標本平均が統計的に有意に異なっていると言って良いかどうかの判断をするときに用いる。 たとえば7件法のデータを取ったときに，ある項目の平均が中点4より有意に離れていると言って良いかどうか，といった判定をするときに用いる。かりに，サンプルサイズ10で7件法のデータが得られたとしよう。ここでは平均4,SD1の正規乱数を10件生成することで表現する。実際にはこの値を，人に対する尺度カテゴリへの反応として得ているはずである。\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(17)\nn &lt;- 10\nmu &lt;- 4\nX &lt;- rnorm(n, mean = mu, sd = 1)\nprint(X)\n\n [1] 2.984991 3.920363 3.767013 3.182732 4.772091 3.834388 4.972874 5.716534\n [9] 4.255237 4.366581\n今回，標本平均は4.177であり，これより極端な値が\\(\\mu = 4\\)の母集団から得られるかどうかを検定する。帰無仮説検定の手順にそって進めていくと，以下のようになる。\nこのあと，検定統計量の計算と判定である。これをRはt.test関数で一気に処理できる。\nresult &lt;- t.test(X, mu = mu)\nprint(result)\n\n\n    One Sample t-test\n\ndata:  X\nt = 0.6776, df = 9, p-value = 0.5151\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 3.585430 4.769131\nsample estimates:\nmean of x \n 4.177281\n結果として，今回の検定統計量の実現値は0.678であり，自由度9のt分布からこれ以上の値が出てくる確率は，0.515であることがわかる。これは5%水準と見比べてより大きいので，レアケースではないと判断できる。つまり，母平均4の正規母集団から，4.177の標本平均が得られることはそれほど珍しいものではなく，統計的に有意に異なっていると判断するには及ばない，ということである。\nレポートなどに記載するときは，これら実現値やp値を踏まえて「\\(t(9)=0.66776,p=0.5151 ,n.s.\\)」などとする。ここでn.s.はnot significantの略である。\nさてこの例では，母平均4の正規乱数を生成し，その平均が4と異なるとはいえない，と結論づけた。これは一見，当たり前のことのようであり，無意味な行為におもえるかもしれない。しかし次の例を見てみよう。\nn &lt;- 3\nmu &lt;- 4\nX &lt;- rnorm(n, mean = mu, sd = 1)\nmean(X) %&gt;%\n  round(3) %&gt;%\n  print()\n\n[1] 5.04\n\nresult &lt;- t.test(X, mu = mu)\nprint(result)\n\n\n    One Sample t-test\n\ndata:  X\nt = 5.1723, df = 2, p-value = 0.03541\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 4.174825 5.904710\nsample estimates:\nmean of x \n 5.039768\nここではサンプルサイズ\\(n=3\\)であり，標本平均が5.04であった。このときt値は5%臨界値を上回っており，「母平均4のところから得られる値にしては極端」であるから，統計的に有意に異なる，と判断することになる。乱数生成時は平均を確かに4に設定したが，母平均から取り出したごく一部が，そこから大きく離れてしまうことはあり得るのである。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#一標本検定",
    "href": "chapter08.html#一標本検定",
    "title": "8  平均値差の検定",
    "section": "",
    "text": "帰無仮説は母平均が理論的な値(ここでは尺度の中点4)であること，すなわち\\(\\mu =4\\)であり，対立仮説は\\(\\mu \\neq 4\\) である。\n検定統計量は，正規母集団から得られる標本平均が従う標本分布であり，母分散が未知の場合の区間推定に用いたT統計量になる。\n判断基準は心理学の慣例に沿って5%とする。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#二標本検定",
    "href": "chapter08.html#二標本検定",
    "title": "8  平均値差の検定",
    "section": "8.2 二標本検定",
    "text": "8.2 二標本検定\n続いて二標本の検定について考えよう。実験群と統制群のように，無作為割り当てをすることで平均因果効果をみる際に行われるのが，この検定である。帰無仮説は「群間差はない」であり，対立仮説はその否定である。また，正規母集団からの標本を仮定するので，検定統計量はここでもt分布に従う値になる。帰無仮説検定の手順に沿って，改めて確認しておこう。\n\n帰無仮説は「二群の母平均に差がない」である。二群の母平均をそれぞれ\\(\\mu_1,\\mu_2\\)とすると，帰無仮説は\\(\\mu_1 = \\mu_2\\)，あるいは\\(\\mu_1 - \\mu_2 = 0\\)と表される。対立仮説は\\(\\mu_1 \\neq \\mu_2\\)あるいは\\(\\mu_1-\\mu_2 \\neq 0\\)である。\n検定統計量は，正規母集団から得られる標本平均が従う標本分布であり，母分散が未知の場合の区間推定に用いたT統計量になる。\n判断基準は心理学の慣例に沿って5%とする。\n\nこれを検証するために，サンプルデータを乱数で生成しよう。 まず，各群のサンプルサイズをn1,n2とする。ここでは話を簡単にするため，サンプルサイズは両群ともに10とした。つぎに両群の母平均だが，群1の母平均を\\(\\mu_1\\)，群2の母平均を\\(\\mu_2 = \\mu_1 + \\delta\\)で表現した。この\\(\\delta\\)は差分であり，これが\\(\\delta=0\\)であれば母平均が等しいこと，\\(\\delta \\neq 0\\)であれば母平均が異なることになる。最後に両群の母SDを設定した。\nここでの検定は，この差分\\(d\\)が母平均0の母集団から得られたと判断して良いかどうか，という形で行われる。検定統計量\\(T\\)は，次式で算出されるものである。\n\\[ T = \\frac{d - \\mu_0}{\\sqrt{U^2_p/\\frac{n_1n_2}{n_1+n_2}}}\\]\nここで\\(d\\)は二群の標本平均の差であり，\\(U^2_p\\)はプールされた不偏分散と呼ばれ，二群を合わせて計算された全体の母分散推定量である。各群の標本分散をそれぞれ\\(S^2_1, S^2_2\\)とすると，次式で算出される。\n\\[ U^2_p = \\frac{n_1S^2_1+ n_2S^2_2}{n_1 + n_2 -2} \\]\nこれらの式はつまり，サンプルサイズの違いを考慮するため，一旦両群の標本分散に各サンプルサイズを掛け合わせ，プールした全体のサンプルサイズから各々\\(-1\\)をすることで全体として不偏分散にしている。\nこれを踏まえて，具体的な数字で見ていこう。 その上で乱数でデータを生成し，その標本平均を確認した上で，t.test関数によって検定を行っている。\n\nn1 &lt;- 10\nn2 &lt;- 10\nmu1 &lt;- 4\nsigma &lt;- 1\ndelta &lt;- 1\nmu2 &lt;- mu1 + (sigma * delta)\n\nset.seed(42)\nX1 &lt;- rnorm(n1, mean = mu1, sd = sigma)\nX2 &lt;- rnorm(n2, mean = mu2, sd = sigma)\n\nX1 %&gt;%\n  mean() %&gt;%\n  round(3) %&gt;%\n  print()\n\n[1] 4.547\n\nX2 %&gt;%\n  mean() %&gt;%\n  round(3) %&gt;%\n  print()\n\n[1] 4.837\n\nresult &lt;- t.test(X1, X2, var.equal = TRUE)\nprint(result)\n\n\n    Two Sample t-test\n\ndata:  X1 and X2\nt = -0.49924, df = 18, p-value = 0.6237\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.506473  0.927980\nsample estimates:\nmean of x mean of y \n 4.547297  4.836543 \n\n\n今回の母平均は\\(\\mu_1 = 4, \\mu_2 = 4+1\\)にしているが，標本平均は4.547と4.837であり，標本上では大きな差が見られなかった。結果として，t値は0.4992369であり，自由度18のもとでのp値は0.6236593である。5%水準を上回る値であるから，結論としては対立仮説を採択するには至らない，差があるとはいえない，である。\n今回の設定では母平均に差があるはず(\\(4 \\neq 4 + 1\\))なのだから，これは誤った判断で，タイプ2エラーが生じているケースということになる。研究実践場面では，母平均やその差については知り得ないのだから，このような判断ミスが生じていたかどうかは分かり得ないことに留意しよう。\nなお，ここではわかりやすく2群であることを示すためにX1,X2と2つのオブジェクトを用意したが，実践的にはデータフレームの中で群わけを示す変数があり，formulaの形で次のように書くことが多いだろう。\n\ndataSet &lt;- data.frame(group = c(rep(1, n1), rep(2, n2)), value = c(X1, X2)) %&gt;%\n  mutate(group = as.factor(group))\nt.test(value ~ group, data = dataSet, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  value by group\nt = -0.49924, df = 18, p-value = 0.6237\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.506473  0.927980\nsample estimates:\nmean in group 1 mean in group 2 \n       4.547297        4.836543",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#二標本検定ウェルチの補正",
    "href": "chapter08.html#二標本検定ウェルチの補正",
    "title": "8  平均値差の検定",
    "section": "8.3 二標本検定(ウェルチの補正)",
    "text": "8.3 二標本検定(ウェルチの補正)\n先ほどのt.test関数には，var.equal = TRUEというオプションが追加されていた。これは2群の分散が等しいと仮定した場合の検定になる。t検定は歴史的にこちらが先に登場しているが，2群の分散が等しいかどうかはいきなり前提できるものでもない。等分散性の検定は，Levene検定を行うのが一般的であり，R においては，carパッケージやlawstat パッケージが対応する関数を持っている。ここではcarパッケージの leveneTest関数を用いる例を示す。\n\nlibrary(car)\nleveneTest(value ~ group, data = dataSet, center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  1  2.9405 0.1035\n      18               \n\n\nこの結果を見ると，p値から明らかなように，2群の分散が等しいという帰無仮説が棄却できなかったので，等しいと考えてt検定に進むことができる。もしこれが棄却されてしまったら，2群の分散が等しいという帰無仮説が成り立たないのだから，等分散性の仮定を外す必要がある。実行は簡単で，var.equalをFALSEにすれば良い。\n\nresult2 &lt;- t.test(value ~ group, data = dataSet, var.equal = FALSE)\nprint(result2)\n\n\n    Welch Two Sample t-test\n\ndata:  value by group\nt = -0.49924, df = 13.421, p-value = 0.6257\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.5369389  0.9584459\nsample estimates:\nmean in group 1 mean in group 2 \n       4.547297        4.836543 \n\n\nよく見ると，タイトルがWelch Two Sample t-testに変わっている。Welchの補正が入ったt検定という意味である。また自由度が実数(13.421)になっているが，このようにt分布の自由度を調整することで等分散性の仮定から逸脱した場合の補正となる。もちろん報告する際は「\\(t(\\) 13.421 \\()=\\) -0.499, \\(p=\\) 0.626」のように書くことになるから，自由度が実数であれば補正済みであると考えられるだろう。\nしかし，分散が等しいという仮定は，等しくない場合の特殊な場合であるから，最初からWelchの補正がはいった検定だけで十分である。このような考え方から，Rにおけるt.test 関数のデフォルトではvar.equal = FALSEとなっており，特段の指定をしなければ等分散性の仮定をしない。こちらの方が検定を重ねることがないので，より望ましい。\n\n8.3.1 効果量の算出\n今回の例は，仮想データとして\\(\\mu_1 = 4,\\mu_2 = \\mu_1 + \\sigma d\\)であり，明らかに\\(\\mu_1 \\neq \\mu_2\\)なのだが，有意差を検出するには至らなかった。統計的な有意差はあくまでも「統計的な」観点からのものであり，我々が現実に検証したいのは本当に差があるかどうか，いわば「実質的な差」があるかどうかであるのだから，統計的な有意差を得ることを目的にするのははっきりと不適切な目標設定であると言えるだろう1。\nところで，統計的に差があるとはっきり言えるのはどのような時だろうか。これは次の4つのデータの分布を見てもらうとわかりやすい。\n\n\n\n\n\n\n\n\n\n左列は平均差が大きいデータ，右列は小さいデータである。 上段は分散が小さいデータ，下段は大きいデータである。 この4つそれぞれのシーンにおいて，「差がある」と判断しやすいのはどれかを考えてみるとよい。当然，左上のシーンが最も明確に差があると言えるであろう。なぜなら，両群が明確に分かれており，群間の重複がないからである。左下は同じ平均値差であっても，群内の広がりが大きいから群間の重複がみられるため，「差がある」という判断を受けても各群の中には該当しないケースがちらほらみられることだろう。右上パネルのようなケースでは，重複は少ないが差が小さいため，「差がある」と判断できるかどうかが微妙である。右下に至っては，差も小さく分布の重複も大きいから，「差がある」と判断しても該当しないケースが多くなる。たとえば「男性は女性よりも力が強い(体力・筋力に差がある)」というデータがあったとしても，「女性より非力な男性」もかなり多く存在するだろう。そういう反例が多くみられるような場合，統計的に差があるという結果が示されたとしても，受け入れられないのではないだろうか。\nここから明らかなように，差の判断には平均値差だけでなく分散も関わってくる。そこで平均値差を標準偏差で割った，標準化された差が重要になってくるのであり，これが効果量と呼ばれるものである2。\n今回2群の差のデータを作る時に，\\(\\sigma d\\) としたが，平均値差の効果量esは， \\[ es = \\frac{\\mu_1 - \\mu_2}{\\sigma} \\]\nで表現されるから，\\(d\\)が効果量を表していたのである。もちろん我々は母平均，母SDなどを知り得ないのでこれもデータから推定する他ない。幸いRにはeffsizeパッケージなど，効果量を算出するものが用意されている。\n\nlibrary(effsize)\ncohen.d(value ~ group, data = dataSet)\n\n\nCohen's d\n\nd estimate: -0.2232655 (small)\n95 percent confidence interval:\n    lower     upper \n-1.165749  0.719218 \n\ncohen.d(value ~ group, data = dataSet, hedges.correction = TRUE)\n\n\nHedges's g\n\ng estimate: -0.2138318 (small)\n95 percent confidence interval:\n     lower      upper \n-1.1162608  0.6885973 \n\n\n平均値差の検定の後は，ここに示したCohenのdやHedgesのgといった効果量を添えて報告することが一般的である。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#対応のある二標本検定",
    "href": "chapter08.html#対応のある二標本検定",
    "title": "8  平均値差の検定",
    "section": "8.4 対応のある二標本検定",
    "text": "8.4 対応のある二標本検定\n実験群と統制群のように異なる2群ではなく，プレポスト実験のように対応がある2群の場合は，t検定の定式化が異なる。対応がないt検定の場合は，群平均の差\\(\\mu_1 - \\mu_2\\)の分布を考えたが，対応がある場合は個々の測定の差，つまり\\(X_{i1} - X_{i2} = D_i\\)を考える。この一つの標本統計量を検定するのだから，一標本検定の一種であるとも言える。またこのDの分布の標準誤差は，標本標準誤差\\(U_D\\)を使った\\(U_D/\\sqrt{n}\\)を使って推定する3。検定統計量\\(T\\)は，次式で算出される。\n\\[ T = \\frac{\\bar{D}}{U_D/\\sqrt{n}} = \\frac{\\sum D_i/n}{\\sqrt{\\frac{\\frac{1}{n-1}\\sum(D_i-\\bar{D})^2}{n}}}\\]\n検定にあたっては，t.test関数の引数pairedをTRUEにするだけで良い。\n\n8.4.1 仮想データの組成\n仮想データを作って演習してみよう。データの組成については，2種類のアプローチで説明が可能である。ひとつは次のシミュレーションで表されるような形である。\n\nn &lt;- 10\nmu1 &lt;- 4\nsigma &lt;- 1\nd &lt;- 1\nX1 &lt;- rnorm(n, mu1, sigma)\nX2 &lt;- X1 + sigma * d + rnorm(n, 0, sigma)\nt.test(X1, X2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  X1 and X2\nt = -1.8036, df = 9, p-value = 0.1048\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.4339112  0.1617193\nsample estimates:\nmean difference \n     -0.6360959 \n\n\nすなわち，第一の群が\\(\\mu_1\\)を平均にばらついた実現値として得られ，第二の群はその実現値に一定の効果\\(\\sigma *d\\)が加わり，その測定にさらに誤差がつく形である。この方法は具体的なデータ生成プロセスをそのまま模したような形でデータを作っているが，測定誤差を二重に計上している点が気になるかもしれない。\nもう一つの考え方は，プレポスト型のデータに限らず，何らかの形で「対応がある」ことも表現できるものである。対応があるということは，2つのデータがそれぞれ独立した一変数正規分布から得られているのではなく，二変数正規分布から得られると考えるのである。二変数正規分布は，それぞれの変数は正規分布しているが，両者の間に相関があると考えるものである。変数が一つだけの正規分布は \\[X \\sim N(\\mu,\\sigma)\\] で表現されているのに対し，複数の変数を同時に生成する多変数(多次元)正規分布Multivariate Normal Distributionは，以下のように表現される。 \\[ \\mathbf{X} \\sim MVN(\\mathbf{\\mu},\\mathbf{\\Sigma})\\]\nここで\\(\\mathbf{X}\\)や\\(\\mathbf{\\mu}\\)は\\(n\\)次元ベクトルであり，\\(\\mathbf{\\Sigma}\\)は分散共分散行列を表している。二変数の場合は以下のように書くことができる。\n\\[\\mathbf{\\Sigma} = \\begin{pmatrix} \\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2 \\end{pmatrix} = \\begin{pmatrix} \\sigma_1^2 & \\rho_{12}\\sigma_1\\sigma_2 \\\\ \\rho_{21}\\sigma_2\\sigma_1 & \\sigma_2^2 \\end{pmatrix}\\]\n共分散\\(\\sigma_{ij}\\)は相関係数\\(\\rho_{ij}\\)を用いて書けることからわかるように，変数間に相関があることを想定してデータを生成するのである。この組成に従った仮想データの作成は以下のとおりである。\n\nlibrary(MASS) # 多次正規乱数を生成するのに必要\nn &lt;- 10\nmu1 &lt;- 4\nsigma &lt;- 1\nd &lt;- 1\nmu &lt;- c(mu1, mu1 + sigma * d)\nrho &lt;- 0.4\nSIG &lt;- matrix(c(sigma^2, rho * sigma * sigma, rho * sigma * sigma, sigma^2), ncol = 2, nrow = 2)\nX &lt;- mvrnorm(n, mu, SIG)\nt.test(X[, 1], X[, 2], paired = TRUE)\n\n\n    Paired t-test\n\ndata:  X[, 1] and X[, 2]\nt = -2.4313, df = 9, p-value = 0.0379\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.96934592 -0.07095313\nsample estimates:\nmean difference \n       -1.02015 \n\n\n効果量については，対応のないt検定の場合と同じで良い。\n\ncohen.d(X[, 1], X[, 2])\n\n\nCohen's d\n\nd estimate: -1.04088 (large)\n95 percent confidence interval:\n      lower       upper \n-2.04204357 -0.03971697 \n\ncohen.d(X[, 1], X[, 2], hedges.correction = TRUE)\n\n\nHedges's g\n\ng estimate: -0.9968994 (large)\n95 percent confidence interval:\n     lower      upper \n-1.9510179 -0.0427809 \n\n\n\n\n8.4.2 検定の方向性\nここまでの検定では，主に「差があるかどうか」といった仮説に対応するものを扱ってきた。差があるかどうか，というのはその差がプラスの方向にでているのか，マイナスの方向に出ているのかといったことを問題にしていない。そこで検定統計量の分布についても，分布の両裾を考えて有意水準を設定していた。\nしかしプレポスト実験などでは，効果が「上がった」のか「下がった」のか，ということが大きな関心時でもあることが多いだろう。効果がある，ただし逆効果である，というのでは意味がないからである。このように方向性をもった仮説を検証する場合は，検定統計量の分布も一方向だけ考えればよく，t.test関数にはalternativeオプションをつかって表現する。\nt.test(x,y,alternatives = \"less\") とすると\\(x &lt; y\\)の帰無仮説を検証することになるし，alternatives = \"greater\"とすると\\(x &gt; y\\)の帰無仮説を検証することになる。デフォルトではalternatives = \"two.sided\"であり，両側検定が選ばれている。\nただし，両裾から片裾にかわるということは，検定統計量が超えるかどうかの判断をする臨界値が小さくなることでもある。必然的に，片裾(片側検定)のほうが緩やかな基準で検定をしていることにもなる。デフォルトで普段から厳しく検定しているから大丈夫だろう，というのも一つの考え方だが，やはり本来の研究仮説に適した帰無仮説の設定をするべきだろう。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#課題",
    "href": "chapter08.html#課題",
    "title": "8  平均値差の検定",
    "section": "8.5 課題",
    "text": "8.5 課題\n\n平均が50、標準偏差が10の正規分布からランダムに選んだ30個のサンプルを用意し，このサンプルの平均が母集団の平均と異なるかどうかを検定してください。検定結果を，心理学のフォーマット(心理学会編「論文執筆投稿の手引き」)に準拠した書き方で，結果を記述してください。\n以下のデータセットを使用して，2つの独立した群の平均に差があるかどうかをt検定してください。検定結果を，心理学のフォーマット(心理学会編「論文執筆投稿の手引き」)に準拠した書き方で，結果を記述してください。 \\[ group1 =\\{45, 50, 55, 60, 65 \\} \\] \\[ group2 = \\{57, 60, 62, 77, 75 \\} \\]\n多次元正規分布を用いた仮想データ生成方で，対応のあるt検定の練習をしましょう。サンプルサイズを\\(n=20\\)とし, 平均ベクトル\\(\\mu = (12, 15)\\), 分散共分散行列\\(\\Sigma = \\begin{pmatrix} 4 & 2.8 \\\\ 2.8 & 4\\end{pmatrix}\\)の多次元正規分布から作られた乱数を使って，対応のあるt検定をしてください。検定結果を，心理学のフォーマット(心理学会編「論文執筆投稿の手引き」)に準拠した書き方で，結果を記述してください。\n自由度が10, 20, 30のt分布のグラフを，標準正規分布のグラフとともに描画してください。自由度が増えるとt分布がどのように変化するでしょうか。\n自由度が15のt分布において、有意水準5%の片側検定と両側検定の臨界値(検定の判断基準となる理論値)を求めてください。\n\n\n\n\n\n豊田秀樹. 2009. 検定力分析入門: Rで学ぶ最新データ解析ー. 東京図書.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#footnotes",
    "href": "chapter08.html#footnotes",
    "title": "8  平均値差の検定",
    "section": "",
    "text": "たとえば物理学などのシーンでは，測定の精度が高く，単一の物理世界を対象にした検証を行うのだから，予測が真であるか偽であるかを確率的に考えるような必要はない。そのような世界における検証–あえて理論的な正しさが明確な世界，と表現するが–であれば，統計的な差があるかどうかの情報はあくまでも理論を支持するおまけ情報にすぎない。いわば統計的検定の結果を報告するのは，論文を書くためのレトリックである。翻って，人間を対象にした小サンプルの科学である心理学は，統計的な判断に頼らざるを得ないという側面はあるだろう。しかしだからと言って，実質的な差が本質的であることを忘れてしまっては本末転倒である。↩︎\n統計的な有意差よりも効果量，効果量よりも実質的な差のほうが意味のある差であることを忘れてはならない。詳しくは 豊田 (2009) を参照。↩︎\n対応があるケースを考えているので，当然\\(n\\)は前後の群で同数である。↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html",
    "href": "chapter09.html",
    "title": "9  多群の平均値差の検定",
    "section": "",
    "text": "9.1 分散分析の基礎\n分散分析は「分散」の分析であるかのような名称であるが，平均値差を検定するためのものである。なぜ「分散」を冠するかといえば，効果量のところで見たように，平均値差の判断には群内分散の情報が必要だからである。\n多群の平均値の差，その散らばりを群間分散といい，群に含まれるデータの散らばりを群内分散とよぶ。分散分析は群内分散に対する群間分散の比が十分に大きいと考えられる場合，群間に統計的な有意差があると判断する。分散の比を表す確率分布はF分布と呼ばれる。F分布は群間・群内それぞれの自由度を母数にもつ。\nまた，実験計画はBetweenデザインとWithinデザインに区分される。t検定でみたような，対応のない独立した群を対象にしたデザインがBetween，群間に相関が想定される対応のある群を対象にしたデザインがWithinである。Withinデザインは同じ個体から複数回の反応を得る(ex. period 1-2-3…)ため，反復測定デザインRepeated measured designともよばれることがある。この場合，群内分散から個人内の分散すなわち個人差を取り出すことができるため，これが分離できないBetweenデザインよりも基本的にWithinデザインのほうが目的となる変動を捉えやすい。ただし，反復測定による個体への負担を考えると，毎回Withinデザインでいいというわけにもいかないところが難点である。\n\\[Between Design: \\text{全変動} = \\text{群間変動}+ \\text{群内変動(誤差)}\\] \\[Within Design:  \\text{全変動} = \\text{群間変動}+ \\text{個人差変動} + \\text{誤差}\\]\n分散分析は要因が複数ある場合も考えられるから，要因AがBetween，要因BがWithinといった場合は混合計画と呼ばれることがある。慣例的に，要因Factorとその要因に含まれる水準Levelを同時に表現し，\\(\\text{間}2 \\times \\text{間}3\\)の分散分析(二要因の分散分析で，いずれもBetweenデザインであり，水準数がそれぞれ2と3)，といった言い方をすることがある。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#分散分析のステップ",
    "href": "chapter09.html#分散分析のステップ",
    "title": "9  多群の平均値差の検定",
    "section": "9.2 分散分析のステップ",
    "text": "9.2 分散分析のステップ\nt検定において等分散性の仮定が成立するかどうかが事前に問題になったように，Betweenデザインにおいても分散の等質性は仮定されており，Leveneの検定などで事前に検証しておくべきである。またWithinデザインにおいては，データの組成に関わる分散共分散行列の非対角要素が全て等しいことが望ましいが，実践的にはそこまでの仮定が成立しているとは考えにくい。ただし分散分析としては，等分散性の仮定よりも，より緩やかな球面性の仮定が成立していればよいとされており，これを事前に検定することが一般的である。Welchの補正のように，球面性の仮定が成立していない場合は，自由度を補正することで検定の精度が維持される。\n分散分析は多要因・多水準の平均値差の検定である。各水準ごとにt検定を繰り返せば良いのではないか，というアイデアは誰しも思いつくことであろうが，この方法は検定の目的である\\(\\alpha\\)水準の制御ができなくなるという問題を含む。そこで多水準の場合は分散分析を行うことで，すべての要因・水準の母平均が同じであるという帰無仮説を検定し，効果の有無をまず明確にする。この帰無仮説が棄却されたらどこかに差があるわけだから，以後は慎重に\\(\\alpha\\)水準を制御しつつ事後的な検定にすすむ。\n水準間の差をみるための事後的な検定は，下位検定とも呼ばれる。その方法は多岐に渡り，ゴールドスタンダードは存在せず，往々にして分析者が利用しているソフトウェアが対応する手法が選択される。要因・水準が多くなると検証すべき組み合わせも多くなり，下位検定の手続きも非常に煩雑になる。統計ソフトウェアはそれこそ機械的に，幾重にも細かく分散分析表を分解して下位検定をつづけていってくれるが，いくら制御されているとはいえ検定を繰り返していることに変わりはないし，各下位検定の結果を一貫した総合的解釈をするのは困難である。実験計画はシンプルであるほうが望ましいし，複雑なモデルになるようであれば分散分析を超えた，階層線形モデルやベイジアンアプローチなどを取る方が良いだろう。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#anova君を使う",
    "href": "chapter09.html#anova君を使う",
    "title": "9  多群の平均値差の検定",
    "section": "9.3 ANOVA君を使う",
    "text": "9.3 ANOVA君を使う\n分散分析をRで実行するには，基本関数であるaovやcarパッケージなどを用いることができる。 もっとも，その出力は必ずしも親切ではないし，下位検定や効果量の算出などは別のパッケージ，別の関数を用いる必要がある。\n筆者がお勧めするのは，大正大学の井関龍太が開発したanovakunである。パッケージ化されていないので，リンク先からソースコードを読み込んでanovakun関数を実行する必要があるが，さまざまな実験デザインに対応し，また下位検定や効果量，球面性の補正などおよそ分散分析で必要な手法は網羅されている。以下ではこれを用いた実践を行う。\nanovakunの読み込みは，ソースコードをプロジェクトフォルダにダウンロードしてsource関数で読み込むか，インターネットに繋がっている状態でリンク先から直接ソースファイル(anovakun_489.txt)1をsource関数で読み込むといいだろう。\n\nsource(\"https://riseki.cloudfree.jp/?plugin=attach&refer=ANOVA%E5%90%9B&openfile=anovakun_489.txt\")\n\n読み込みが終わるとEnvironタブにanovakun関数が含まれていることを確認しよう。\n\n9.3.1 ANOVA君の入力とデータ\nANOVA君は伝統的にワイド型データから読み込むようになっている。すなわち，一行に1オブザベーション入っている形式である。Between計画の場合は，データの前に水準数を表すインデックスと最終的な従属変数の形に整形したデータが必要である。Within計画の場合は1行に1Obs.なのだから，反復した水準の数だけ右にデータを入れていく形に整形する。\nしかしChapter 3.7 で述べたように，昨今は計算機にとって優しい型，ロング型での入力もおおく，ANOVA君もversion 4.4.0からロング型での入力も許すようになった。その場合はオプションlong=TRUE とロング型であることを明記する必要がある。\nANOVA君を使う時は，関数anovakunに，データ，要因計画の型，各要因の水準の順で入力する。ここで要因計画の型とは，文字列でBetween/Withinの違いを明示することになる。被験者のラベルを表す小文字のsを挟んで，左側に間(Between)要因，右側に内(Within)要因を入れる。例えば一要因Between計画の場合は\"As\"，二要因Within計画の場合は\"sAB\"，間1内2の混合計画であれば\"AsBC\"のようにする。\n続いて入力する水準数は，要因の数だけ必要である。ただし，ロング型で入力した場合は自動的に水準数が計算されるので入力の必要がない。\nこのテキストでは，データの持ち替えについてすでに触れているので，色々扱いやすいロング型に整形して利用していくものとする。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#betweenデザイン",
    "href": "chapter09.html#betweenデザイン",
    "title": "9  多群の平均値差の検定",
    "section": "9.4 Betweenデザイン",
    "text": "9.4 Betweenデザイン\n\n9.4.1 1way-ANOVA\nもっとも単純な一要因3水準，Between計画の例から始めよう。仮想データの生成を行うことで，分散分析のメカニズムと共に見ていくことにする。\n\nset.seed(123)\n# 各群のサンプルサイズ\nn1 &lt;- 5\nn2 &lt;- 4\nn3 &lt;- 6\n# 母平均，効果量，母SD\nmu &lt;- 10\ndelta &lt;- 1\nsigma &lt;- 3\n# 群平均\nmu1 &lt;- mu - (delta * sigma)\nmu2 &lt;- mu\nmu3 &lt;- mu + (delta * sigma)\n# データセット\nX1 &lt;- rnorm(n1, mu1, sigma)\nX2 &lt;- rnorm(n2, mu2, sigma)\nX3 &lt;- rnorm(n3, mu3, sigma)\n## 組み上げる\ndat &lt;- data.frame(\n  ID = 1:(n1 + n2 + n3),\n  group = as.factor(rep(LETTERS[1:3], c(n1, n2, n3))),\n  value = c(X1, X2, X3)\n)\n## データの確認\ndat\n\n   ID group     value\n1   1     A  5.318573\n2   2     A  6.309468\n3   3     A 11.676125\n4   4     A  7.211525\n5   5     A  7.387863\n6   6     B 15.145195\n7   7     B 11.382749\n8   8     B  6.204816\n9   9     B  7.939441\n10 10     C 11.663014\n11 11     C 16.672245\n12 12     C 14.079441\n13 13     C 14.202314\n14 14     C 13.332048\n15 15     C 11.332477\n\n### 実行\nanovakun(dat, \"As\", long = TRUE, peta = TRUE)\n\n\n[ As-Type Design ]\n\nThis output was generated by anovakun 4.8.9 under R version 4.4.1.\nIt was executed on Mon Aug  5 15:36:31 2024.\n\n \n&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;\n\n------------------------------\n group   n     Mean    S.D. \n------------------------------\n     A   5   7.5807  2.4331 \n     B   4  10.1681  3.9548 \n     C   6  13.5469  1.9483 \n------------------------------\n\n\n&lt;&lt; ANOVA TABLE &gt;&gt;\n\n== This data is UNBALANCED!! ==\n== Type III SS is applied. ==\n\n--------------------------------------------------------------\n Source       SS  df      MS  F-ratio  p-value      p.eta^2 \n--------------------------------------------------------------\n  group  98.3840   2 49.1920   6.5897   0.0117 *     0.5234 \n  Error  89.5804  12  7.4650                                \n--------------------------------------------------------------\n  Total 187.9644  14 13.4260                                \n                  +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\n\n&lt;&lt; POST ANALYSES &gt;&gt;\n\n&lt; MULTIPLE COMPARISON for \"group\" &gt;\n\n== Shaffer's Modified Sequentially Rejective Bonferroni Procedure ==\n== The factor &lt; group &gt; is analysed as independent means. == \n== Alpha level is 0.05. == \n \n------------------------------\n group   n     Mean    S.D. \n------------------------------\n     A   5   7.5807  2.4331 \n     B   4  10.1681  3.9548 \n     C   6  13.5469  1.9483 \n------------------------------\n\n-------------------------------------------------------\n Pair     Diff  t-value  df       p   adj.p          \n-------------------------------------------------------\n  A-C  -5.9662   3.6062  12  0.0036  0.0108  A &lt; C * \n  B-C  -3.3789   1.9159  12  0.0795  0.0795  B = C   \n  A-B  -2.5873   1.4117  12  0.1834  0.1834  A = B   \n-------------------------------------------------------\n\n\noutput is over --------------------///\n\n\n出力結果は大きく分けて記述統計&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;と，分散分析表&lt;&lt; ANOVA TABLE &gt;&gt;，下位検定&lt;&lt; POST ANALYSES &gt;&gt;に分けられる。記述統計はデータが正しく読み込めているかどうかのチェックに使おう。\n一番のメインは分散分析表であり，平方和sum of squaresを自由度dfで割った，1自由度あたりのデータの散らばりを，群間と群内(誤差)との比で検証しているのが見て取れる。群間平方和が\\(98.38\\)，群内平方和が\\(89.58\\)であり，それぞれ自由度\\(2\\)(\\(3\\)水準\\(-1\\))と\\(12\\)(\\(\\sum_{j=1}^3 n_j-1\\))から生じているので，平均平方Mean Squaresがそれぞれ\\(49.19\\)と\\(7.47\\)である。この比が\\(6.5897\\)で，自由度\\(F(2,12)\\)のF分布においてこの値以上の極端な数字が出る確率が5%を下回っている(実に\\(p=0.0117\\)である)ため，統計的に有意であると判断できる。 分散分析表のTotalのところで，全体のSSが群間SS+群内SSに一致していること，自由度も全体df=群間df+群内dfになっていることを確認しておこう。\nまた，anovakun関数の引数としてpeta = TRUEを指定したが，これは偏\\(\\eta^2\\)(partial eta)と呼ばれる効果量を出力するためのオプションである。\n今回は分散分析の時点で統計的な有意差が認められたため(\\(F(2,12)=6.59, p &lt; 0.05, \\eta^2=0.52\\))，続いて下位検定が表示されている。ANOVA君は下位検定についても複数のオプションを持っているが，デフォルトではShafferの修正Bonferroni検定が行われる。詳しくは専門書(永田 and 吉田 1997)を参照してほしいが，概略を説明すると，検証すべき仮説の数で有意水準を分割するというBonferroniの方法を，競合する仮説の数も考慮して分母を調整するというものである。\nこの計算の結果，A群とC群の間にのみ統計的な有意差が確認された(\\(t(12)=3.61,p&lt;0.05\\))と言える。\n\n\n9.4.2 2way-ANOVA\n二要因の場合も見ておこう。ANOVA君の表記方法は要因計画の型が変わるだけで大きな変更はないが，交互作用interactionを考える必要があるところがポイントである。これも仮想データの組成を見ることでその意義がわかりやすくなるだろう。間2\\(\\times\\)間2の実験デザインを例に，まずは各水準の理論的平均値がどのようにつくられるかをみておこう。\n\nset.seed(123)\n# 各群のサンプルサイズ\nn &lt;- 10\n# 全体平均，効果量，母SD\nmu &lt;- 10\ndelta1 &lt;- 1\ndelta2 &lt;- 0 # ここではあえて要因Bの効果を0にしている\ndelta3 &lt;- 2\nsigma &lt;- 3\n# 効果の計算\neffectA &lt;- delta1 * sigma # Factor A\neffectB &lt;- delta2 * sigma # Factor B\neffectAB &lt;- delta3 * sigma # interaction\n# 各群の平均\nmu11 &lt;- mu + effectA + effectB + effectAB\nmu12 &lt;- mu + effectA - effectB - effectAB\nmu21 &lt;- mu - effectA + effectB - effectAB\nmu22 &lt;- mu - effectA - effectB + effectAB\n\n効果の現れ方は相対的だから，要因Aが第一水準に+effectAの形で現れたら，第二水準には-effectA の形で現れる。要因Bについても同様である。交互作用については組み合わせにおいて生じるから，要因Aの第一水準と要因Bの第一水準の組み合わせのところに+effectABを充てる。ここでも効果は相対的に現れるという条件を守るために，要因Aの第一水準の中で+effectABの効果を相殺するために，要因Aの第一水準と要因Bの第二水準の組み合わせの符号が反転する。同様に，要因Bの第一水準の中で相殺するために要因Aの第二水準と要因Bの第一水準には-effectABが加わる。\nこのようにして考えられる理論的平均値に対して，外乱要因である誤差が生じて実現値が得られる。 組み上げて得られたデータを確認しておこう。\n\nX11 &lt;- rnorm(n, mean = mu11, sd = sigma)\nX12 &lt;- rnorm(n, mean = mu12, sd = sigma)\nX21 &lt;- rnorm(n, mean = mu21, sd = sigma)\nX22 &lt;- rnorm(n, mean = mu22, sd = sigma)\ndat &lt;- data.frame(\n  ID = 1:(n * 4),\n  FactorA = rep(1:2, each = n * 2),\n  FactorB = rep(rep(1:2, each = n), 2),\n  value = c(X11, X12, X21, X22)\n)\ndat\n\n   ID FactorA FactorB      value\n1   1       1       1 17.3185731\n2   2       1       1 18.3094675\n3   3       1       1 23.6761249\n4   4       1       1 19.2115252\n5   5       1       1 19.3878632\n6   6       1       1 24.1451950\n7   7       1       1 20.3827486\n8   8       1       1 15.2048163\n9   9       1       1 16.9394414\n10 10       1       1 17.6630141\n11 11       1       2 10.6722454\n12 12       1       2  8.0794415\n13 13       1       2  8.2023144\n14 14       1       2  7.3320481\n15 15       1       2  5.3324766\n16 16       1       2 12.3607394\n17 17       1       2  8.4935514\n18 18       1       2  1.1001485\n19 19       1       2  9.1040677\n20 20       1       2  5.5816258\n21 21       2       1 -2.2034711\n22 22       2       1  0.3460753\n23 23       2       1 -2.0780133\n24 24       2       1 -1.1866737\n25 25       2       1 -0.8751178\n26 26       2       1 -4.0600799\n27 27       2       1  3.5133611\n28 28       2       1  1.4601194\n29 29       2       1 -2.4144108\n30 30       2       1  4.7614448\n31 31       2       2 14.2793927\n32 32       2       2 12.1147856\n33 33       2       2 15.6853770\n34 34       2       2 15.6344005\n35 35       2       2 15.4647432\n36 36       2       2 15.0659208\n37 37       2       2 14.6617530\n38 38       2       2 12.8142649\n39 39       2       2 12.0821120\n40 40       2       2 11.8585870\n\n\nもちろん実際には，計画に応じたデータセットが得られているはずであり，各群のサンプルサイズが異なるなどの事情もあるだろう。しかしこうして，理論的にデータの組成を見ておくことで，サンプルサイズを変えたり効果量を変えたりしながら，どのように結果が変わってくるかを確認しながら進めることができる2。\nそれではこの仮想データを分析してみよう。\n\nanovakun(dat, \"ABs\", long = TRUE, peta = TRUE)\n\n\n[ ABs-Type Design ]\n\nThis output was generated by anovakun 4.8.9 under R version 4.4.1.\nIt was executed on Mon Aug  5 15:36:31 2024.\n\n \n&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;\n\n-----------------------------------------\n FactorA  FactorB   n     Mean    S.D. \n-----------------------------------------\n       1        1  10  19.2239  2.8614 \n       1        2  10   7.6259  3.1142 \n       2        1  10  -0.2737  2.7924 \n       2        2  10  13.9661  1.5819 \n-----------------------------------------\n\n\n&lt;&lt; ANOVA TABLE &gt;&gt;\n\n---------------------------------------------------------------------------\n           Source        SS  df        MS  F-ratio  p-value      p.eta^2 \n---------------------------------------------------------------------------\n          FactorA  432.7854   1  432.7854  61.4190   0.0000 ***   0.6305 \n          FactorB   17.4478   1   17.4478   2.4761   0.1243 ns    0.0644 \nFactorA x FactorB 1668.9825   1 1668.9825 236.8545   0.0000 ***   0.8681 \n            Error  253.6721  36    7.0464                                \n---------------------------------------------------------------------------\n            Total 2372.8878  39   60.8433                                \n                               +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\n\n&lt;&lt; POST ANALYSES &gt;&gt;\n\n&lt; SIMPLE EFFECTS for \"FactorA x FactorB\" INTERACTION &gt;\n\n----------------------------------------------------------------------\n      Source        SS  df        MS  F-ratio  p-value      p.eta^2 \n----------------------------------------------------------------------\nFactorA at 1 1900.7730   1 1900.7730 269.7492   0.0000 ***   0.8823 \nFactorA at 2  200.9950   1  200.9950  28.5243   0.0000 ***   0.4421 \nFactorB at 1  672.5693   1  672.5693  95.4480   0.0000 ***   0.7261 \nFactorB at 2 1013.8610   1 1013.8610 143.8826   0.0000 ***   0.7999 \n       Error  253.6721  36    7.0464                                \n----------------------------------------------------------------------\n                          +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\noutput is over --------------------///\n\n\n基本的な結果の見方については，一要因のときと同じである。今回は要因Aと交互作用の効果を作り，正しく検出されている。下位検定については，要因Aが2水準であったためこちらの主効果の検証は必要なく(記述統計を見て群平均比較をすればよい)，交互作用についての単純効果の検証が行われている。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#withinデザイン",
    "href": "chapter09.html#withinデザイン",
    "title": "9  多群の平均値差の検定",
    "section": "9.5 Withinデザイン",
    "text": "9.5 Withinデザイン\nWithinデザインは対応のあるt検定の時と同じように，多次元正規分布からの生成として考えよう。すなわち各個体から得られるデータが相関しているという仮定をおくのである。以下のサンプルコードを読んで，データ生成過程を確認しよう。なお共分散は\\(\\rho_{xy}=\\frac{s_{xy}}{s_xs_y}\\)より\\(s_{xy}=\\rho_{xy}s_xs_y\\)として整形している。\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(MASS)\n\n\n次のパッケージを付け加えます: 'MASS'\n\n以下のオブジェクトは 'package:dplyr' からマスクされています:\n\n    select\n\nset.seed(42)\n# 各群のサンプルサイズ\nn &lt;- 10\n# 全体平均，効果量，母SD\nmu &lt;- 10\ndelta &lt;- 1\ns1 &lt;- s2 &lt;- s3 &lt;- 1\nrho12 &lt;- 0.1\nrho13 &lt;- 0.3\nrho23 &lt;- 0.8\nmus &lt;- c(mu, mu + s1 * delta, mu - s1 * delta)\n# 共分散行列の生成\nSigma &lt;- matrix(NA, ncol = 3, nrow = 3)\nSigma[1, 1] &lt;- s1^2\nSigma[2, 2] &lt;- s2^2\nSigma[3, 3] &lt;- s3^2\nSigma[1, 2] &lt;- Sigma[2, 1] &lt;- rho12 * s1 * s2\nSigma[1, 3] &lt;- Sigma[3, 1] &lt;- rho13 * s1 * s3\nSigma[2, 3] &lt;- Sigma[3, 2] &lt;- rho23 * s2 * s3\n# データの生成\nX &lt;- mvrnorm(n, mus, Sigma) %&gt;% as.data.frame()\n# データの確認\nX\n\n          V1        V2       V3\n1  10.625304  9.418518 7.493325\n2  12.437964 11.249993 8.806719\n3   8.604481 11.182418 8.722798\n4   9.390742 10.181310 8.786312\n5   9.567609 10.147592 9.194809\n6  10.651739 11.005419 8.917299\n7   9.125913  9.805634 7.511082\n8   7.770294 12.462671 8.790231\n9   6.909722  9.863405 7.429485\n10 11.267590 10.798088 8.754522\n\n# Long型に整形\nX &lt;- X %&gt;%\n  rowid_to_column(\"ID\") %&gt;%\n  pivot_longer(-ID) %&gt;%\n  print()\n\n# A tibble: 30 × 3\n      ID name  value\n   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1 V1    10.6 \n 2     1 V2     9.42\n 3     1 V3     7.49\n 4     2 V1    12.4 \n 5     2 V2    11.2 \n 6     2 V3     8.81\n 7     3 V1     8.60\n 8     3 V2    11.2 \n 9     3 V3     8.72\n10     4 V1     9.39\n# ℹ 20 more rows\n\n# 分析の実行\nanovakun(X, \"sA\", long = TRUE, peta = TRUE, GG = TRUE)\n\n\n[ sA-Type Design ]\n\nThis output was generated by anovakun 4.8.9 under R version 4.4.1.\nIt was executed on Mon Aug  5 15:36:31 2024.\n\n \n&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;\n\n-----------------------------\n name   n     Mean    S.D. \n-----------------------------\n   V1  10   9.6351  1.6609 \n   V2  10  10.6115  0.9057 \n   V3  10   8.4407  0.6777 \n-----------------------------\n\n\n&lt;&lt; SPHERICITY INDICES &gt;&gt;\n\n== Mendoza's Multisample Sphericity Test and Epsilons ==\n\n-------------------------------------------------------------------------\n Effect  Lambda  approx.Chi  df      p         LB     GG     HF     CM \n-------------------------------------------------------------------------\n   name  0.0068      8.8720   2 0.0118 *   0.5000 0.5988 0.6392 0.5547 \n-------------------------------------------------------------------------\n                              LB = lower.bound, GG = Greenhouse-Geisser\n                             HF = Huynh-Feldt-Lecoutre, CM = Chi-Muller\n\n\n&lt;&lt; ANOVA TABLE &gt;&gt;\n\n--------------------------------------------------------------\n  Source      SS  df      MS  F-ratio  p-value      p.eta^2 \n--------------------------------------------------------------\n       s 16.4609   9  1.8290                                \n--------------------------------------------------------------\n    name 23.6422   2 11.8211  10.7022   0.0009 ***   0.5432 \ns x name 19.8819  18  1.1045                                \n--------------------------------------------------------------\n   Total 59.9849  29  2.0684                                \n                  +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\n\n&lt;&lt; POST ANALYSES &gt;&gt;\n\n&lt; MULTIPLE COMPARISON for \"name\" &gt;\n\n== Shaffer's Modified Sequentially Rejective Bonferroni Procedure ==\n== The factor &lt; name &gt; is analysed as dependent means. == \n== Alpha level is 0.05. == \n \n-----------------------------\n name   n     Mean    S.D. \n-----------------------------\n   V1  10   9.6351  1.6609 \n   V2  10  10.6115  0.9057 \n   V3  10   8.4407  0.6777 \n-----------------------------\n\n----------------------------------------------------------\n  Pair     Diff  t-value  df       p   adj.p            \n----------------------------------------------------------\n V2-V3   2.1708   9.5342   9  0.0000  0.0000  V2 &gt; V3 * \n V1-V3   1.1945   2.3896   9  0.0406  0.0406  V1 &gt; V3 * \n V1-V2  -0.9764   1.6250   9  0.1386  0.1386  V1 = V2   \n----------------------------------------------------------\n\n\noutput is over --------------------///\n\n\n上記コードについていくつか解説をしておこう。 今回，各群の分散は同じにしつつ，変数間相関に大きな違いを持たせた。あえて球面性の仮定が成立しないような例を得たかったからで，出力の&lt;&lt; SPHERICITY INDICES &gt;&gt;をみると統計量\\(\\lambda\\)のあとの\\(p\\)値が5%を下回っており，「球面性が成立している」という帰無仮説が棄却されていることがわかる。この時いくつかの補正法があるが，今回はGreenhouse-Geisserの補正を当てることにしている。それがanovakun関数のなかのGG=TRUEの箇所である。\nこれを踏まえて分散分析表が示されている。ここでも全体平方和SS，自由度dfが各行の要素の和になっていることが確認できるが，その因子名のところにsが含まれていることが確認できる。これが個体ごとの変動を表しており，誤差から個人差を取り除いて効果の検証ができていることがわかる。\n分散分析は加法的，線形的な分解であるからわかりやすく，要因が複雑に組み合わさることがあっても基本的に今回のパーツを組み上げることで理解できる。言い換えると，データが先にある実践的な場合には平方和をひとつひとつ丁寧に紐解いていくことで理解できる。実にanovakunの前進であるanova4では4要因，anovakunでは26要因までのデザインを分析することが可能である。もっとも4要因計画にもなると3次の交互作用まで考えられ，主効果と合わせてこれらの交互作用効果を解釈するのは困難である。anoakunは2次以上の交互作用が見られた場合，自動的に下位検定を行ってくれないので，要因の水準ごとにデータを分割して，分散分析表を解体しつつ分析する必要がある。3\nしかしもちろん，これには検定の多重性の問題が関わってくるから，あまり推奨される手法ではない。ごく少ない要因で，主効果の有無を検証することを主眼においた丁寧な実験デザインを組み立てることを試みるべきである。\nまたここでは，仮想データを作ることで，得られたデータの背後にある生成メカニズムに注目した。「与えられたデータを分解する」のが分散分析であるのに対し，リバースエンジニアリングからアプローチしたのである。こうすることで，分散分析の見えない仮定に注意が向くことを期待している。簡便のために，いくつかのパラメータを均質化するなどしたが，実践的には群ごとのサンプルサイズが異なることも少なくないだろうし，群間の分散や共分散が均質であることを前提とするのは難しいだろう。これを考慮した細かい作り込みも，リバースエンジニアリングによって生成メカニズムがわかっている場合には応用が可能である。さらに，どこの水準間にどのような効果があると仮定されるか，といった精緻な仮説があるのなら，そこだけをターゲットにした分析を行うことも可能である。\n分散分析は，あくまでも大雑把な全体的傾向を見るためのものであることに留意しよう。心理学のデータがより精緻な仮定に耐えうる精度を持つものになれば，分散分析は過去の遺物となるかもしれない。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#課題",
    "href": "chapter09.html#課題",
    "title": "9  多群の平均値差の検定",
    "section": "9.6 課題",
    "text": "9.6 課題\n\n以下のデータセットは一要因4水準Between計画で得られたものです。分散分析を行って，要因の効果があるかどうか，水準間に差があるとすればどこに見られるかを報告してください。なおこのデータセットはこちらex_anova1.csvからダウンロード可能です。\n\n\n\n   ID group value\n1   1     A 14.37\n2   2     A 15.11\n3   3     A 16.11\n4   4     A 11.17\n5   5     A 14.51\n6   6     A  7.85\n7   7     A 10.65\n8   8     B 16.45\n9   9     B 11.76\n10 10     B 19.11\n11 11     B 19.62\n12 12     C  2.92\n13 13     C  6.27\n14 14     C  1.82\n15 15     C -0.10\n16 16     C  5.30\n17 17     C  1.57\n18 18     D  8.33\n19 19     D  2.71\n20 20     D  5.97\n21 21     D  4.97\n22 22     D  1.65\n23 23     D  8.73\n24 24     D  5.93\n25 25     D  4.27\n\n\n\n以下のデータセットは一要因4水準Within計画で得られたものです。分散分析を行って，要因の効果があるかどうか，水準間に差があるとすればどこに見られるかを報告してください。なおこのデータセットはこちらex_anova2.csvからダウンロード可能です。\n\n\n\n      V1    V2    V3    V4\n1  11.32 12.99  9.34 -0.14\n2  10.77 13.84 14.74  3.52\n3   9.86 12.26 12.56  2.60\n4   8.74 11.59 14.27  0.68\n5  11.12 12.93 12.92  1.13\n6   9.65 16.55 12.60  2.32\n7   9.72 14.64  9.69 -1.34\n8  12.02 11.18 14.43  2.64\n9  10.00 10.79  9.19 -1.09\n10 10.04 15.53 13.38  1.82\n11 10.20 11.56 11.02 -0.05\n12  7.81  9.29 12.20 -3.25\n\n\n\n間(3)\\(\\times\\)間(3)の分散分析モデルの仮想データセットを作りましょう。そのデータに分散分析を適用し，仮定した要因の効果がみられるか(あるいは効果がないと仮定した場合に正しく検出されないか)を確認しましょう。\n【発展課題】二要因混合計画分散分析(間\\(\\times\\)内)の仮想データセットを作りましょう。そのデータに分散分析を適用し，仮定した要因の効果がみられるか(あるいは効果がないと仮定した場合に正しく検出されないか)を確認しましょう。\n\n\n\n\n\n永田靖, and 吉田道弘. 1997. 統計的多重比較法の基礎. サイエンティスト社.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#footnotes",
    "href": "chapter09.html#footnotes",
    "title": "9  多群の平均値差の検定",
    "section": "",
    "text": "2024.06.12時点での最新バージョンが4.8.9である。リンク先URLは，公式サイトからソースファイルのリンクをコピーして貼り付けると良い。↩︎\nかつては分散分析は手計算でできる分析モデルであり，得られたデータを平方和に分解していくプロセスをたどりながら分散分析のメカニズムが体得されるという教育が多く見られた。ただしその方法は計算に時間がかかること，ミスが混在しやすいことに加え，手元のデータが唯一無二のものであるという印象を強くすることが懸念される。推測統計学においては，手元のデータはあくまでも実現値に過ぎないと考えるのであり，乱数を生成して幾つでも自在に作り出せる経験を得た方が教育効果として良いのではないか，と筆者は考えている。↩︎\nanovakunの補助関数anovatanを用いることで，注目したい要因ごとにデータを分割してくれる。詳しくは公式サイトのマニュアルを参照してほしい。↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter10.html",
    "href": "chapter10.html",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "10.1 疑わしき研究実践 Questionable Research Practicies",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#疑わしき研究実践-questionable-research-practicies",
    "href": "chapter10.html#疑わしき研究実践-questionable-research-practicies",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "10.1.1 検定の繰り返し\n帰無仮説検定は確率を伴った判断なので，「差がないのにあると判断してしまった(タイプ1エラー)」とか，「差があるのに検出できなかった(タイプ2エラー)」といった問題が生じうる。すでに述べたように，タイプ2エラーの方は本質的に知り得ないので(差がどの程度あるか，事前にわかっていることがない)，せめてタイプ1エラーは制御することを目指すことになる。\nこうした検定は合理的に行われるべきもので，なんとか有意に「したい」といった研究者のお気持ちとは独立しているはずである。しかし(もしかすると)意図せぬところで，この制御に失敗してしまっている可能性がある。\nひとつは検定の繰り返しに関する問題である。たとえば分散分析において，「主効果が出てから下位検定で各ペアの検証をするんだから，最初から各ペアのt検定を繰り返せばいいじゃないか」と考える人がいるかもしれない。これで本当に問題ないのか，シミュレーションで確認してみよう。\n以下のコードは，有意差のないデータセットを作り，1.分散分析を行なって有意になるかどうか，2.各ペアについて繰り返しt検定を行い，どこかに有意差が検出されるかどうか，を比較している。分散分析はANOVA君ではなく，R固有のaov関数を用いた1。また，「どこかに有意差が検出される」をif文を使って書いているところを，注意深く確認しておいてほしい。\n\nlibrary(tidyverse)\nlibrary(broom) # 分析結果をtidyに整形するパッケージ。ない場合はinstallしておこう\n\n\nalpha &lt;- 0.05 # 有意水準を0.05に設定\nn1 &lt;- n2 &lt;- n3 &lt;- 10 # 各グループのサンプルサイズを10に設定\nmu &lt;- 10 # 平均値を10に設定\nsigma &lt;- 2 # 標準偏差を2に設定\n\nmu1 &lt;- mu2 &lt;- mu3 &lt;- mu # 各グループの平均値を同じに設定\n\nset.seed(12345) # 乱数のシードを設定して再現性を確保\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\n\nanova.detect &lt;- rep(NA, iter) # ANOVA検出結果の保存用ベクトルを初期化\nttest.detect &lt;- rep(NA, iter) # t検定検出結果の保存用ベクトルを初期化\n\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  X1 &lt;- rnorm(n1, mu1, sigma) # グループ1のデータを生成\n  X2 &lt;- rnorm(n2, mu2, sigma) # グループ2のデータを生成\n  X3 &lt;- rnorm(n3, mu3, sigma) # グループ3のデータを生成\n\n  dat &lt;- data.frame( # データフレームを作成\n    group = c(rep(1, n1), rep(2, n2), rep(3, n3)), # グループ番号を追加\n    value = c(X1, X2, X3) # データを追加\n  )\n  result.anova &lt;- aov(value ~ group, data = dat) %&gt;% tidy() # ANOVAを実行し結果を整形\n  anova.detect[i] &lt;- ifelse(result.anova$p.value[1] &lt; alpha, 1, 0) # 有意差があるかを判定して保存\n\n  # t検定を繰り返す\n  ttest12 &lt;- t.test(X1, X2)$p.value # グループ1と2のt検定\n  ttest13 &lt;- t.test(X1, X3)$p.value # グループ1と3のt検定\n  ttest23 &lt;- t.test(X2, X3)$p.value # グループ2と3のt検定\n\n  ttest.detect[i] &lt;- ifelse(ttest12 &lt; alpha | ttest13 &lt; alpha | ttest23 &lt; alpha, 1, 0) # いずれかのt検定で有意差があれば保存\n}\n\nttest.detect %&gt;% mean() # t検定で有意差が検出された割合を計算\n\n[1] 0.109\n\nanova.detect %&gt;% mean() # ANOVAで有意差が検出された割合を計算\n\n[1] 0.04\n\n\n結果を見ると，t検定で有意差が検出された確率が0.109であり，設定した\\(\\alpha\\)水準を大きく上回っていることがわかる。有意でないところに有意差を見出しているのだから，これはタイプ1エラーのインフレである。分散分析で検出された結果は0.04であり，正しく\\(\\alpha\\)水準がコントロールできている。\n検定を繰り返すことの問題は，確率的判断にある。5%の水準でタイプ1エラーが起こるということは，95%の確率で正しく判断できるということだが，2回検定を繰り返すとその精度は\\((1-0,05)^2=0.9025\\)であり，3回検定を繰り返すと\\((1-0.05)^3=0.857375\\)と，どんどん小さくなっていってしまう。検定はタイプ1エラーのハンドリングが目的であったことを忘れてはならない。\n\n\n10.1.2 ボンフェロー二の方法\n一つの論文のなかに複数の研究(Study1, Study2,…)があり，それぞれで検定による確率的判断を行っているとしよう。それぞれ別のデータセットに対する検定であっても，一つの露文の中で確率的判断が繰り返されていることに違いはない。このような場合は，どのようにして有意水準をコントロールすれば良いのだろうか。\n最も単純明快な方法のひとつは，分散分析の下位検定でもみられたBonferroniの補正である。すなわち，検定の回数で有意水準を割ることで，検定を厳しくするのである。5%水準の検定を5回繰り返すのなら，\\(0.05/5=0.01\\)とすることで全体的なタイプ1エラー率を抑制するのである。これが正しく機能するかどうか，シミュレーションで確認してみよう。\n反復してデータを生成することになるので，仮想データ生成関数を別途事前に準備しておこう。\n\n# シミュレーション用の関数を定義\nstudyMake &lt;- function(n, mu, sigma, delta) {\n  X1 &lt;- rnorm(n, mu, sigma) # グループ1のデータを生成\n  X2 &lt;- rnorm(n, mu + sigma * delta, sigma) # グループ2のデータを生成（平均値が異なる）\n  dat &lt;- data.frame( # データフレームを作成\n    group = rep(1:2, each = n), # グループ番号を追加\n    value = c(X1, X2) # データを追加\n  )\n  result &lt;- t.test(X1, X2)$p.value # グループ間のt検定を実行\n  return(result) # p値を返す\n}\n\nこの関数は，引数としてサンプルサイズn，平均値mu，標準偏差sigma，効果量deltaをとり，2群のt検定の結果である\\(p\\)値を返す関数である。\n\n# 使用例；t検定の結果のp値が戻ってくる\nstudyMake(n = 10, mu = 10, sigma = 1, delta = 0)\n\n[1] 0.9444895\n\n\nこれ一回で1分析するので，これを複数回行って一つの研究とし，一つの論文のなかでnum_studies回の研究を行ったとしよう。今回はnum_studies = 3としている。Rのreplicate関数で研究回数繰り返した\\(p\\)値ベクトルを得て，どこかに差が検出されるかどうかをチェックする。「どこかに」を表現するためにany関数を使って判定する。判定する有意水準として，\\(\\alpha\\)と補正をかけた\\(\\alpha_{adj}\\)の2つを用意した。\n\nset.seed(12345) # 乱数のシードを設定して再現性を確保\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\nalpha &lt;- 0.05 # 有意水準を0.05に設定\nnum_studies &lt;- 3 # 研究の数を3に設定\nalpha_adjust &lt;- alpha / num_studies # 多重検定補正後の有意水準を計算\n\nFLG.detect &lt;- rep(NA, iter) # 検出結果を保存するベクトルを初期化\nFLG.detect.adj &lt;- rep(NA, iter) # 補正後の検出結果を保存するベクトルを初期化\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  p_values &lt;- replicate(num_studies, studyMake(n = 10, mu = 10, sigma = 1, delta = 0)) # 各研究のp値を生成\n  FLG.detect[i] &lt;- ifelse(any(p_values &lt; alpha), 1, 0) # 補正前の有意差検出を判定して保存\n  FLG.detect.adj[i] &lt;- ifelse(any(p_values &lt; alpha_adjust), 1, 0) # 補正後の有意差検出を判定して保存\n}\n\nFLG.detect %&gt;% mean() # 補正前の有意差検出率を計算\n\n[1] 0.145\n\nFLG.detect.adj %&gt;% mean() # 補正後の有意差検出率を計算\n\n[1] 0.049\n\n\n結果を見ると，\\(\\alpha\\)水準のまま検定を行うと，論文全体でのタイプ1エラー率が0.145と5%を上回っており，3つの研究のどこかで間違った判断をしていることがわかる。補正すると0.049と正しく制御されている。\n一連の研究をまとめた一つの論文に，複数の研究が含まれていることは少なくない。各検定結果をまとめて総合考察とすることも一般的である。総合考察は各分析結果から全体的な結論を導くのだが，その要素のどこかに間違いがあると，全体の論立てが崩れてしまうことにもなりかねない。いわば腐った支柱が紛れ込んでいる土台の上に家屋を建てるようなもので，研究の積み重ねを目的とする科学活動の一環である以上，正しく制御されていることは重要である。\n\n\n10.1.3 N増し問題\n人間を対象にした研究を行って，データを一生懸命取る。その結果，効果があると見られた操作/介入から統計的な有意差が検出されなければ，「悔しい」という心情になることは理解できる。もう少し実験を工夫すれば，もう少しデータが違えばよかったのでは，と思うかもしれない。ではもう少し頑張ってデータを増やしてみればどうだろうか。\n実はこの考え方はQRPsのひとつである。検定は真偽判定をする競技のようなものなので，ゲームの途中でプレイヤーの人数が変わるのはよろしくない。このことをシミュレーションで確認してみよう。\n以下のコードは，t検定のデータを最初n1=n2=10で作成して行っている。タイプ1エラーの検証をするので，効果量は\\(0\\)である。ここでt検定を行い，もしその\\(p\\)値が\\(\\alpha\\)よりも大きかったら，つまり有意であると判断されなかったら，同じ方法でデータを1件追加する。そしてまたt検定を行う。このサンプルの追加は，効果量\\(0\\)なので，偶然のお許しが出るまでいつまで経っても終わることがないため，上限を100にしてある。上限に達したら流石に諦めてもらうとして，さてそうしたQRPsな努力の結果，\\(\\alpha\\)水準はどれぐらいに保たれているだろうか。\n\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\nalpha &lt;- 0.05 # 有意水準を0.05に設定\np &lt;- rep(0, iter) # p値を保存するベクトルを初期化\nadd.vec &lt;- rep(0, iter) # 増やした人数を保存するベクトルを初期化\n\nset.seed(123) # 乱数のシードを設定して再現性を確保\n\nn1 &lt;- n2 &lt;- 10 # 各グループのサンプルサイズを10に設定\nmu &lt;- 10 # 平均値を10に設定\nsigma &lt;- 2 # 標準偏差を2に設定\ndelta &lt;- 0 # 平均の差を0に設定\n\n## シミュレーション本体\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  # 最初のデータを生成\n  Y1 &lt;- rnorm(n1, mu, sigma) # グループ1のデータを生成\n  Y2 &lt;- rnorm(n2, mu + sigma * delta, sigma) # グループ2のデータを生成\n  p[i] &lt;- t.test(Y1, Y2)$p.value # t検定を実行しp値を保存\n  # データを追加する\n  count &lt;- 0 # 追加したデータの数をカウント\n  ## p値が5%を下回るか、データが100になるまでデータを増やし続ける\n  while (p[i] &gt;= alpha && count &lt; 100) { # 条件を満たすまでループを繰り返す\n    # 有意でなかった場合、変数ごとに1つずつデータを追加\n    Y1_add &lt;- rnorm(1, mu, sigma) # グループ1に新しいデータを1つ追加\n    Y2_add &lt;- rnorm(1, mu + sigma * delta, sigma) # グループ2に新しいデータを1つ追加\n    Y1 &lt;- c(Y1, Y1_add) # グループ1のデータを更新\n    Y2 &lt;- c(Y2, Y2_add) # グループ2のデータを更新\n    p[i] &lt;- t.test(Y1, Y2)$p.value # 新しいデータでt検定を再度実行しp値を更新\n    count &lt;- count + 1 # データを追加した回数をカウント\n  }\n  add.vec[i] &lt;- count\n}\n\n## 結果\nifelse(p &lt; 0.05, 1, 0) |&gt; mean() # p値が5%未満の割合を計算\n\n[1] 0.306\n\nhist(p)\n\n\n\n\n\n\n\nhist(add.vec)\n\n\n\n\n\n\n\n\n結果をみると，0.306とかなり逸脱して，誤った結論に辿り着いていることがわかる。努力の結果得られた有意差は，偶然の賜物でもあり，誤った研究実践による幻想にすぎない。加えたデータのヒストグラムからわかるように，悲しいかな，75%もの割合で上限100まで達してしまう。百害あって一利なしとはこのことである。\n\n\n10.1.4 サンプルサイズを事前に決めないことの問題\nサンプルサイズを事前に決めずに検定する，という状況を別の角度から見てみよう。 クルシュケ ([2014] 2017) は「コインフリップを24回して，うち7回表が出た」というシーンを例に挙げて説明している。7/24は半分を下回っているから，やや裏が出やすいコインであるように思える。帰無仮説として，このコインは公平である(表と裏が出る確率が半々である)，というのを検証したいとする。\nこの24回中7回成功，という話の背後に「24施行する」ということを決めていたかどうか(サンプルサイズを事前に決めていたか)ということを考えてみよう。\nまずは正直に，最初から24回コインフリップすることを決めていたとする。コインフリップはベルヌーイ試行2 であり，それを繰り返すので二項分布に従うと考えられる。そこで，二項検定として次のように計算できるだろう。\n\nN &lt;- 24\n# 7回表が出る確率\npbinom(7, N, 0.5) * 2\n\n[1] 0.06391466\n\n\n二項分布のp値を出すにはpbinomを使いった。また帰無仮説として，このコインフリップは公平であると考えているのだから，\\(\\theta=0.5\\)が帰無仮説の状態である。この\\(\\theta=0.5\\)とした時に，\\(N=24,k=7\\)という結果になる確率を計算し，かつ両側検定(公平でない，が対立仮説なので裏が7回でもよい)であることを考えて確率を2倍した。p値は0.0639147であるから，5%水準では有意であると判定できない。これぐらいの確率はあるということだ。\nしかしここで第二の状況を考えてみよう。24回コインフリップすることを決めていたのではなくて，7回成功するまでコインフリップを続けたところ，結果的に24回で終わったということだった，とするのである。このようなシーンの確率分布は負の二項分布と呼ばれ，pnbinomで次のように計算できる。\n\nk &lt;- 7\n# 24回以上必要な確率\npnbinom(k, 24, 0.5) * 2\n\n[1] 0.003326893\n\n\nこの結果から，\\(\\theta=0.5\\)の時に7回表がでるまでに24施行も必要とする確率は，0.0033269だから，5%水準で有意である。つまり，滅多にこんなことが起きないので，\\(\\theta=0.5\\)という帰無仮説が疑わしいことになる。ここではシーンが異なるとp値が違っている，ということに注意してほしい。\nさらに第3のシーンを考えよう。これは「何回やるかは決めてないけど，まあ5分ぐらいかな」と試行にかける時間だけ決めていたという状況である。結果的に24回になったけど，もしかすると23回だったかもしれないし，25回や20回，30回だったかもしれない。これをシミュレーションするために，「24がピークになるような頻度の分布」をポアソン分布を使って生成する3。\n4 ポアソン分布は正の整数を実現値に取る分布で，カウント変数の確率分布として用いられる。パラメータは\\(\\lambda\\)だけであり，期待値と分散が\\(\\lambda\\)に一致する，非常にシンプルな分布である。\n\nset.seed(12345)\niter &lt;- 100000 # 発生させる乱数の数\n## 24回がピークに来るトライアル回数\ntrial &lt;- rpois(iter, 24)\nhist(trial)\n\n\n\n\n\n\n\n\nこの各トライアルにおいて，二項分布で成功した回数を計算し，トライアル回数で割ることによって，表が出る確率のシミュレーションができる。その時の割合は，\\(7/24\\)よりもレアな現象だろうか?\n\nresult &lt;- rep(NA, iter)\nfor (i in 1:iter) {\n  result[i] &lt;- rbinom(1, trial[i], 0.5) / trial[i]\n}\n## 7/24よりも小さい確率で起こった?\nlength(result[result &lt; (7 / 24)]) / iter\n\n[1] 0.02262\n\n\nこれを見ると，両側検定にしても0.04524なので，ギリギリ有意になるかどうか，というところだろうか。\nさて判断にこまった。「24回やる」と決めていたのであれば\\(\\theta=0.5\\)は棄却されないし，「7回成功するまで」と決めていたのであれば\\(\\theta=0.5\\)は棄却される。「5分間」と決めていても棄却されるが，そもそもこうした実験者の意図によって判断が揺らいで良いものだろうか?というのが クルシュケ ([2014] 2017) の指摘する疑問点である。\n問題は，「24回中7回成功」という事実に，二項分布，負の二項分布，あるいは組み合わさった分布のような，確率分布の情報が含まれていないことにある。この確率分布はデータが既知で母数が未知だから尤度関数であり，データ生成メカニズムであるとも言えるだろう。想定するメカニズムが明示されない検定は，ともすれば事後的に「実は負の二項分布を想定していたんですよ，へへ」ということも可能になってしまう。こうした点からも，研究者の自由度をなるべく少なくする研究計画の事前登録制度が必要であることがわかる。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#サンプルサイズ設計",
    "href": "chapter10.html#サンプルサイズ設計",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "10.2 サンプルサイズ設計",
    "text": "10.2 サンプルサイズ設計\nどのようにデータを取り，どのように分析・検定し，どのような基準で判断するかを事前に決めることに加え，事前にサンプルサイズを見積もっておく必要があるだろう。サンプルはとにかく多ければ多いほど良いか，というとそうではなく，過剰にサンプルを集めることは研究コストの増大であり，回答者の負担増でしかない。またサンプルサイズが大きくなると有意差を検出しやすくなるが，必要なのは有意差ではなく実質的に効果を見積もることであり，有意差が見つかれば良いというものではないことに注意が必要である。もちろん上で見てきたように，有意差が検出できるかどうかを指標にしてサンプルサイズを変動させてしまうのは，明らかに誤った研究実践である。\n事前にサンプルサイズを決定するのに必要なのは，これまでのリバースエンジニアリングの演習例からもわかるように，効果量の見積もりである。5 これをどのように定めるかについては，先行研究を考えるとか，研究領域で「これぐらい差がないと意味がないよね」とコンセンサスが取れている程度で決めることになる。6\n\n10.2.1 対応のないt検定\nサンプルサイズの設計には，これまで使ってきた検定統計量に非心度non-centrality parameterというパラメータを加えて考える必要がある。\n具体的に，対応のないt検定を例にサンプルサイズ設計の方法を見てみよう。t検定は言葉の通り，t分布を用いて帰無仮説の元での検定統計量の実現値が問題になるのであった。帰無仮説は\\(\\mu_0 = \\mu_1-\\mu_2 = 0\\)であり，検定統計量は次式で表されるのであった。\n\\[ T = \\frac{d - \\mu_0}{\\sqrt{U^2_p/\\frac{n_1n_2}{n_1+n_2}}}\\]\nこの分子において，\\(d - \\mu_0 = (\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)\\)の第二項を,\\(\\mu_1-\\mu_2 = 0\\)と仮定するから，\\(0\\)が中心の標準化されたt分布が用いられたのである。帰無仮説はこのように理論的に特定できる比較点をもとに置かれているのであって，帰無仮説下ではない現実の世界では，検定統計量は母平均の差\\(\\mu_1 - \\mu_2\\)に応じた分布から生じている。このように中心がずれているt分布のことを非心分布といい，ズレの程度が非心度パラメータである。t検定における非心度\\(\\lambda\\)は，以下の式で表される。\n\\[ \\lambda = \\frac{(\\mu_1-\\mu2) - \\mu_0}{\\sigma\\sqrt{n}} \\]\nこの非心度の分だけ，非心t分布はt分布からズレていることになる。Rではdt関数にncpパラメータがあり，デフォルトではncp=0になっていた。これを変えて描画してみよう。\n\n# データの準備\ndf &lt;- 10 # 自由度を指定\n# ggplotでプロット\nggplot(data.frame(x = c(-5, 5)), aes(x = x)) +\n  stat_function(fun = dt, args = list(df = df, ncp = 0), aes(color = \"ncp=0\")) +\n  stat_function(fun = dt, args = list(df = df, ncp = 3), aes(color = \"ncp=3\")) +\n  labs(\n    title = \"非心t分布\",\n    x = \"x\",\n    y = \"密度\",\n    color = \"ncp\"\n  )\n\n\n\n\n\n\n\n\nncp=0の時は，中心が0にある帰無仮説の世界であり，これを使ってタイプ1エラー，つまり\\(\\alpha\\)が算出されたのであった。ncpを効果量で表現すれば，母平均の差がゼロでない時の分布が描けるのだから，タイプ2エラー，つまり\\(\\beta\\)が計算できる。\n自由度df = 10，非心度ncp = 3の例で考えてみよう。 タイプ1エラーになるのは，自由度10のt分布で上2.5%の臨界値以上の実現値が得られた時である。\n\nqt(0.975, df = 10, ncp = 0)\n\n[1] 2.228139\n\n\nこのとき，実際はncp = 3ほどずれていたのだから，タイプ2エラーが生じる確率は次のとおりである。\n\nqt(0.975, df = 10, ncp = 0) %&gt;% pt(df = 10, ncp = 3)\n\n[1] 0.2285998\n\n\n当然，ncp = 0から離れるほどタイプ2エラーは生じにくくなる。非心度は母効果量\\(\\delta = \\frac{(\\mu_1 - \\mu_2)-\\mu_0}{\\delta}\\)を使って，\\(\\lambda = \\delta \\sqrt{n}\\)で表すことができる。\nこれを使って，t検定のサンプルサイズを設計してみよう。話を簡単にするために，サンプルサイズは2群で等しいものとする。\n検定統計量の式を思い出して，\\(\\sqrt{n}\\)にあたるところは2群のサンプルサイズから計算される，プールされた標本サイズから得られることに注意しよう7。\n\nalpha &lt;- 0.05\nbeta &lt;- 0.2\ndelta &lt;- 0.5\n\nfor (n in 10:1000) {\n  df &lt;- n + n - 2\n  lambda &lt;- delta * (sqrt((n * n) / (n + n)))\n  cv &lt;- qt(p = 1 - alpha / 2, df = df) # Type1errorの臨界値\n  er &lt;- pt(cv, df = df, ncp = lambda) # Type2errorの確率\n  if (er &lt;= beta) {\n    break\n  }\n}\nprint(n)\n\n[1] 64\n\n\nここでは，サンプルサイズを10から徐々に増やしていき，1000までの間で目標とする\\(\\beta\\)まで抑えられたところで，カウントしていくforループをbreakで脱出する，というかたちで組んでいる。結果的に，各群64名，合計128名のサンプルがあれば，目標が達成できることがわかる。サンプルサイズが2群で異なる場合など，詳細は@kosugi2023 に詳しい。\n\n\n10.2.2 シミュレーションによるサンプルサイズ設計\n非心F分布を使えば分散分析でもサンプルサイズができるし，そのほかの検定についても同様に非心分布を活用すると良い。しかし，非心分布の理解や非心度の計算など，ケースバイケースで学ぶべきことは多い。\nそこで，電子計算機の演算力をたのみに，データ生成のシミュレーションを通じて設計していくことを考えてみよう。サンプルサイズや効果量を定めれば，仮想データを作ることができるし，それ対して検定をかけることもできる。仮想データの生成と検定を反復し，タイプ2エラーがどの程度生じるかを相対度数で近似することもできるだろう。であれば，その近似をサンプルサイズを徐々に変えることで繰り返してサンプルサイズを定めることもできる。\n以下は，母相関が\\(\\rho = 0.5\\)とした時に，検出力が80%になるために必要なサンプルサイズを求めるシミュレーションコードである。\n\nlibrary(MASS)\nset.seed(12345)\nalpha &lt;- 0.05\nbeta &lt;- 0.2\nrho &lt;- 0.5\nsd &lt;- 1\nSigma &lt;- matrix(NA, ncol = 2, nrow = 2)\nSigma[1, 1] &lt;- Sigma[2, 2] &lt;- sd^2\nSigma[1, 2] &lt;- Sigma[2, 1] &lt;- sd * sd * rho\n\niter &lt;- 1000\n\nfor (n in seq(from = 10, to = 1000, by = 1)) {\n  FLG &lt;- rep(0, iter)\n  for (i in 1:iter) {\n    X &lt;- mvrnorm(n, c(0, 0), Sigma)\n    cor_test &lt;- cor.test(X[, 1], X[, 2])\n    FLG[i] &lt;- ifelse(cor_test$p.value &gt; alpha, 1, 0)\n  }\n  t2error &lt;- mean(FLG)\n  print(paste(\"n=\", n, \"のとき，betaは\", t2error, \"です。\"))\n  if (t2error &lt;= beta) {\n    break\n  }\n}\n\n[1] \"n= 10 のとき，betaは 0.681 です。\"\n[1] \"n= 11 のとき，betaは 0.639 です。\"\n[1] \"n= 12 のとき，betaは 0.612 です。\"\n[1] \"n= 13 のとき，betaは 0.566 です。\"\n[1] \"n= 14 のとき，betaは 0.563 です。\"\n[1] \"n= 15 のとき，betaは 0.471 です。\"\n[1] \"n= 16 のとき，betaは 0.462 です。\"\n[1] \"n= 17 のとき，betaは 0.419 です。\"\n[1] \"n= 18 のとき，betaは 0.402 です。\"\n[1] \"n= 19 のとき，betaは 0.385 です。\"\n[1] \"n= 20 のとき，betaは 0.353 です。\"\n[1] \"n= 21 のとき，betaは 0.344 です。\"\n[1] \"n= 22 のとき，betaは 0.312 です。\"\n[1] \"n= 23 のとき，betaは 0.285 です。\"\n[1] \"n= 24 のとき，betaは 0.256 です。\"\n[1] \"n= 25 のとき，betaは 0.265 です。\"\n[1] \"n= 26 のとき，betaは 0.21 です。\"\n[1] \"n= 27 のとき，betaは 0.227 です。\"\n[1] \"n= 28 のとき，betaは 0.176 です。\"\n\nprint(n)\n\n[1] 28\n\n\nここではシミュレーション回数1000，上限1000，刻み幅を1にしているが，状況に応じて変更すると良い。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#課題",
    "href": "chapter10.html#課題",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "10.3 課題",
    "text": "10.3 課題\n\n一要因3水準のBetweenデザインの分散分析において、1.分散分析で有意差が見られる場合と、2.任意の2水準の組み合わせのどこかで有意差が見られる場合を考えたとき、タイプ2エラーはどのように異なるかをシミュレーションで確かめてみましょう。設定として、n1=n2=n3=10、標準偏差も各群等しくsigma = 1とし、効果量delta = 2でモデル化してみましょう。\nN増し問題は相関係数の検定の時も生じるでしょうか。母相関が\\(\\rho = 0.0\\)のとき、サンプルサイズを10から始めて、有意になるまでデータを追加する仮想研究を1000回行ってみましょう。データ追加の上限は100、有意水準は\\(\\alpha = 0.05\\)として、最終的に有意になる割合を計算してみましょう。\n\\(\\alpha = 0.05, \\beta = 0.2\\)とし、効果量\\(\\delta = 1\\)とした時の対応のないt検定のサンプルサイズ設計をしたいです。1.非心t分布を使った解析的な方法と、2.シミュレーションによる近似的な方法の両方で、同等の結果が出ることを確認しましょう。\n\n\n\n\n\nクルシュケJ. K. (2014) 2017. ベイズ統計モデリング: R, JAGS, Stanによるチュートリアル 原著第2版. Translated by 前田和寛 and 小杉考司. 共立出版.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#footnotes",
    "href": "chapter10.html#footnotes",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "ANOVA君は結果をコンソールに直接出力し，戻り値を持たない。ここでは結果の\\(p\\)値が必要だったので，このようにした。↩︎\n表(1)が出るか，裏(0)が出るか，という2値の結果変数だけを持つ施行のことで，この確率変数がベルヌーイ分布にし違う。ベルヌーイ分布は，表が出る確率\\(\\theta\\)をパラメータに持つ。\\(P(X=k) = \\theta^k(1-\\theta)^{1-k},\\text{ただし}k=\\{0,1\\}\\)という確率変数である。1/0というのが生死，男女，成功失敗などさまざまなメタファに適用できるので応用範囲が広い。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nこの「最低限検出したい効果」のことをSmallest Effect Size of Interest, SESOIと呼ぶ。小杉, 紀ノ定, and 清水 (2023) も参照。↩︎\nt統計量の実現値の式にある分母，\\(\\sqrt{U_p^2/\\frac{n_1n_2}{n_1+n_2}}\\)に見られる，プールされた不偏分散を割るための標本サイズであり，2群の母分散が等しいと仮定して計算するなら，\\(\\sigma^2(\\frac{1}{n_1} + \\frac{1}{n_2}) = \\sigma^2(\\frac{n_1+n_2}{n_1n_2}) = \\sigma^2 / \\frac{n_1n_2}{n_1 + n_2}\\)から得られる。↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter11.html",
    "href": "chapter11.html",
    "title": "11  重回帰分析の基礎",
    "section": "",
    "text": "11.1 回帰分析の基礎\nここでは回帰分析を扱う。説明変数\\(x\\)と被説明変数\\(y\\)の関数関係\\(y=f(x)\\)に，次の一次式を当てはめるのが単回帰分析である。\n\\[ y_i  = \\beta_0 + \\beta_1 x_i + e_i = \\hat{y}_i + e_i\\]\n一次式を\\(\\hat{y}\\)とまとめたものを予測値といい，予測値と実測値\\(y\\)の差分\\(e_i\\)を残差residualsという。\n空間上の一次直線の切片，傾きを求めるというのが基本的な問題であり，二点であれば一意に定めることができるが，データ分析の場面では3点以上の多くのデータセットの中に直線を当てはめることになるので，なんらかの外的な基準が必要になる。この時，「残差の分散が最も小さくなるように」と考えるのが最小二乗法の考え方であり，「残差が正規分布に従っていると考え，その尤度が最も大きくなるように」と考えるのが最尤法の考え方である。前者は記述統計的な，後者は確率モデルとしての感が過多になっていることに注意してほしい。また確率モデルの推定方法としては，事前分布を用いたベイズ推定が用いられることもある。\n最小二乗法による推定値は，次の式で表される。証明は他書(小杉 2018; 西内 2017)に譲るが，ロジックとして残差の二乗和\\(\\sum e_i^2 = \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2\\)を最小にすることを考え，この式を展開するか偏微分を用いて極小値を求めることで算出できるとだけ伝えておこう。いずれにせよ，平均値\\(\\bar{x},\\bar{y}\\)や分散・共分散\\(s_x,s_y,r_{xy}\\)など標本統計量から推定できるのはありがたいことである。\n\\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x},\\quad \\beta_1 = r_{xy} \\frac{s_y}{s_x}\\]\nまた，ここでは\\(x,y\\)ともに連続変数を想定しているが，説明変数\\(x\\)が二値，あるいはカテゴリカルなものであれば\\(y\\)の平均値を通る直線を探すことになる。直線の傾きが0であれば「平均値が同じ」という線形モデルであり，これは平均値差の検定における帰無仮説と同等である。このように，t検定やANOVAは回帰分析の特殊ケースとも考えられ，まとめて一般線形モデルと呼ばれる。一般線形モデルは，被説明変数が連続的で，線形モデルによる平均値に正規分布に従う残差が加わったものとして考えるという意味で統一的に表現される。\nANOVAの場合は，二つ以上の要因による効果を考えることもあった。交互作用項を考慮しなければ，2要因のモデルは次のように表現することができる。\n\\[ y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + e_i \\]\nこのように説明変数が複数ある回帰分析を特に重回帰分析Multiple Regression Analysisと呼ぶ。一次式なので，ある変数に限れば線形性が担保されているから，これも線形モデルの仲間である。重回帰分析を用いる場合は，説明変数同士を比較して「どちらの説明変数の方が影響力が大きいか」ということが論じられることが多いが，係数は当然\\(x_n, y\\)の単位に依存するため，素点の回帰係数は使い勝手が悪い。そこですべての変数を標準化した標準化係数が用いられることが多い。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#回帰分析の特徴",
    "href": "chapter11.html#回帰分析の特徴",
    "title": "11  重回帰分析の基礎",
    "section": "11.2 回帰分析の特徴",
    "text": "11.2 回帰分析の特徴\n以下，具体的なデータを用いて回帰分析の特徴を見てみよう\n\n11.2.1 パラメータリカバリ\n回帰分析のモデル式にそってデータを生成し，分析によってパラメータリカバリを行ってみよう。\n説明変数については制約がないので一様乱数から生成し，平均0，標準偏差\\(\\sigma\\)の誤差とともに被説明変数を作り，\n\nlibrary(tidyverse)\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データの生成\nx &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x + e\n\ndat &lt;- data.frame(x, y)\n# データの確認\nhead(dat)\n\n          x          y\n1 -4.248450 -11.120952\n2  5.766103  18.736432\n3 -1.820462  -3.805302\n4  7.660348  25.071541\n5  8.809346  30.026546\n6 -9.088870 -25.355175\n\ndat %&gt;% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y~x\") # 線形モデルの描画\n\n\n\n\n\n\n\n\nこのデータに基づいて回帰分析を実行した結果が以下のとおりである。\n\nresult.lm &lt;- lm(y ~ x, data = dat)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82796 -0.61831  0.03553  0.69367  2.68062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.021928   0.045010   44.92   &lt;2e-16 ***\nx           3.002194   0.007919  379.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.006 on 498 degrees of freedom\nMultiple R-squared:  0.9965,    Adjusted R-squared:  0.9965 \nF-statistic: 1.437e+05 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\nここでは\\(\\beta_0 =2, \\beta_1=3, \\sigma = 1\\)と設定しており，ほぼ理論通りの係数がリカバリーできていることを出力から確認しておこう。 もちろんリカバリの精度は，データの線形性の強さに依存するから，残差の分散が大きかったりサンプルサイズが小さくなると，必ずしもうまくリカバリできないことがあることは想像に難くないだろう。\n\n\n11.2.2 残差の正規性と相関関係\nlm関数が返した結果オブジェクトには，表示されていない多くの情報が含まれている。例えば予測値や残差も含まれているので，これを使って回帰分析の特徴を見てみよう。\n\ndat &lt;- bind_cols(dat, yhat = result.lm$fitted.values, residuals = result.lm$residuals)\nsummary(dat)\n\n       x                  y                yhat            residuals       \n Min.   :-9.99069   Min.   :-28.216   Min.   :-27.9721   Min.   :-2.82796  \n 1st Qu.:-5.08007   1st Qu.:-13.074   1st Qu.:-13.2294   1st Qu.:-0.61831  \n Median :-0.46887   Median :  0.301   Median :  0.6143   Median : 0.03553  \n Mean   :-0.09433   Mean   :  1.739   Mean   :  1.7387   Mean   : 0.00000  \n 3rd Qu.: 4.65795   3rd Qu.: 15.963   3rd Qu.: 16.0060   3rd Qu.: 0.69367  \n Max.   : 9.98809   Max.   : 32.638   Max.   : 32.0081   Max.   : 2.68062  \n\n\n予測値\\(\\hat{y}\\)はfitted.valuesとして保存されている。これの平均値が被説明変数\\(y\\)の平均値に一致していることが確認できる。回帰分析は説明変数\\(x\\)を伸ばしたり(\\(\\beta_1\\)倍する)ズラしたり(\\(\\beta_0\\)を加える)しながら，被説明変数\\(y\\)に当てはめるのであり，位置合わせがなされた予測値の中心が被説明変数の中心と一致することは理解しやすいだろう1。\n次に，残差の平均が0になっていることも確認しておこう。これが0でない\\(c\\)であれば，回帰係数が常に\\(c\\)だけズレていることになるので，そのような系統的ズレは最適な線形の当てはめにおいて除外されているべきだからである2。\nまた，回帰分析において残差は正規分布に従うことが仮定されていた。これを検証するにはQ-Qプロットを見ると良い。\n\ndat %&gt;%\n  ggplot(aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\n\nQ-Qプロットとは2つの確率分布を比較するためのグラフであり，横軸には理論的分布の分位点が，縦軸に実データが並ぶもので，右上がりの直線上にデータが載っていれば分布に従っている，と判断するものである。直線から逸脱している点は理論的分布からの逸脱と考えられる。今回の結果はほとんどが正規分布の直線上にあることから，大きな逸脱がないことが認められる。\nデータ生成メカニズムによっては，被説明変数が二値的であったり，順序的であったり，カウント変数であったり，と正規分布がそぐわないものもあるだろう。そのようなデータに無理やり回帰分析を当てはめることは適切ではない。いかなる時もデータは可視化して，モデルを当てはめることの適切さをチェックすることを忘れたはならない。\nちなみに出力結果を直接plot関数に入れてもよい。ここから残差と予測値の相関関係や，Q-Qプロット，標準化残差のスケールロケーションプロット，レバレッジと標準化残差3 などがプロットされる。\n\nplot(result.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差と予測値のプロットからも想像できるが，両者の相関はゼロである。図で確認しておこう。\n\nlibrary(GGally) # 必要ならインストールしよう\nggpairs(dat)\n\n\n\n\n\n\n\n\nこの関係から明らかなように，残差は説明変数や予測値と相関しない4。説明変数と残差に相関関係があるとすると，説明変数でまだ説明できていない分散が残っていることになるし，予測値と残差に相関がないことは予測値が高いか低いかにかかわらず，残差が一様に分布していることを意味する。このことを踏まえて，重回帰分析の特徴を理解していこう。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#重回帰分析の特徴",
    "href": "chapter11.html#重回帰分析の特徴",
    "title": "11  重回帰分析の基礎",
    "section": "11.3 重回帰分析の特徴",
    "text": "11.3 重回帰分析の特徴\n重回帰分析においては，回帰係数は偏回帰係数partial regression coefficientsと呼ばれる。この「偏」の一文字が意味することを考えていこう。\n\n11.3.1 回帰係数と偏回帰係数\n単回帰分析の回帰係数は，説明変数\\(x\\)が一単位上昇した時の被説明変数の変化量，と解釈すればよい。これに対して重回帰分析の偏回帰係数を，「説明変数\\(x_1\\)が一単位上昇した時の被説明変数の変化量」とすることはできない。というのも，説明変数が複数(\\(x_2,x_3,\\ldots\\))あり，他の説明変数の次元についての変化を考慮していない変化量になっているからである。\n重回帰分析において，説明変数が完全に無相関で直交しているのであれば，\\(x_1\\)の変化と\\(x_2\\)の変化を独立して説明できるが，往々にしてそのようなことはない。偏回帰係数は当該変数以外の変動を統制した回帰係数である。\n上で単回帰係数において，説明変数と残差が相関しないことを確認した。言い換えれば，説明変数で説明でき分散は全て説明し尽くされており，残差は説明変数で説明できない被説明変数の分散，つまり説明変数の影響を除外した被説明変数の分散と考えることができる(被説明変数の分散=説明変数が説明する分散+残差の分散)。\nここで第二の変数\\(x_2\\)があったとする。第一の変数\\(x_1\\)で\\(y\\)を説明した残差\\(e_y\\)と，第一の変数で第二の変数を説明した残差\\(e_{x2}\\)との相関を偏相関partial correlationという。これは第一の変数\\(x_1\\)からの影響を両者から取り除いているので，\\(x_1\\)で統制した相関係数ということができる。偏相関は単純な相関が「見せかけの関係」でないことを検証するための重要な指標である。\n偏相関係数を計算してみよう。\n\nlibrary(MASS)\nlibrary(psych)\nSigma &lt;- matrix(c(1, 0.3, 0.5, 0.3, 1, 0.8, 0.5, 0.8, 1), ncol = 3)\nX &lt;- mvrnorm(1000, c(0, 0, 0), Sigma, empirical = TRUE) %&gt;% as.data.frame()\n## 相関行列\ncor(X)\n\n    V1  V2  V3\nV1 1.0 0.3 0.5\nV2 0.3 1.0 0.8\nV3 0.5 0.8 1.0\n\n## 回帰分析をして残差を求める\nresult.lm1 &lt;- lm(V2 ~ V1, data = X)\nresult.lm2 &lt;- lm(V3 ~ V1, data = X)\ncor(result.lm1$residuals, result.lm2$residuals)\n\n[1] 0.7867958\n\n## 偏相関を求めるR関数で確認\npsych::partial.r(X)[2, 3]\n\n[1] 0.7867958\n\n\n最後はpsychパッケージの偏相関行列を求める関数で検証した。確かに残差同士の相関係数が偏相関係数になっていることが確認できたと思う。\nそして，ここでは残差同士の相関係数として算出しているが，残差をつかった回帰分析の係数が偏回帰係数になるのである。このデータセットの第一変数を従属変数にした重回帰分析の結果から，これを確認してみよう。\n\nresult.mra &lt;- lm(V1 ~ V2 + V3, data = X)\n# 回帰係数を取り出す\nresult.mra$coefficients\n\n  (Intercept)            V2            V3 \n-3.414371e-17 -2.777778e-01  7.222222e-01 \n\n# 残差をつかって偏回帰係数を確認する\nresult.lm3 &lt;- lm(V1 ~ V3, data = X)\nresult.lm4 &lt;- lm(V2 ~ V3, data = X)\nresult.lm5 &lt;- lm(result.lm3$residuals ~ result.lm4$residuals)\n#\nresult.lm5$coefficients\n\n         (Intercept) result.lm4$residuals \n        3.242857e-17        -2.777778e-01 \n\n\n重回帰分析の結果result.mraのV2からV1への回帰係数は-0.2778である。また，V3でV1,V2を統制した残差同士をつかい，回帰係数を求めた結果は-0.2778と，同じ値になっていることが確認できただろう。\nV3からV1への偏回帰係数も同様で，V2で両者を統制した残差同士による回帰係数になっている。このように，重回帰分析の回帰係数は，他の説明変数で統制した値になっており，日本語で説明するなら「他の変数の値が同じであると想定した条件つきの，当該変数の影響力」とでもいうべき値になっている。\nなぜこのような持って回った説明をするかというと，つい「条件付きの」という話を忘れて報告，解釈してしまうことが多いからで，吉田 and 村井 (2021) の論文での指摘は議論を呼んだのは記憶に新しい5。 たとえば今回の例でも，回帰係数が-0.2778であったのに対し，V1とV2の単相関が0.3 であったことを思い出そう。符号が反転しているため，解釈は真逆になってしまう。実際の単相関は正の関係であるから，条件付きであることを忘れて「V2は負の影響，V3は正の影響」と表現してしまうと，ミスリーディングなことになるからである。\nまた，豊田 (2017) は重回帰分析のこうした誤用を避けるために，独立変数を事前に直交化したデザインで行うコンジョイント分析の積極的な利用を提案している。我々が重回帰分析をうまく使いこなせないのであれば，そうした手法も有用であるだろう。\n\n\n11.3.2 多重共線性\n偏回帰係数の解釈が難しい理由の一つは，説明変数同士に相関関係がみられることにある。 特に，説明変数間の相関関係が高くなることは多重共線性Multicollinearityの問題という。この問題は，回帰係数の標準誤差がインフレを起こすことを指す。\n例えば先ほどの例で，説明変数V2とV3は相関係数\\(0.8\\)を持っていた。この時の回帰係数の標準誤差を確認しておこう。\n\nsummary(result.mra)\n\n\nCall:\nlm(formula = V1 ~ V2 + V3, data = X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.59118 -0.54717 -0.03692  0.55044  2.90735 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.414e-17  2.690e-02   0.000        1    \nV2          -2.778e-01  4.486e-02  -6.192 8.65e-10 ***\nV3           7.222e-01  4.486e-02  16.100  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8507 on 997 degrees of freedom\nMultiple R-squared:  0.2778,    Adjusted R-squared:  0.2763 \nF-statistic: 191.7 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n標準誤差は0.0449であり，それほど大きくない標準誤差で問題がないようである。しかし両者の相関係数がより高くなり，一方が他方に線形的に従属してしまうと係数の推定値が不安定になるため，注意が必要である。\nこのインフレを確認するための指標がVariance Infration Factor: VIFである。Rではcarパッケージにあるvif関数に重回帰モデルを入れることでこの指標が算出される。一般にVIFが3，あるいは10を超えると多重共線性が生じており，解釈に注意が必要と言われている6。\n\nlibrary(car) # なければ入れておこう\nvif(result.mra)\n\n      V2       V3 \n2.777778 2.777778 \n\n\n幸い，今回の値はこれらの基準を下回っていたので許容範囲内である。\n\n\n11.3.3 変数の投入順序\n重回帰分析の場合は複数の説明変数があるが，これを投入するときに全ての変数を同時に投入するか，順番をつけて投入するかといった手法の違いがある。前者を強制投入法と呼ぶこともある。 順番をつけて投入する方法は，逐次投入と呼ばれる。この場合は，適合度指標などを参考に変数を追加あるいは削除して，適合度が統計的に有意に向上するかどうかを考えながら進めていく。\n重回帰係数の予測値\\(\\hat{y}\\)と，被説明変数\\(y\\)の相関係数\\(R_{y\\hat{y}}\\)は重相関係数と呼ばれ，予測がうまくいっているかどうかを表す適合度の一つである。相関係数なので\\(-1\\)から\\(+1\\)までの値を取りうるが，\\(-1\\)は逆に完全に合致していることになるので，この相関係数の符号は大して情報を持たない。そこでこれを二乗した\\(R_{y\\hat{y}}^2\\)を考える。これは決定係数とも呼ばれ，説明変数の分散のうち予測値の分散が占める割合を表している。7\n説明変数の逐次投入は，説明変数を持たないヌルモデルから一つずつ追加していくForward Selection，全ての変数を投入してから一つずつ減らしていくBackward Selectionがある。Forwardのほうは追加することによって\\(R^2\\)が有意に増加するか，Backwardのほうは削除しても有意に\\(R^2\\)が減らないか，を確認しながら進めることになる。この方法は手元のデータに最も適した説明変数のペアを選出できる方法ではあるが，検定を繰り返していることの問題と，手元のデータ以外に一般化する時の根拠の乏しさから，用いられないこともある。\n逐次投入法には別の観点からの手法もある。それが階層的回帰分析である。この手法は，重回帰分析における交互作用項の投入を検討する文脈で発展した。重回帰分析では，説明変数同士の相関がない，もしくは小さい方が望ましい。しかし，交互作用とは分散分析における組み合わせの効果を表すものであり，実験デザインによっては交互作用効果が重要な変動であることも少なくない。回帰分析と分散分析は，一般線形モデルという形で統一的に理解されるが，回帰分析でも連続的に変化する組み合わせの効果を考えることができる。交互作用があるということは説明変数間に相関があることを意味するため，回帰分析の大前提に抵触する可能性があり，その投入には慎重を期する必要がある。\nこうした文脈から，まずは要因の効果を投入し，次に交互作用項を投入してモデル適合度の有意な改善がみられるかどうかを検証する手順が推奨されている。この逐次投入法を特に階層的回帰分析と呼ぶ。ここでの「階層」とは，手順が重要度順に進められていることを意味し，データの特徴に関するものではないことに注意が必要である8。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#係数の標準誤差と検定",
    "href": "chapter11.html#係数の標準誤差と検定",
    "title": "11  重回帰分析の基礎",
    "section": "11.4 係数の標準誤差と検定",
    "text": "11.4 係数の標準誤差と検定\n\n11.4.1 係数の検定\nサンプルが母集団から得られた確率変数であるのだから，(偏)回帰係数もまた確率変数である。すなわち，サンプルが変わるごとに変化し，その揺らぎがある確率分布に従うと考えられる。これを確認するためには，データ生成過程をモデリングし，反復することで近似させて理解するのがいいだろう。\n\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データ生成関数\ndataMake &lt;- function(n, beta0, beta1, sigma) {\n  x &lt;- runif(n, -10, 10)\n  e &lt;- rnorm(n, 0, sigma)\n  y &lt;- beta0 + beta1 * x + e\n  dat &lt;- data.frame(x, y)\n  return(dat)\n}\n\n# 結果オブジェクトの準備\niter &lt;- 2000\nbeta0.est &lt;- rep(NA, iter)\nbeta1.est &lt;- rep(NA, iter)\n# simulation\nfor (i in 1:iter) {\n  sample &lt;- dataMake(n, beta0, beta1, sigma)\n  result.lm &lt;- lm(y ~ x, data = sample)\n  beta0.est[i] &lt;- result.lm$coefficients[1]\n  beta1.est[i] &lt;- result.lm$coefficients[2]\n}\n\ndata.frame(x = beta0.est) %&gt;% ggplot(aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\ndata.frame(x = beta1.est) %&gt;% ggplot(aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n図から明らかなように，回帰係数も確率的に分布する。ただしその平均は理論値に近似している。\n\nmean(beta0.est)\n\n[1] 1.999257\n\nmean(beta1.est)\n\n[1] 2.999798\n\n\nこの分布の幅が回帰係数の標準誤差である。\n\nsd(beta0.est)\n\n[1] 0.04580387\n\nsd(beta1.est)\n\n[1] 0.007659277\n\n\n回帰係数はt分布に従い，その自由度はサンプルサイズからモデルで用いる係数の数を引いたものになる。先ほどのヒストグラムを基準化し，理論分布を重ねて描画してみることで確認しておこう。\n\ndata.frame(x = beta1.est) %&gt;%\n  scale() %&gt;%\n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 100) +\n  stat_function(fun = function(x) dt(x, df = n - 2), color = \"red\", linewidth = 2)\n\n\n\n\n\n\n\n\nこのt分布を用いて，係数が0の母集団から得られたサンプルなのかどうかの検定が行われる。\n\n\n11.4.2 モデル適合度の検定\n一方で，出力の最後にはF統計量による検定も行われていたことを確認しておこう。次に示すのは重回帰分析の例である。\n\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 0\nbeta2 &lt;- 0\nsigma &lt;- 1\nx1 &lt;- runif(n, -10, 10)\nx2 &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + e\nsample &lt;- data.frame(y,x1,x2)\nresult.lm &lt;- lm(y ~ x1 + x2, data = sample)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = sample)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85235 -0.68275 -0.01436  0.67809  2.70488 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.996999   0.045263  44.120   &lt;2e-16 ***\nx1          -0.006453   0.007970  -0.810    0.418    \nx2          -0.003928   0.007795  -0.504    0.615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.012 on 497 degrees of freedom\nMultiple R-squared:  0.001893,  Adjusted R-squared:  -0.002124 \nF-statistic: 0.4713 on 2 and 497 DF,  p-value: 0.6245\n\n\n上の例では，統計量\\(F\\)が，自由度\\(F(\\) 2,497 \\()\\)のもとで，0.4713であり，統計的に有意ではないと判断される(p=0.6245,n.s.)。\nこれは重相関係数に対する検定であり，母集団においてモデル全体としての説明力が0である，という帰無仮説を検証しているものである。この有意性検定には，説明変数の数\\(p\\)，サンプルサイズ\\(n\\)，重相関係数\\(R^2\\)を用いて，以下の式で用いられる検定統計量Fを利用する(南風原 2014)。\n\\[ F= \\frac{R^2}{1-R^2}\\cdot\\frac{n-p-1}{p} \\]\nここで右辺第一項目はCohenの効果量(\\(f^2=\\frac{R^2}{1-R^2}\\)) といわれ，サンプルサイズ設計においてはこの指標が利用される。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#サンプルサイズ設計",
    "href": "chapter11.html#サンプルサイズ設計",
    "title": "11  重回帰分析の基礎",
    "section": "11.5 サンプルサイズ設計",
    "text": "11.5 サンプルサイズ設計\n重回帰分析のサンプルサイズ設計は，変数の効果の大きさ(回帰係数)が事前にわかっているのであれば，nを徐々に増やしていくシミュレーションによって行える。しかしそのようなケースは稀であり，実際には\\(R^2\\)の検定を用いて，ある効果量と検出力の下で，正しく検出できるサイズを算出することになる。\nサンプルサイズの算出には，非心F分布を用いる。この時の非心度は，効果量\\(f^2\\)に\\(n\\)をかけたものになる。これを使ってサンプルサイズ設計をする例は以下のとおりである。\n\nf2 &lt;- 0.15      # 効果量\nalpha &lt;- 0.05   # タイプ1エラー率\nbeta &lt;- 0.2     # タイプ2エラー率\np &lt;- 5         # 説明変数の数\n\nfor(n in 10 : 500){\n    lambda &lt;- f2 * n\n    df1 &lt;- p\n    df2 &lt;- n - p - 1\n    cv &lt;- qf(p = 1- alpha, df1, df2)\n    t2error &lt;- pf(q = cv, df1, df2, ncp  = lambda)\n    if(t2error &lt; beta){\n        break\n    }\n}\n\nprint(n)\n\n[1] 92\n\n\nこの設定では，n = 92以上であればモデルとして影響力がないとは言えない，ということがわかる。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#まとめ",
    "href": "chapter11.html#まとめ",
    "title": "11  重回帰分析の基礎",
    "section": "11.6 まとめ",
    "text": "11.6 まとめ\n重回帰分析は，人文社会科学で多用される技術ではあるが，技術が先行して理解が伴わないまま利用されているケースも少なくない。繰り返しになる点もあるが，以下に注意点をまとめておく。\n\n偏回帰係数の意味；重回帰分析における偏回帰係数は，ほかの変数を統制した上での値であり，あたかも各係数が独立直交しているかのように解釈するのは適切ではない。\n誤差の正規性；誤差は正規分布に従っているという仮定があり，二値データや整数しかとらないカウントデータなどに盲目的にモデルを適用してはならない。誤差の正規性が満たされているかどうかは，分析後にQ-Qプロットを用いて確認する。\n誤差の均質性；誤差はモデル全体にわたって同じ正規分布に従っているというのもモデルの仮定である。すなわち，独立変数に応じて誤差分散が変わるといった均質でないデータの場合は，正しく推定されない。誤差の均質性については，分析後のQ-Qプロットを用いて確認する。\n誤差間の独立性；誤差はモデル全体にわたって同じ正規分布から独立に生成されている(i.i.d)というのがモデルの仮定である。時系列データのように，誤差間に対応(自己回帰)がみられるデータの場合は回帰分析は適切な手法とならない。状態空間モデルなど，誤差間関係を適切にモデリングしたものを当てはめる必要がある。\nモデルの適切な定式化；モデルには被説明変数に影響を与えるすべての変数が正しく含まれている必要がある。例えば，影響を与えることがわかっている変数\\(X_o\\)を意図的に除外して分析をしたとする。そのモデルに含まれる変数\\(X_a\\)が被説明変数\\(y\\)に影響を与えていたとしても，\\(X_a\\)と\\(X_o\\)に相関があれば，\\(X_o\\)の影響力が\\(X_a\\)を通じて\\(y\\)に伝播するkから，\\(X_a\\)の影響力が課題に評価されることになる。自らの仮説のために，意図的に変数を選択するのはQRPsに該当する。\n説明変数間の相関関係；説明変数のうちにあまりにも相関関係が高い変数ペアがあれば，多重共線性の疑いが生じる。多重共線性は推定値の不安定さとなって現れる。このような場合は，説明変数を主成分分析で合成変数にまとめるといった対応が考えられる。また，高い相関ではないが交互作用効果が見たいといった場合は，逐次投入など慎重に個々の影響を考えながら投入するようにする(階層的重回帰分析)。なお交互作用項は，各変数の平均からの偏差をかけ合わせたものにすることが一般的である。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#課題",
    "href": "chapter11.html#課題",
    "title": "11  重回帰分析の基礎",
    "section": "11.7 課題",
    "text": "11.7 課題\n\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。画面には一部しか表示しておらず，全体(\\(n=100\\))はこちらex_regression1.csvからダウンロード可能です。このデータセットを用いて重回帰分析を行い，結果を出力してください。\n\n\n\n           y        x1         x2\n1  1.8685595 -4.248450  1.9997792\n2 -0.5728781  5.766103 -3.3435292\n3  1.0321850 -1.820462 -0.2277393\n4 10.0468488  7.660348  9.0894765\n5 -1.1968078  8.809346 -0.3419521\n6  9.6719213 -9.088870  7.8070044\n\n\n\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。画面には一部しか表示しておらず，全体(\\(n=300\\))はこちらex_regression2.csvからダウンロード可能です。このデータセットを用いて重回帰分析を行ってください。結果のプロットから，上に挙げた重回帰分析の仮定に反しているところを指摘してください。\n\n\n\n          y         x1           x2\n1  3.586304 -0.4248450  0.132767341\n2  8.599252  0.5766103  0.922713561\n3  2.397115 -0.1820462  0.053684622\n4  3.505236  0.7660348  0.007801881\n5  6.517720  0.8809346  0.633076091\n6 -1.394231 -0.9088870 -0.895346802\n\n\n\n\\(R^2=0.3\\)を目標として，説明変数の数\\(p=10\\)の重回帰分析を行う際に，必要なサンプルサイズはいくつになるか，計算してみましょう。ここで，\\(\\alpha = 0.05,\\beta=0.2\\)とします。\n\n\n\n\n\n南風原朝和. 2014. 心理統計学の基礎: 続・統合的理解のために. 有斐閣.\n\n\n吉田寿夫, and 村井潤一郎. 2021. “心理学的研究における重回帰分析の適用に関わる諸問題.” 心理学研究 92 (3): 178–87. https://doi.org/10.4992/jjpsy.92.19226.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房. http://ci.nii.ac.jp/ncid/BB27527420.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.\n\n\n西内啓. 2017. 統計学が最強の学問である[数学編]: データ分析と機械学習のための新しい教科書. ダイヤモンド社.\n\n\n豊田秀樹. 2017. もうひとつの重回帰分析. 東京図書.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#footnotes",
    "href": "chapter11.html#footnotes",
    "title": "11  重回帰分析の基礎",
    "section": "",
    "text": "もちろん証明できる。\\(\\beta_0 = \\bar{y} - \\beta_1\\bar{x},\\beta_1 = r_{xy} \\frac{s_y}{s_x}\\)より，\\(\\bar{\\bar{y}} = \\frac{1}{n}\\sum(\\bar{y} - \\beta_1\\bar{x} + \\beta_1x_i) = \\bar{y} - \\beta_1\\bar{x} + \\beta_1\\frac{1}{n}\\sum x_i = \\bar{y}\\)である。↩︎\nもちろん証明できる。\\(\\bar{e} = \\frac{1}{n}\\sum e_i = \\frac{1}{n} \\sum (y_i - \\hat{y}_i) = \\bar{y} = \\bar{\\hat{y}} = 0\\)である。↩︎\n縦軸の標準化された残差の大きな値は解釈に注意が必要な外れ値である可能性が高い。レバレッジも同様に回帰係数に大きな影響を与える値の指標であり，この図の端に位置する変数は注意が必要，と考える。↩︎\nもちろん証明できる。詳細は 小杉 (2018) を参照すること。↩︎\n論文が早期公開された後，心理学会が主催するオンラインシンポジウムでは著者とこの論文で取り上げられた論文の著者が登場して，議論が交わされた(日本心理学会YouTubeライブ・話題の論文について著者と語るシリーズ,2021年7月2日20時-21時40分)。平日の夜という設定，早期公開版における議論であったにも関わらず，1700名近い視聴者が参加した。↩︎\n2変数重回帰分析モデルで，VIFが3であれば説明変数間の相関は\\(r=0.81\\)程度である。VIFが10であれば\\(r=0.97\\)にもなる。詳しくは 小杉, 紀ノ定, and 清水 (2023) を参照。↩︎\nもちろん証明できる。小杉 (2018) を参照。↩︎\nこれに対して，データの階層性(ex.学級\\(\\subset\\)市区町村\\(\\subset\\)都道府県)を考慮する線形モデルのことを，階層線形モデルHierarichal Linear Model:HLM という。↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter12.html",
    "href": "chapter12.html",
    "title": "12  線型モデルの展開",
    "section": "",
    "text": "12.1 一般線型モデル",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter12.html#一般化線型モデル",
    "href": "chapter12.html#一般化線型モデル",
    "title": "12  線型モデルの展開",
    "section": "12.2 一般化線型モデル",
    "text": "12.2 一般化線型モデル",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter12.html#階層線型モデル",
    "href": "chapter12.html#階層線型モデル",
    "title": "12  線型モデルの展開",
    "section": "12.3 階層線型モデル",
    "text": "12.3 階層線型モデル",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html",
    "href": "chapter13.html",
    "title": "13  多変量解析の入り口",
    "section": "",
    "text": "13.1 因子分析",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>多変量解析の入り口</span>"
    ]
  },
  {
    "objectID": "chapter13.html#構造方程式モデリング",
    "href": "chapter13.html#構造方程式モデリング",
    "title": "13  多変量解析の入り口",
    "section": "13.2 構造方程式モデリング",
    "text": "13.2 構造方程式モデリング",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>多変量解析の入り口</span>"
    ]
  },
  {
    "objectID": "practices.html",
    "href": "practices.html",
    "title": "16  演習問題",
    "section": "",
    "text": "16.1 最終課題",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>演習問題</span>"
    ]
  },
  {
    "objectID": "practices.html#最終課題",
    "href": "practices.html#最終課題",
    "title": "16  演習問題",
    "section": "",
    "text": "無相関検定において，真の状態が母相関\\(\\rho=0.4\\)であったときに，サンプルサイズ\\(n=20\\)のデータをとって検定を行うとします。この時の帰無仮説の分布と，真の状態の分布を重ねて図示し，\\(\\alpha=0.05\\)の臨界値，検出力を可視化する図を描くコードを書いてください。(参考；南風原 (2002) ,Pp.144)\n2要因Betweenデザインの分散分析において，交互作用のみ有意になるようなサンプルデータを作るコードを書いてください。また，サンプルデータが正しくできているかどうかを確認するために，anovakunでの分析結果も出力させてください。\n2つの変数X,Yをもつ3つの群があり，群ごとX,Yの相関を見るとすべて\\(r = -0.3\\)程度の負の相関を持っているが，3つの群をあわせてX,Yの相関を見ると正の相関を示すようなデータセットを作流コードを書いてください。なお，出来上がったデータは群ごとに色分けした散布図で図示するようにしてください。\n\nヒント：群ごとに回帰分析のサンプルデータを作ることを考え，傾きは一貫して\\(\\beta_1=-0.3\\)であるのに対し，群ごとの切片\\(\\beta_0\\)を適当に調整すると良いでしょう。\nねらい：このようなデータは，相関を見るときに可視化することの重要性を伝えるとともに，階層線形モデルの必要性を理解することに役立ちます。\n\n\n\n\n\n\n南風原朝和. 2002. 心理統計学の基礎: 統合的理解のために. 有斐閣. http://amazon.co.jp/o/ASIN/4641121605/.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>演習問題</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bernaards, Coen A., and Robert I. Jennrich. 2005. “Gradient\nProjection Algorithms and Software for Arbitrary Rotation Criteria in\nFactor Analysis.” Educational and Psychological\nMeasurement 65: 676–96. https://doi.org/10.1177/0013164404272507.\n\n\nGabry, Jonah, Rok Češnovar, and Andrew Johnson. 2023. Cmdstanr: R\nInterface to ’CmdStan’.\n\n\nHadley, Wickham. 2014. “Tidy Data.” Journal of\nStatistical Software 59: 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nRevelle, William. 2021. Psych: Procedures for Psychological,\nPsychometric, and Personality Research. Evanston, Illinois:\nNorthwestern University. https://CRAN.R-project.org/package=psych.\n\n\nRosseel, Yves. 2012. “lavaan: An\nR Package for Structural Equation Modeling.”\nJournal of Statistical Software 48 (2): 1–36. https://doi.org/10.18637/jss.v048.i02.\n\n\nStevens, Stanley Smith. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nZeileis, Achim. 2005. “CRAN Task Views.” R\nNews 5 (1): 39–40. https://CRAN.R-project.org/doc/Rnews/.\n\n\nキーラン・ヒーリー. (2018) 2021.\nデータ分析のためのデータ可視化入門. Translated by 瓜生真也,\n江口哲史, and 三村喬生. 講談社.\n\n\nクルシュケJ. K. (2014) 2017. ベイズ統計モデリング: R, JAGS,\nStanによるチュートリアル 原著第2版. Translated by 前田和寛 and\n小杉考司. 共立出版.\n\n\nシ. 2016. 計算機言語のまとめノート. 暗黒通信団.\n\n\nランダー，J.P. (2017) 2018. みんなのr 第2版. Translated by\n高柳慎一, 津田真樹, 牧山幸史, 松村杏子, and 簑田高志. マイナビ出版.\n\n\n佐藤坦. 1994. はじめての確率論: 測度から確率へ. 共立出版.\n\n\n南風原朝和. 2002. 心理統計学の基礎: 統合的理解のために. 有斐閣.\nhttp://amazon.co.jp/o/ASIN/4641121605/.\n\n\n———. 2014. 心理統計学の基礎: 続・統合的理解のために. 有斐閣.\n\n\n吉田伸生. 2021. 確率の基礎から統計へ. 新装版. 日本評論社.\n\n\n吉田寿夫, and 村井潤一郎. 2021.\n“心理学的研究における重回帰分析の適用に関わる諸問題.”\n心理学研究 92 (3): 178–87. https://doi.org/10.4992/jjpsy.92.19226.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房.\nhttp://ci.nii.ac.jp/ncid/BB27527420.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023.\n数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計.\n技術評論社.\n\n\n平岡和幸, and 堀玄. 2009. プログラミングのための確率統計.\nオーム社. http://amazon.co.jp/o/ASIN/4274067750/.\n\n\n松村優哉, 湯谷啓明, 紀ノ定保礼, and 前田和寛. 2021. 改訂2版\nRユーザのためのRStudio[実践]入門:\nTidyverseによるモダンな分析フローの世界. 技術評論社.\n\n\n株式会社ホクソエム, trans. (2016) 2017. Rプログラミング本格入門:\n達人データサイエンティストへの道. 単行本. 共立出版.\n\n\n永田靖, and 吉田道弘. 1997. 統計的多重比較法の基礎.\nサイエンティスト社.\n\n\n池田功毅, and 平石界. 2016.\n“心理学における再現可能性危機：問題の構造と解決策.”\n心理学評論 59 (1): 3–14. https://doi.org/10.24602/sjpr.59.1_3.\n\n\n河野敬雄. 1999. 確率概論. 京都大学学術出版会.\n\n\n石田基広, 市川太祐, 高柳慎一, and 福島真太朗, trans. (2015) 2016.\nR言語徹底解説. 共立出版.\n\n\n総務省. 2020.\n“統計表における機械判別可能なデータ作成に関する表記方法.”\n統計企画会議申し合わせ. https://www.soumu.go.jp/main_content/000723697.pdf.\n\n\n西内啓. 2017. 統計学が最強の学問である[数学編]:\nデータ分析と機械学習のための新しい教科書. ダイヤモンド社.\n\n\n豊田秀樹. 2009. 検定力分析入門: Rで学ぶ最新データ解析ー.\n東京図書.\n\n\n———. 2017. もうひとつの重回帰分析. 東京図書.\n\n\n高橋康介. 2018. 再現可能性のすゝめ. Edited by 石田基広. Vol. 3.\nWonderful r. 共立出版.",
    "crumbs": [
      "References"
    ]
  }
]