[
  {
    "objectID": "chapter01.html",
    "href": "chapter01.html",
    "title": "1  はじめようR/RStudio",
    "section": "",
    "text": "1.1 環境の準備",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#環境の準備",
    "href": "chapter01.html#環境の準備",
    "title": "1  はじめようR/RStudio",
    "section": "",
    "text": "1.1.1 Rのインストール\nRのインストールに関して，初心者でも利用可能な資料がオンラインで公開されている。\nRはThe Comprehensive R Archive Network，通称CRAN2というネットワークで公開されている。CRANのトップページにはダウンロードリンクが用意されており，自分のプラットフォームにあった最新版をダウンロードしよう3。\n\n\n1.1.2 RStudioのインストール\nRのインストールが終われば，次はRStudioをインストールしよう。 RStudioは総合開発環境(IDE)と呼ばれるものである。Rは単体で，統計の分析や関数の描画など，専門的な利用に耐えうる分析機能を有している。その本質はもちろん計算機能であって，計算を実行する命令文(スクリプト)を与えれば，必要な返答をあたえてくれる。このように分析の本質が計算機能であったとしても，実際の分析活動に際しては，スクリプトの下書きと清書，入出力データや描画ファイルの生成・管理，パッケージ(後述)の管理など，分析にまつわるさまざまな周辺活動が含まれる。喩えるなら料理の本質が包丁・まな板・コンロによる加工であったとしても，実際の調理に際しては，広い調理スペースや使いやすいシンク，ボウルやタッパーなどの補助的な調理器具があった方がスムーズにことが進む。 いわば，R単体で分析をするのは飯盒炊爨のような必要最低限かつワイルドな調理法であり，RStudioは総合的な調理環境を提供してくれるものなのである。\n繰り返しになるが，本質的にはR単体で作業が可能である。なるべく単純な環境を維持したいというのであればR単体での利用を否定するものではないが，RStudioはエディタや文書作成ソフトとしても有用であるので，本授業ではRStudioを使うことを前提とする4。\n\n\n1.1.3 環境の準備に関する導入サイト\n以下に執筆時点(2024年1月)で参照可能な，導入に関するWeb教材を挙げておく。自分に合ったものを適宜参照し，RとRStudioを自身のPC環境に導入してほしい。もちろん自身で「R RStudio インストール」などとして検索しても良いし，chatGPTに相談しても良い。\n\n1.1.3.1 For Windows\n\n東京大学・大学院農学生命科学研究科アグリバイオインフォマティクス教育研究プログラムによるPDF資料\n初心者向けRのインストールガイド\n関西学院大学商学部土方ゼミ資料\n多摩大学情報社会研究所・応用統計学室資料\n奥村 晴彦先生のページ\n\n\n\n1.1.3.2 For Macintosh\n\n東京大学・大学院農学生命科学研究科アグリバイオインフォマティクス教育研究プログラムによるPDF資料\nnoteの記事\nいちばんやさしい，医療統計記事\n\nなお，Macの場合はHomebrewなどのパッケージ管理ソフトを使って導入することもできる(し，そのほうがいい)。その場合は以下の資料を参照。\n\n群馬大学大学院医学系研究科機能形態学の記事\nコアラさばお氏のnote記事\nRyu Takahashi氏のQiita記事\nYuhki Yano氏のQiita記事",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#rstudioの基礎４つのペイン",
    "href": "chapter01.html#rstudioの基礎４つのペイン",
    "title": "1  はじめようR/RStudio",
    "section": "1.2 RStudioの基礎（４つのペイン）",
    "text": "1.2 RStudioの基礎（４つのペイン）\nここまでで，RおよびRStudioを利用する準備が整っているものとする。\nさて，RStudioを起動すると大きくわけて4つの領域に分かれた画面が出てくる。この領域のことをペインと呼ぶ。図中の「領域1」がないように見えるときもあるが，下のペインが最大化され折りたたまれているだけなので，ペイン上部のサイズ変更ボタンを操作することで出てくるだろう。\n\n\n\nRStudioの初期画面\n\n\nこのペインのレイアウトは，メニューのTools &gt; Global Options… &gt; Pane Layoutから変更することもできる。基本的に4分割であることに変わりはないが，自分が利用しやすい位置にレイアウトを変更するとよい。\n\n\n\nレイアウト変更画面。このほかにも背景色などを変えることもできる\n\n\n以下，各ペイン(領域)が何をするところかを簡単に解説する。\n\n1.2.1 領域1；エディタ・ペイン\nエディタ領域。Rのスクリプトはもちろん，レポートの文章など，基本的に入力するときはこのペインに書く。ここで作業するファイルの種類は，File &gt; New Fileから見ると明らかなように，R言語だけでなくC言語，Python言語などのスクリプトや，Rmd，md,Qmd,HTMLなどのマークアップ言語，StanやSQLなど特殊な言語などにも対応している。ペインの右下に現在開かれているファイルの種類が表示されているのを確認しておこう。\nR言語でスクリプトを書く例で解説しよう。Rは命令を逐次実行していくインタプリタ形式であり，ここに記述されたRコードを，右上のRunボタンでコンソールに送って計算を実行するように使う。一回の命令をコマンド，コマンドが積み重ねられた全体をスクリプト，あるいはプログラムと呼ぶ。複数のコマンドを実行したい場合は，エディタ領域で複数行選択してRunボタンを，スクリプトファイル全体を実行したいときはRunボタンのとなりにあるSourceを押す。CTRL+Enter(Macの場合はコマンド+Enter)でRunボタンのショートカットになる。\n\n\n1.2.2 領域2；コンソール・ペイン\nR単体で利用する場合は，ここのペインだけを利用するようなものである。すなわち，ここに示されているのがR本体というか，Rの計算機能そのものである。ここに「＞」の記号が表示されているところをプロンプトといい，プロンプトが表示されているときはRが入力待ちの状態である。\nRは逐次的に計算を行うので，プロンプトのある状態でコマンドを入力すると計算結果が返される。 ここに直接コマンドを書いて行っても良いが，書き間違えたりすることもあるし，コマンドが複数行に渡ることが一般的になってくるので，エディタ領域に清書するつもりで記述していったほうがよい。ごくたまに，一時的に確認したいことがある時だけ，直接コンソールを触るようにすると良い。\nなお，コンソールを綺麗にしたいときは右上の箒ボタンをおすとよい。\n\n\n1.2.3 領域3；環境ペイン\n基本的にこのペインと次の領域4のペインは複数のタブが含まれる。Pane Layoutでどちらにどのタブを含めるかを自分好みにカスタマイズすることもできる。ここでは代表的な2つのタブについてのみ言及する。\nEnvironmentタブは，Rの実行メモリ内に保管されている変数や関数などが表示されている。「変数や関数など」をまとめてオブジェクトというが，ここで内容や構造をGUIで確認することができる。\nHistoryタブは履歴である。これまでコンソールに送られてきたコマンドが順に記録されている。Historyタブからエディタ，コンソールにコマンドを送ることも可能であり，「さっきの命令をもう一度実行したい」といったときに参照すると良い。\n\n\n1.2.4 領域4；ファイルペイン\nここでも代表的なタブについてのみ解説する。\nFilesタブはMacでいうFinder，Windowsでいうエクスプローラーのような，ファイル操作画面である。フォルダの作成，ファイルの削除，リネーム，コピーなどの操作が可能である。\nPlotタブはRコマンドで描画命令が出された時の結果がここに表示される。RStudioの利点の一つは，このPlotから図をファイルにExportすることが可能であり，その際にファイルサイズやファイル形式を指定できるところにある。\nPackagesタブは読み込まれているパッケージ，(読み込まれていないが)保管しているパッケージのリストが表示されている。新しくパッケージを導入するときも，ここのinstallボタンから可能であり，保管しているパッケージのアップデートもボタンひとつで可能である。なお，パッケージについては後ほど言及する。\nHelpタブはRコマンドでヘルプを表示する命令(help関数)が実行された時の結果が表示される領域である。ヘルプを使うことで関数の引数，戻り値，使用例などを参照できる。\n\n\n1.2.5 そのほかのタブ\nそのほか，表示の有無もオプションになっているようないくつかのタブについて，簡単に解説しておく。\nConnectionsタブはRを外部データベースなどに繋げるときに参照する。大規模データをローカルにすべて取り込むことなく，SQLで必要なテーブルだけ取り出すといった操作をする際は必要になってくるだろう。\nGitタブはR，とくにRプロジェクト(後述)のバージョンを管理するときに利用する。Gitとは複数のプログラマによって同時並行的にプログラムを作っていく時の管理システムである。時系列的な差分の記録を得意とするシステムなので，レポートの作成時などに応用すればラボノートの記録としても利用できる。\nBuildタブはRパッケージやWebサイトを構築するときに利用する。なおこの資料もRStudioを利用して作られており，資料を生成(原稿からHTMLやPDFにする)ときにはこのタブを利用している。\nTutorialタブはチュートリアルツアーを楽しむ時のタブである。\nViewerタブはRStudioで作られたHTMLやPDFなどを見るためのタブである。\nPresentationタブはRStudioで作られたプレゼンテーションを見るためのタブである。\nTerminalタブはWindows/MacでいうTerminal，Linuxでいう端末についてのタブであり，Rに限らず，コマンドラインを通じてOSに命令するときに使う。\nBackground Jobsタブはその名の通りバックグラウンドで作業をさせるときに利用する。Rは基本的にシングルコアで計算が実行されるが，このタブを使ってスクリプトファイルをバックグラウンドで実行することで並列的に作業が可能になる。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#rのパッケージ",
    "href": "chapter01.html#rのパッケージ",
    "title": "1  はじめようR/RStudio",
    "section": "1.3 Rのパッケージ",
    "text": "1.3 Rのパッケージ\nRは単体でも線型モデルなどの基本的な分析は可能であるが，より進んだ統計モデルを利用したい場合は専門のパッケージを導入することになる。パッケージとは関数群のことであり，これもCRANやGithubなどインターネットを介して提供されている。ちなみに提供されているパッケージは，CRANで公開されているものだけで344,607件あり5，Github6で公開されているものなど，CRANを介さないパッケージも少なくない。\nパッケージを利用する際は，まずローカルにパッケージファイルをインストールしなければならない。その上で，Rを起動するごとに(セッションごとに)，関数libraryでパッケージを呼び出して利用する。インストールを毎回行う必要はないことに注意。\nインストールはRのコマンドでも可能だが，RStudioのPackagesペインを使って導入するのが簡単だろう。以下に，一部の有名かつ有用なパッケージ名とその簡単な説明を挙げる。本講義の中で使うものもあるので，事前に準備しておくことが望ましい。\n\ntidyverseパッケージ(Wickham et al. 2019)；Rが飛躍的に使いやすくなったのは，このtidyverseパッケージ導入以後のことである。開発者のHadley WickhamはR業界で神と崇められており，R業界に与えたインパクトは大きい。このパッケージは「パッケージ群」「パッケージのパッケージ」であり，tidyverseとはtidyな(整然とした)verse(世界)というような意味合いである。このパッケージは統計分析モデルを提供するものではなく，その前のデータの前処理に関する便利な関数を提供する7。このパッケージをインストールすると，関連する依存パッケージが次々取り込まれるので，少々時間がかかる。\npsychパッケージ(Revelle 2021)；名前の通り，心理学統計に関する統計モデルの多くが含まれている。特に特殊な相関係数や，因子分析モデルなどは非常に便利なので，インストールしておいて間違いない。\nGPArotationパッケージ(Bernaards and Jennrich 2005)；因子分析における因子軸の回転に使うパッケージ。\nstylerパッケージ；スタイルを整えてくれるパッケージ。スクリプトの清書に便利。\nlavaanパッケージ(Rosseel 2012)；潜在変数を含んだモデル(LAtent VAriable ANalysis)の分析，要するに構造方程式モデリング(Structural Equation Modeling;SEM，共分散構造分析ともいう)を実行するパッケージ。\nctvパッケージ(Zeileis 2005); CRAN Task Viewsの略で，膨大に膨れ上がったCRANから必要なパッケージを見つけ出すのは困難であることから，ある程度のジャンルごとに関連しそうなパッケージをまとめて導入してくれるのがこのパッケージ。例えば，このパッケージをインストールした後で，install.views(\"Psychometrics\")とすると，心理統計関係の多くのパッケージを次々導入してくれる。\ncmdstanrパッケージ(Gabry, Češnovar, and Johnson 2023)；複雑な統計モデルで利用される，確率的プログラミング言語stanをRから使うことができるようになるパッケージ。導入にはこのパッケージの他にもstanやコンパイル環境の準備が必要なので，公式の導入サイトも参考にしてほしい。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#rstudioのプロジェクト",
    "href": "chapter01.html#rstudioのプロジェクト",
    "title": "1  はじめようR/RStudio",
    "section": "1.4 RStudioのプロジェクト",
    "text": "1.4 RStudioのプロジェクト\n実際にRを使っていく前に，最後の準備としてRStudioにおけるプロジェクトについて解説しておく。\nみなさんも，PCをつかって文書を作ったり保管したりするときに，フォルダにまとめて入れておくことがあるだろう。フォルダは例えば「文書」&gt;「心理学」&gt;「心理学統計演習」のように階層的に整理することが一般的で，そうしておくことで必要なファイルをすぐに取り出すことができる。\n逆に言えば，こうしたフォルダ管理をしておかなければファイルがPCのなかで散乱してしまい，必要な情報を得るために逐一PCの中身を検索しなければならないだろう。\nR/RStudioをつかった分析実践の場合も同様で，一回のテーマについて複数のファイル(スクリプトファイル，データファイル，画像ファイル，レポートなど文書ファイル等々)があり，シーンに合わせて(例えば「授業」「卒論」など)フォルダで管理することになる。\nさらに，PC環境には作業フォルダ(Working Directory)8という概念がある。たとえばR/RStudioを起動・実行しているときに，Rが「今どこで」実行されているか，どこを管理場所としているか，を表す概念である。例えばこの作業フォルダの中にsample.csvというファイルがあって，それをスクリプト上から読み込みたい，というコマンドを実行するのであれば，そのままファイル名を書けば良い。しかし別の場所にそのファイルが保存されているのなら，作業フォルダから見た相対的な位置を含めて指示してやるか(相対パス)，あるいはPC環境全体からみた絶対的な位置を含めて(絶対パス)指示してやる必要がある。相対・絶対パスの違いは，「ここから二つ目の角を右」のように指示するか，住所で指示するかの違いであると考えれば良い。\nともあれ，この作業フォルダがどこに設定されているかは，実行するときに常に気にしていなければならない。ちなみにこの作業フォルダは，RStudioのファイルペイン・Filesタブでひらいているところとは限らないことに注意してほしい。GUI上でエクスプローラ/Finderで開いたからといって，作業フォルダが自動的に切り替わるようにはなっていない。\nそこでRStudioのプロジェクトである。RStudioには「プロジェクト」という概念があり，作業フォルダや環境の設定などをそこで管理することができる。新しくプロジェクトを始めるときはFile&gt;New Project，すでに一度プロジェクトを作っているときはFile &gt; Open Projectとしてプロジェクトファイル(拡張子が.projのファイル)を開くようにする。そうすると，作業フォルダが当該フォルダに設定される。プロジェクトをGitに連携しておくとバージョン管理などもフォルダ単位で行える。\n以後，本講義で外部ファイルを参照する場合，プロジェクトフォルダの中にそのファイルがあるものとして(パスを必要としない形で)論じるので注意されたし。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#課題",
    "href": "chapter01.html#課題",
    "title": "1  はじめようR/RStudio",
    "section": "1.5 課題",
    "text": "1.5 課題\n\nRの最新版をCRANからダウンロードし，自分のPCにインストールしてください。\nRStudioのDesktop版をPosit社のサイトからダウンロードし，自分のPCにインストールしてください。\nRStidoを起動し，ペインレイアウトをデフォルトではない状態に並べ直してみてください。ソースペインを3列にするのも良いでしょう。\nコンソールペインに書かれている文字を全て消去してみてください。\nファイルペインにあるFilesタブをつかって，色々なフォルダを開けてみたり，不要なファイルを削除したり，ファイル名を変更したりしてみてください。\nファイルペインにあるFilesタブを開き，MoreのところからGo To Working Directoryを選択・実行してください。何か起こったでしょうか。\nこの授業のために，新しいプロジェクトを作成してください。プロジェクトは新しいフォルダでも，既存のフォルダでも構いません。\nプロジェクトが開いた状態のとき，RStudioのウィンドウ・タブのどこかに「プロジェクト名」が表示されているはずです。確認してください。\nまたファイルペインのFilesタブから，色々なファイル操作をした上で，改めてGo To Working Directoryをしてください。プロジェクトフォルダの中に戻ってこれたら成功です。\n新しいRスクリプトファイルを開き，空白のままで結構ですからファイル名をつけて保存してください。\nRStudioを終了あるいは最小化させ，OSのエクスプローラ/Finderから，プロジェクトフォルダに移動してください。先ほど作ったファイルが保存されていることを確認してください。\nプロジェクトフォルダには，プロジェクト名+.projというファイルが存在するはずです。これを開いて，RStudioのプロジェクトを開いてください。\nRStudioのFile &gt; Close Projectからプロジェクトを閉じてください。画面の細部でどこが変わったか，確認してください。\nRStudioを終了し，再びRStudioを起動してください。起動の方法はプロジェクトファイルからでも，アプリケーションの起動でも構いません。起動後に，プロジェクトを開いてください(あるいはプロジェクトが開かれていることを確認してください。)。\n\n\n\n\n\nBernaards, Coen A., and Robert I. Jennrich. 2005. “Gradient Projection Algorithms and Software for Arbitrary Rotation Criteria in Factor Analysis.” Educational and Psychological Measurement 65: 676–96. https://doi.org/10.1177/0013164404272507.\n\n\nGabry, Jonah, Rok Češnovar, and Andrew Johnson. 2023. Cmdstanr: R Interface to ’CmdStan’.\n\n\nRevelle, William. 2021. Psych: Procedures for Psychological, Psychometric, and Personality Research. Evanston, Illinois: Northwestern University. https://CRAN.R-project.org/package=psych.\n\n\nRosseel, Yves. 2012. “lavaan: An R Package for Structural Equation Modeling.” Journal of Statistical Software 48 (2): 1–36. https://doi.org/10.18637/jss.v048.i02.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nZeileis, Achim. 2005. “CRAN Task Views.” R News 5 (1): 39–40. https://CRAN.R-project.org/doc/Rnews/.\n\n\n松村優哉, 湯谷啓明, 紀ノ定保礼, and 前田和寛. 2021. 改訂2版 RユーザのためのRStudio[実践]入門: Tidyverseによるモダンな分析フローの世界. 技術評論社.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter01.html#footnotes",
    "href": "chapter01.html#footnotes",
    "title": "1  はじめようR/RStudio",
    "section": "",
    "text": "2024年1月現在で，TokyoだけでなくFukuoka，Sapporo，Yamaguchi，Irumaなどで地方コミュニティがあり，参加者みんなで楽しまれている。↩︎\nCRANは「しーらん」，あるいは「くらん」と発音される。筆者はしーらん派。↩︎\nこの授業のために自身のPCにRをインストールしたとして，次に使うときに半年以上間隔が空いたのなら，改めて最新版をチェックし，バージョンが上がっていたら旧版をアンインストールして最新版をインストールするところから始めた方が良い。Rで利用するパッケージなどが新しい版にしか対応していないことなどもある。Rと畳は新しい方が良い。↩︎\nVSCodeのようなエディタから使うことも可能であるし，Jupyter Notebookの計算エンジンをRにすることも可能。最近では分析ソフトウェアを個々人で準備せず，環境として提供することも一般的になってきており，例えばGoogle ColaboratoryのエンジンをRにすることもできるようになっている。ローカルPCに自前の環境を作るということが，時代遅れになる日も近いかもしれない。↩︎\n2024年01月18日調べ↩︎\nGitはバージョン管理システムであるが，これをインターネット上のサーバ(レポジトリ)で行うものをGithubという。RStudioはGithubとも連携しており，プロジェクトをGithubと紐づけることで簡単にバージョン管理ができる。しかもここで言及しているように，Github上でパッケージを公開することもできるので，最近はCRANの校閲を待たずに公開できるGithubが好まれている側面もある。↩︎\n実は統計データの解析にかかる時間のほとんどが，解析に適切な形にデータを整形する「前処理」に費やされる。前処理，別名データハンドリングをいかに上手く，素早く，直感的にできるかは，その後の分析にも影響するほど重要な手順であるため，tidyverseパッケージの登場はありがたかった。これを使ったデータハンドリングだけの専門書 松村 et al. (2021) が重宝されるほどである。↩︎\nここでは，フォルダとディレクトリは同じ意味であると思ってもらって良い。一般に，CUIではディレクトリ，GUIではフォルダという用語が好まれる。語幹directにあるように，ファイルやアクセス先など具体的な指し示す先を強調しているのがディレクトリであり，それにファイル群などまとまった容れもの，という意味を付加したのがフォルダである。フォルダの方が言葉としてわかりやすいし。↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめようR/RStudio</span>"
    ]
  },
  {
    "objectID": "chapter10.html",
    "href": "chapter10.html",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "10.1 疑わしき研究実践 Questionable Research Practicies",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#疑わしき研究実践-questionable-research-practicies",
    "href": "chapter10.html#疑わしき研究実践-questionable-research-practicies",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "10.1.1 検定の繰り返し\n帰無仮説検定は確率を伴った判断なので，「差がないのにあると判断してしまった(タイプ1エラー)」とか，「差があるのに検出できなかった(タイプ2エラー)」といった問題が生じうる。すでに述べたように，タイプ2エラーの方は本質的に知り得ないので(差がどの程度あるか，事前にわかっていることがない)，せめてタイプ1エラーは制御することを目指すことになる。\nこうした検定は合理的に行われるべきもので，なんとか有意に「したい」といった研究者のお気持ちとは独立しているはずである。しかし(もしかすると)意図せぬところで，この制御に失敗してしまっている可能性がある。\nひとつは検定の繰り返しに関する問題である。たとえば分散分析において，「主効果が出てから下位検定で各ペアの検証をするんだから，最初から各ペアのt検定を繰り返せばいいじゃないか」と考える人がいるかもしれない。これで本当に問題ないのか，シミュレーションで確認してみよう。\n以下のコードは，有意差のないデータセットを作り，1.分散分析を行なって有意になるかどうか，2.各ペアについて繰り返しt検定を行い，どこかに有意差が検出されるかどうか，を比較している。分散分析はANOVA君ではなく，R固有のaov関数を用いた1。また，「どこかに有意差が検出される」をif文を使って書いているところを，注意深く確認しておいてほしい。\n\nlibrary(tidyverse)\nlibrary(broom) # 分析結果をtidyに整形するパッケージ。ない場合はinstallしておこう\n\n\nalpha &lt;- 0.05 # 有意水準を0.05に設定\nn1 &lt;- n2 &lt;- n3 &lt;- 10 # 各グループのサンプルサイズを10に設定\nmu &lt;- 10 # 平均値を10に設定\nsigma &lt;- 2 # 標準偏差を2に設定\n\nmu1 &lt;- mu2 &lt;- mu3 &lt;- mu # 各グループの平均値を同じに設定\n\nset.seed(12345) # 乱数のシードを設定して再現性を確保\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\n\nanova.detect &lt;- rep(NA, iter) # ANOVA検出結果の保存用ベクトルを初期化\nttest.detect &lt;- rep(NA, iter) # t検定検出結果の保存用ベクトルを初期化\n\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  X1 &lt;- rnorm(n1, mu, sigma) # グループ1のデータを生成\n  X2 &lt;- rnorm(n2, mu, sigma) # グループ2のデータを生成\n  X3 &lt;- rnorm(n3, mu, sigma) # グループ3のデータを生成\n\n  dat &lt;- data.frame( # データフレームを作成\n    group = c(rep(1, n1), rep(2, n2), rep(3, n3)), # グループ番号を追加\n    value = c(X1, X2, X3) # データを追加\n  )\n  result.anova &lt;- aov(value ~ group, data = dat) %&gt;% tidy() # ANOVAを実行し結果を整形\n  anova.detect[i] &lt;- ifelse(result.anova$p.value[1] &lt; alpha, 1, 0) # 有意差があるかを判定して保存\n\n  # t検定を繰り返す\n  ttest12 &lt;- t.test(X1, X2)$p.value # グループ1と2のt検定\n  ttest13 &lt;- t.test(X1, X3)$p.value # グループ1と3のt検定\n  ttest23 &lt;- t.test(X2, X3)$p.value # グループ2と3のt検定\n\n  ttest.detect[i] &lt;- ifelse(ttest12 &lt; alpha | ttest13 &lt; alpha | ttest23 &lt; alpha, 1, 0) # いずれかのt検定で有意差があれば保存\n}\n\nttest.detect %&gt;% mean() # t検定で有意差が検出された割合を計算\n\n[1] 0.109\n\nanova.detect %&gt;% mean() # ANOVAで有意差が検出された割合を計算\n\n[1] 0.04\n\n\n結果を見ると，t検定で有意差が検出された確率が0.109であり，設定した\\(\\alpha\\)水準を大きく上回っていることがわかる。有意でないところに有意差を見出しているのだから，これはタイプ1エラーのインフレである。分散分析で検出された結果は0.04であり，正しく\\(\\alpha\\)水準がコントロールできている。\n検定を繰り返すことの問題は，確率的判断にある。5%の水準でタイプ1エラーが起こるということは，95%の確率で正しく判断できるということだが，2回検定を繰り返すとその精度は\\((1-0,05)^2=0.9025\\)であり，3回検定を繰り返すと\\((1-0.05)^3=0.857375\\)と，どんどん小さくなっていってしまう。検定はタイプ1エラーのハンドリングが目的であったことを忘れてはならない。\n\n\n10.1.2 ボンフェロー二の方法\n一つの論文のなかに複数の研究(Study1, Study2,…)があり，それぞれで検定による確率的判断を行っているとしよう。それぞれ別のデータセットに対する検定であっても，一つの露文の中で確率的判断が繰り返されていることに違いはない。このような場合は，どのようにして有意水準をコントロールすれば良いのだろうか。\n最も単純明快な方法のひとつは，分散分析の下位検定でもみられたBonferroniの補正である。すなわち，検定の回数で有意水準を割ることで，検定を厳しくするのである。5%水準の検定を5回繰り返すのなら，\\(0.05/5=0.01\\)とすることで全体的なタイプ1エラー率を抑制するのである。これが正しく機能するかどうか，シミュレーションで確認してみよう。\n反復してデータを生成することになるので，仮想データ生成関数を別途事前に準備しておこう。\n\n# シミュレーション用の関数を定義\nstudyMake &lt;- function(n, mu, sigma, delta) {\n  X1 &lt;- rnorm(n, mu, sigma) # グループ1のデータを生成\n  X2 &lt;- rnorm(n, mu + sigma * delta, sigma) # グループ2のデータを生成（平均値が異なる）\n  dat &lt;- data.frame( # データフレームを作成\n    group = rep(1:2, each = n), # グループ番号を追加\n    value = c(X1, X2) # データを追加\n  )\n  result &lt;- t.test(X1, X2)$p.value # グループ間のt検定を実行\n  return(result) # p値を返す\n}\n\nこの関数は，引数としてサンプルサイズn，平均値mu，標準偏差sigma，効果量deltaをとり，2群のt検定の結果である\\(p\\)値を返す関数である。\n\n# 使用例；t検定の結果のp値が戻ってくる\nstudyMake(n = 10, mu = 10, sigma = 1, delta = 0)\n\n[1] 0.9444895\n\n\nこれ一回で1分析するので，これを複数回行って一つの研究とし，一つの論文のなかでnum_studies回の研究を行ったとしよう。今回はnum_studies = 3としている。Rのreplicate関数で研究回数繰り返した\\(p\\)値ベクトルを得て，どこかに差が検出されるかどうかをチェックする。「どこかに」を表現するためにany関数を使って判定する。判定する有意水準として，\\(\\alpha\\)と補正をかけた\\(\\alpha_{adj}\\)の2つを用意した。\n\nset.seed(12345) # 乱数のシードを設定して再現性を確保\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\nalpha &lt;- 0.05 # 有意水準を0.05に設定\nnum_studies &lt;- 3 # 研究の数を3に設定\nalpha_adjust &lt;- alpha / num_studies # 多重検定補正後の有意水準を計算\n\nFLG.detect &lt;- rep(NA, iter) # 検出結果を保存するベクトルを初期化\nFLG.detect.adj &lt;- rep(NA, iter) # 補正後の検出結果を保存するベクトルを初期化\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  p_values &lt;- replicate(num_studies, studyMake(n = 10, mu = 10, sigma = 1, delta = 0)) # 各研究のp値を生成\n  FLG.detect[i] &lt;- ifelse(any(p_values &lt; alpha), 1, 0) # 補正前の有意差検出を判定して保存\n  FLG.detect.adj[i] &lt;- ifelse(any(p_values &lt; alpha_adjust), 1, 0) # 補正後の有意差検出を判定して保存\n}\n\nFLG.detect %&gt;% mean() # 補正前の有意差検出率を計算\n\n[1] 0.145\n\nFLG.detect.adj %&gt;% mean() # 補正後の有意差検出率を計算\n\n[1] 0.049\n\n\n結果を見ると，\\(\\alpha\\)水準のまま検定を行うと，論文全体でのタイプ1エラー率が0.145と5%を上回っており，3つの研究のどこかで間違った判断をしていることがわかる。補正すると0.049と正しく制御されている。\n一連の研究をまとめた一つの論文に，複数の研究が含まれていることは少なくない。各検定結果をまとめて総合考察とすることも一般的である。総合考察は各分析結果から全体的な結論を導くのだが，その要素のどこかに間違いがあると，全体の論立てが崩れてしまうことにもなりかねない。いわば腐った支柱が紛れ込んでいる土台の上に家屋を建てるようなもので，研究の積み重ねを目的とする科学活動の一環である以上，正しく制御されていることは重要である。\n\n\n10.1.3 N増し問題\n人間を対象にした研究を行って，データを一生懸命取る。その結果，効果があると見られた操作/介入から統計的な有意差が検出されなければ，「悔しい」という心情になることは理解できる。もう少し実験を工夫すれば，もう少しデータが違えばよかったのでは，と思うかもしれない。ではもう少し頑張ってデータを増やしてみればどうだろうか。\n実はこの考え方はQRPsのひとつである。検定は真偽判定をする競技のようなものなので，ゲームの途中でプレイヤーの人数が変わるのはよろしくない。このことをシミュレーションで確認してみよう。\n以下のコードは，t検定のデータを最初n1=n2=10で作成して行っている。タイプ1エラーの検証をするので，効果量は\\(0\\)である。ここでt検定を行い，もしその\\(p\\)値が\\(\\alpha\\)よりも大きかったら，つまり有意であると判断されなかったら，同じ方法でデータを1件追加する。そしてまたt検定を行う。このサンプルの追加は，効果量\\(0\\)なので，偶然のお許しが出るまでいつまで経っても終わることがないため，上限を100にしてある。上限に達したら流石に諦めてもらうとして，さてそうしたQRPsな努力の結果，\\(\\alpha\\)水準はどれぐらいに保たれているだろうか。\n\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\nalpha &lt;- 0.05 # 有意水準を0.05に設定\np &lt;- rep(0, iter) # p値を保存するベクトルを初期化\nadd.vec &lt;- rep(0, iter) # 増やした人数を保存するベクトルを初期化\n\nset.seed(123) # 乱数のシードを設定して再現性を確保\n\nn1 &lt;- n2 &lt;- 10 # 各グループのサンプルサイズを10に設定\nmu &lt;- 10 # 平均値を10に設定\nsigma &lt;- 2 # 標準偏差を2に設定\ndelta &lt;- 0 # 平均の差を0に設定\n\n## シミュレーション本体\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  # 最初のデータを生成\n  Y1 &lt;- rnorm(n1, mu, sigma) # グループ1のデータを生成\n  Y2 &lt;- rnorm(n2, mu + sigma * delta, sigma) # グループ2のデータを生成\n  p[i] &lt;- t.test(Y1, Y2)$p.value # t検定を実行しp値を保存\n  # データを追加する\n  count &lt;- 0 # 追加したデータの数をカウント\n  ## p値が5%を下回るか、データが100になるまでデータを増やし続ける\n  while (p[i] &gt;= alpha && count &lt; 100) { # 条件を満たすまでループを繰り返す\n    # 有意でなかった場合、変数ごとに1つずつデータを追加\n    Y1_add &lt;- rnorm(1, mu, sigma) # グループ1に新しいデータを1つ追加\n    Y2_add &lt;- rnorm(1, mu + sigma * delta, sigma) # グループ2に新しいデータを1つ追加\n    Y1 &lt;- c(Y1, Y1_add) # グループ1のデータを更新\n    Y2 &lt;- c(Y2, Y2_add) # グループ2のデータを更新\n    p[i] &lt;- t.test(Y1, Y2)$p.value # 新しいデータでt検定を再度実行しp値を更新\n    count &lt;- count + 1 # データを追加した回数をカウント\n  }\n  add.vec[i] &lt;- count\n}\n\n## 結果\nifelse(p &lt; 0.05, 1, 0) |&gt; mean() # p値が5%未満の割合を計算\n\n[1] 0.306\n\nhist(p)\n\n\n\n\n\n\n\nhist(add.vec)\n\n\n\n\n\n\n\n\n結果をみると，0.306とかなり逸脱して，誤った結論に辿り着いていることがわかる。努力の結果得られた有意差は，偶然の賜物でもあり，誤った研究実践による幻想にすぎない。加えたデータのヒストグラムからわかるように，悲しいかな，75%もの割合で上限100まで達してしまう。百害あって一利なしとはこのことである。\n\n\n10.1.4 サンプルサイズを事前に決めないことの問題\nサンプルサイズを事前に決めずに検定する，という状況を別の角度から見てみよう。 クルシュケ ([2014] 2017) は「コインフリップを24回して，うち7回表が出た」というシーンを例に挙げて説明している。7/24は半分を下回っているから，やや裏が出やすいコインであるように思える。帰無仮説として，このコインは公平である(表と裏が出る確率が半々である)，というのを検証したいとする。\nこの24回中7回成功，という話の背後に「24施行する」ということを決めていたかどうか(サンプルサイズを事前に決めていたか)ということを考えてみよう。\nまずは正直に，最初から24回コインフリップすることを決めていたとする。コインフリップはベルヌーイ試行2 であり，それを繰り返すので二項分布に従うと考えられる。そこで，二項検定として次のように計算できるだろう。\n\nN &lt;- 24\n# 7回表が出る確率\npbinom(7, N, 0.5) * 2\n\n[1] 0.06391466\n\n\n二項分布のp値を出すにはpbinomを使いった。また帰無仮説として，このコインフリップは公平であると考えているのだから，\\(\\theta=0.5\\)が帰無仮説の状態である。この\\(\\theta=0.5\\)とした時に，\\(N=24,k=7\\)という結果になる確率を計算し，かつ両側検定(公平でない，が対立仮説なので裏が7回でもよい)であることを考えて確率を2倍した。p値は0.0639147であるから，5%水準では有意であると判定できない。これぐらいの確率はあるということだ。\nしかしここで第二の状況を考えてみよう。24回コインフリップすることを決めていたのではなくて，7回成功するまでコインフリップを続けたところ，結果的に24回で終わったということだった，とするのである。このようなシーンの確率分布は負の二項分布と呼ばれ，pnbinomで次のように計算できる。\n\nk &lt;- 7\n# 24回以上必要な確率\npnbinom(k, 24, 0.5) * 2\n\n[1] 0.003326893\n\n\nこの結果から，\\(\\theta=0.5\\)の時に7回表がでるまでに24施行も必要とする確率は，0.0033269だから，5%水準で有意である。つまり，滅多にこんなことが起きないので，\\(\\theta=0.5\\)という帰無仮説が疑わしいことになる。ここではシーンが異なるとp値が違っている，ということに注意してほしい。\nさらに第3のシーンを考えよう。これは「何回やるかは決めてないけど，まあ5分ぐらいかな」と試行にかける時間だけ決めていたという状況である。結果的に24回になったけど，もしかすると23回だったかもしれないし，25回や20回，30回だったかもしれない。これをシミュレーションするために，「24がピークになるような頻度の分布」をポアソン分布を使って生成する[^10:3]。\n[^10:3] ポアソン分布は正の整数を実現値に取る分布で，カウント変数の確率分布として用いられる。パラメータは\\(\\lambda\\)だけであり，期待値と分散が\\(\\lambda\\)に一致する，非常にシンプルな分布である。\n\nset.seed(12345)\niter &lt;- 100000 # 発生させる乱数の数\n## 24回がピークに来るトライアル回数\ntrial &lt;- rpois(iter, 24)\nhist(trial)\n\n\n\n\n\n\n\n\nこの各トライアルにおいて，二項分布で成功した回数を計算し，トライアル回数で割ることによって，表が出る確率のシミュレーションができる。その時の割合は，\\(7/24\\)よりもレアな現象だろうか?\n\nresult &lt;- rep(NA, iter)\nfor (i in 1:iter) {\n  result[i] &lt;- rbinom(1, trial[i], 0.5) / trial[i]\n}\n## 7/24よりも小さい確率で起こった?\nlength(result[result &lt; (7 / 24)]) / iter\n\n[1] 0.02262\n\n\nこれを見ると，両側検定にしても0.04524なので，ギリギリ有意になるかどうか，というところだろうか。\nさて判断にこまった。「24回やる」と決めていたのであれば\\(\\theta=0.5\\)は棄却されないし，「7回成功するまで」と決めていたのであれば\\(\\theta=0.5\\)は棄却される。「5分間」と決めていても棄却されるが，そもそもこうした実験者の意図によって判断が揺らいで良いものだろうか?というのが クルシュケ ([2014] 2017) の指摘する疑問点である。\n問題は，「24回中7回成功」という事実に，二項分布，負の二項分布，あるいは組み合わさった分布のような，確率分布の情報が含まれていないことにある。この確率分布はデータが既知で母数が未知だから尤度関数であり，データ生成メカニズムであるとも言えるだろう。想定するメカニズムが明示されない検定は，ともすれば事後的に「実は負の二項分布を想定していたんですよ，へへ」ということも可能になってしまう。こうした点からも，研究者の自由度をなるべく少なくする研究計画の事前登録制度が必要であることがわかる。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#サンプルサイズ設計",
    "href": "chapter10.html#サンプルサイズ設計",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "10.2 サンプルサイズ設計",
    "text": "10.2 サンプルサイズ設計\nどのようにデータを取り，どのように分析・検定し，どのような基準で判断するかを事前に決めることに加え，事前にサンプルサイズを見積もっておく必要があるだろう。サンプルはとにかく多ければ多いほど良いか，というとそうではなく，過剰にサンプルを集めることは研究コストの増大であり，回答者の負担増でしかない。またサンプルサイズが大きくなると有意差を検出しやすくなるが，必要なのは有意差ではなく実質的に効果を見積もることであり，有意差が見つかれば良いというものではないことに注意が必要である。もちろん上で見てきたように，有意差が検出できるかどうかを指標にしてサンプルサイズを変動させてしまうのは，明らかに誤った研究実践である。\n事前にサンプルサイズを決定するのに必要なのは，これまでのリバースエンジニアリングの演習例からもわかるように，効果量の見積もりである。3 これをどのように定めるかについては，先行研究を考えるとか，研究領域で「これぐらい差がないと意味がないよね」とコンセンサスが取れている程度で決めることになる。4\n\n10.2.1 対応のないt検定\nサンプルサイズの設計には，これまで使ってきた検定統計量に非心度non-centrality parameterというパラメータを加えて考える必要がある。\n具体的に，対応のないt検定を例にサンプルサイズ設計の方法を見てみよう。t検定は言葉の通り，t分布を用いて帰無仮説の元での検定統計量の実現値が問題になるのであった。帰無仮説は\\(\\mu_0 = \\mu_1-\\mu_2 = 0\\)であり，検定統計量は次式で表されるのであった。\n\\[ T = \\frac{d - \\mu_0}{\\sqrt{U^2_p/\\frac{n_1n_2}{n_1+n_2}}}\\]\nこの分子において，\\(d - \\mu_0 = (\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)\\)の第二項を,\\(\\mu_1-\\mu_2 = 0\\)と仮定するから，\\(0\\)が中心の標準化されたt分布が用いられたのである。帰無仮説はこのように理論的に特定できる比較点をもとに置かれているのであって，帰無仮説下ではない現実の世界では，検定統計量は母平均の差\\(\\mu_1 - \\mu_2\\)に応じた分布から生じている。このように中心がずれているt分布のことを非心分布といい，ズレの程度が非心度パラメータである。t検定における非心度\\(\\lambda\\)は，以下の式で表される。\n\\[ \\lambda = \\frac{(\\mu_1-\\mu2) - \\mu_0}{\\sigma\\sqrt{n}} \\]\nこの非心度の分だけ，非心t分布はt分布からズレていることになる。Rではdt関数にncpパラメータがあり，デフォルトではncp=0になっていた。これを変えて描画してみよう。\n\n# データの準備\ndf &lt;- 10 # 自由度を指定\n# ggplotでプロット\nggplot(data.frame(x = c(-5, 5)), aes(x = x)) +\n  stat_function(fun = dt, args = list(df = df, ncp = 0), aes(color = \"ncp=0\")) +\n  stat_function(fun = dt, args = list(df = df, ncp = 3), aes(color = \"ncp=3\")) +\n  labs(\n    title = \"非心t分布\",\n    x = \"x\",\n    y = \"密度\",\n    color = \"ncp\"\n  )\n\n\n\n\n\n\n\n\nncp=0の時は，中心が0にある帰無仮説の世界であり，これを使ってタイプ1エラー，つまり\\(\\alpha\\)が算出されたのであった。ncpを効果量で表現すれば，母平均の差がゼロでない時の分布が描けるのだから，タイプ2エラー，つまり\\(\\beta\\)が計算できる。\n自由度df = 10，非心度ncp = 3の例で考えてみよう。 タイプ1エラーになるのは，自由度10のt分布で上2.5%の臨界値以上の実現値が得られた時である。\n\nqt(0.975, df = 10, ncp = 0)\n\n[1] 2.228139\n\n\nこのとき，実際はncp = 3ほどずれていたのだから，タイプ2エラーが生じる確率は次のとおりである。\n\nqt(0.975, df = 10, ncp = 0) %&gt;% pt(df = 10, ncp = 3)\n\n[1] 0.2285998\n\n\n当然，ncp = 0から離れるほどタイプ2エラーは生じにくくなる。非心度は母効果量\\(\\delta = \\frac{(\\mu_1 - \\mu_2)-\\mu_0}{\\delta}\\)を使って，\\(\\lambda = \\delta \\sqrt{n}\\)で表すことができる。\nこれを使って，t検定のサンプルサイズを設計してみよう。話を簡単にするために，サンプルサイズは2群で等しいものとする。\n検定統計量の式を思い出して，\\(\\sqrt{n}\\)にあたるところは2群のサンプルサイズから計算される，プールされた標本サイズから得られることに注意しよう5。\n\nalpha &lt;- 0.05\nbeta &lt;- 0.2\ndelta &lt;- 0.5\n\nfor (n in 10:1000) {\n  df &lt;- n + n - 2\n  lambda &lt;- delta * (sqrt((n * n) / (n + n)))\n  cv &lt;- qt(p = 1 - alpha / 2, df = df) # Type1errorの臨界値\n  er &lt;- pt(cv, df = df, ncp = lambda) # Type2errorの確率\n  if (er &lt;= beta) {\n    break\n  }\n}\nprint(n)\n\n[1] 64\n\n\nここでは，サンプルサイズを10から徐々に増やしていき，1000までの間で目標とする\\(\\beta\\)まで抑えられたところで，カウントしていくforループをbreakで脱出する，というかたちで組んでいる。結果的に，各群64名，合計128名のサンプルがあれば，目標が達成できることがわかる。サンプルサイズが2群で異なる場合など，詳細は@kosugi2023 に詳しい。\n\n\n10.2.2 シミュレーションによるサンプルサイズ設計\n非心F分布を使えば分散分析でもサンプルサイズができるし，そのほかの検定についても同様に非心分布を活用すると良い。しかし，非心分布の理解や非心度の計算など，ケースバイケースで学ぶべきことは多い。\nそこで，電子計算機の演算力をたのみに，データ生成のシミュレーションを通じて設計していくことを考えてみよう。サンプルサイズや効果量を定めれば，仮想データを作ることができるし，それ対して検定をかけることもできる。仮想データの生成と検定を反復し，タイプ2エラーがどの程度生じるかを相対度数で近似することもできるだろう。であれば，その近似をサンプルサイズを徐々に変えることで繰り返してサンプルサイズを定めることもできる。\n以下は，母相関が\\(\\rho = 0.5\\)とした時に，検出力が80%になるために必要なサンプルサイズを求めるシミュレーションコードである。\n\nlibrary(MASS)\nset.seed(12345)\nalpha &lt;- 0.05\nbeta &lt;- 0.2\nrho &lt;- 0.5\nsd &lt;- 1\nSigma &lt;- matrix(NA, ncol = 2, nrow = 2)\nSigma[1, 1] &lt;- Sigma[2, 2] &lt;- sd^2\nSigma[1, 2] &lt;- Sigma[2, 1] &lt;- sd * sd * rho\n\niter &lt;- 1000\n\nfor (n in seq(from = 10, to = 1000, by = 1)) {\n  FLG &lt;- rep(0, iter)\n  for (i in 1:iter) {\n    X &lt;- mvrnorm(n, c(0, 0), Sigma)\n    cor_test &lt;- cor.test(X[, 1], X[, 2])\n    FLG[i] &lt;- ifelse(cor_test$p.value &gt; alpha, 1, 0)\n  }\n  t2error &lt;- mean(FLG)\n  print(paste(\"n=\", n, \"のとき，betaは\", t2error, \"です。\"))\n  if (t2error &lt;= beta) {\n    break\n  }\n}\n\n[1] \"n= 10 のとき，betaは 0.681 です。\"\n[1] \"n= 11 のとき，betaは 0.639 です。\"\n[1] \"n= 12 のとき，betaは 0.612 です。\"\n[1] \"n= 13 のとき，betaは 0.566 です。\"\n[1] \"n= 14 のとき，betaは 0.563 です。\"\n[1] \"n= 15 のとき，betaは 0.471 です。\"\n[1] \"n= 16 のとき，betaは 0.462 です。\"\n[1] \"n= 17 のとき，betaは 0.419 です。\"\n[1] \"n= 18 のとき，betaは 0.402 です。\"\n[1] \"n= 19 のとき，betaは 0.385 です。\"\n[1] \"n= 20 のとき，betaは 0.353 です。\"\n[1] \"n= 21 のとき，betaは 0.344 です。\"\n[1] \"n= 22 のとき，betaは 0.312 です。\"\n[1] \"n= 23 のとき，betaは 0.285 です。\"\n[1] \"n= 24 のとき，betaは 0.256 です。\"\n[1] \"n= 25 のとき，betaは 0.265 です。\"\n[1] \"n= 26 のとき，betaは 0.21 です。\"\n[1] \"n= 27 のとき，betaは 0.227 です。\"\n[1] \"n= 28 のとき，betaは 0.176 です。\"\n\nprint(n)\n\n[1] 28\n\n\nここではシミュレーション回数1000，上限1000，刻み幅を1にしているが，状況に応じて変更すると良い。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#課題",
    "href": "chapter10.html#課題",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "10.3 課題",
    "text": "10.3 課題\n\n一要因3水準のBetweenデザインの分散分析において、1.分散分析で有意差が見られる場合と、2.任意の2水準の組み合わせのどこかで有意差が見られる場合を考えたとき、タイプ2エラーはどのように異なるかをシミュレーションで確かめてみましょう。設定として、n1=n2=n3=10、標準偏差も各群等しくsigma = 1とし、効果量delta = 2でモデル化してみましょう。\nN増し問題は相関係数の検定の時も生じるでしょうか。母相関が\\(\\rho = 0.3\\)のとき、サンプルサイズを10から始めて、有意になるまでデータを追加する仮想研究を1000回行ってみましょう。データ追加の上限は100、有意水準は\\(\\alpha = 0.05\\)として、最終的に有意になる割合を計算してみましょう。\n\\(\\alpha = 0.05, \\beta = 0.2\\)とし、効果量\\(\\delta = 1\\)とした時の対応のないt検定のサンプルサイズ設計をしたいです。1.非心t分布を使った解析的な方法と、2.シミュレーションによる近似的な方法の両方で、同等の結果が出ることを確認しましょう。\n\n\n\n\n\nクルシュケJ. K. (2014) 2017. ベイズ統計モデリング: R, JAGS, Stanによるチュートリアル 原著第2版. Translated by 前田和寛 and 小杉考司. 共立出版.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#footnotes",
    "href": "chapter10.html#footnotes",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "ANOVA君は結果をコンソールに直接出力し，戻り値を持たない。ここでは結果の\\(p\\)値が必要だったので，このようにした。↩︎\n表(1)が出るか，裏(0)が出るか，という2値の結果変数だけを持つ施行のことで，この確率変数がベルヌーイ分布にし違う。ベルヌーイ分布は，表が出る確率\\(\\theta\\)をパラメータに持つ。\\(P(X=k) = \\theta^k(1-\\theta)^{1-k},\\text{ただし}k=\\{0,1\\}\\)という確率変数である。1/0というのが生死，男女，成功失敗などさまざまなメタファに適用できるので応用範囲が広い。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nこの「最低限検出したい効果」のことをSmallest Effect Size of Interest, SESOIと呼ぶ。小杉, 紀ノ定, and 清水 (2023) も参照。↩︎\nt統計量の実現値の式にある分母，\\(\\sqrt{U_p^2/\\frac{n_1n_2}{n_1+n_2}}\\)に見られる，プールされた不偏分散を割るための標本サイズであり，2群の母分散が等しいと仮定して計算するなら，\\(\\sigma^2(\\frac{1}{n_1} + \\frac{1}{n_2}) = \\sigma^2(\\frac{n_1+n_2}{n_1n_2}) = \\sigma^2 / \\frac{n_1n_2}{n_1 + n_2}\\)から得られる。↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter11.html",
    "href": "chapter11.html",
    "title": "11  重回帰分析の基礎",
    "section": "",
    "text": "11.1 回帰分析の基礎\nここでは回帰分析を扱う。説明変数\\(x\\)と被説明変数\\(y\\)の関数関係\\(y=f(x)\\)に，次の一次式を当てはめるのが単回帰分析である。\n\\[ y_i  = \\beta_0 + \\beta_1 x_i + e_i = \\hat{y}_i + e_i\\]\n一次式を\\(\\hat{y}\\)とまとめたものを予測値といい，予測値と実測値\\(y\\)の差分\\(e_i\\)を残差residualsという。\n空間上の一次直線の切片，傾きを求めるというのが基本的な問題であり，二点であれば一意に定めることができるが，データ分析の場面では3点以上の多くのデータセットの中に直線を当てはめることになるので，なんらかの外的な基準が必要になる。この時，「残差の分散が最も小さくなるように」と考えるのが最小二乗法の考え方であり，「残差が正規分布に従っていると考え，その尤度が最も大きくなるように」と考えるのが最尤法の考え方である。前者は記述統計的な，後者は確率モデルとしての感が過多になっていることに注意してほしい。また確率モデルの推定方法としては，事前分布を用いたベイズ推定が用いられることもある。\n最小二乗法による推定値は，次の式で表される。証明は他書(小杉 2018; 西内 2017)に譲るが，ロジックとして残差の二乗和\\(\\sum e_i^2 = \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2\\)を最小にすることを考え，この式を展開するか偏微分を用いて極小値を求めることで算出できるとだけ伝えておこう。いずれにせよ，平均値\\(\\bar{x},\\bar{y}\\)や分散・共分散\\(s_x,s_y,r_{xy}\\)など標本統計量から推定できるのはありがたいことである。\n\\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x},\\quad \\beta_1 = r_{xy} \\frac{s_y}{s_x}\\]\nまた，ここでは\\(x,y\\)ともに連続変数を想定しているが，説明変数\\(x\\)が二値，あるいはカテゴリカルなものであれば\\(y\\)の平均値を通る直線を探すことになる。直線の傾きが0であれば「平均値が同じ」という線形モデルであり，これは平均値差の検定における帰無仮説と同等である。このように，t検定やANOVAは回帰分析の特殊ケースとも考えられ，まとめて一般線形モデルと呼ばれる。一般線形モデルは，被説明変数が連続的で，線形モデルによる平均値に正規分布に従う残差が加わったものとして考えるという意味で統一的に表現される。\nANOVAの場合は，二つ以上の要因による効果を考えることもあった。交互作用項を考慮しなければ，2要因のモデルは次のように表現することができる。\n\\[ y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + e_i \\]\nこのように説明変数が複数ある回帰分析を特に重回帰分析Multiple Regression Analysisと呼ぶ。一次式なので，ある変数に限れば線形性が担保されているから，これも線形モデルの仲間である。重回帰分析を用いる場合は，説明変数同士を比較して「どちらの説明変数の方が影響力が大きいか」ということが論じられることが多いが，係数は当然\\(x_n, y\\)の単位に依存するため，素点の回帰係数は使い勝手が悪い。そこですべての変数を標準化した標準化係数が用いられることが多い。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#回帰分析の特徴",
    "href": "chapter11.html#回帰分析の特徴",
    "title": "11  重回帰分析の基礎",
    "section": "11.2 回帰分析の特徴",
    "text": "11.2 回帰分析の特徴\n以下，具体的なデータを用いて回帰分析の特徴を見てみよう\n\n11.2.1 パラメータリカバリ\n回帰分析のモデル式にそってデータを生成し，分析によってパラメータリカバリを行ってみよう。\n説明変数については制約がないので一様乱数から生成し，平均0，標準偏差\\(\\sigma\\)の誤差とともに被説明変数を作り，\n\nlibrary(tidyverse)\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データの生成\nx &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x + e\n\ndat &lt;- data.frame(x, y)\n# データの確認\nhead(dat)\n\n          x          y\n1 -4.248450 -11.120952\n2  5.766103  18.736432\n3 -1.820462  -3.805302\n4  7.660348  25.071541\n5  8.809346  30.026546\n6 -9.088870 -25.355175\n\ndat %&gt;% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y~x\") # 線形モデルの描画\n\n\n\n\n\n\n\n\nこのデータに基づいて回帰分析を実行した結果が以下のとおりである。\n\nresult.lm &lt;- lm(y ~ x, data = dat)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82796 -0.61831  0.03553  0.69367  2.68062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.021928   0.045010   44.92   &lt;2e-16 ***\nx           3.002194   0.007919  379.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.006 on 498 degrees of freedom\nMultiple R-squared:  0.9965,    Adjusted R-squared:  0.9965 \nF-statistic: 1.437e+05 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\nここでは\\(\\beta_0 =2, \\beta_1=3, \\sigma = 1\\)と設定しており，ほぼ理論通りの係数がリカバリーできていることを出力から確認しておこう。 もちろんリカバリの精度は，データの線形性の強さに依存するから，残差の分散が大きかったりサンプルサイズが小さくなると，必ずしもうまくリカバリできないことがあることは想像に難くないだろう。\n\n\n11.2.2 残差の正規性と相関関係\nlm関数が返した結果オブジェクトには，表示されていない多くの情報が含まれている。例えば予測値や残差も含まれているので，これを使って回帰分析の特徴を見てみよう。\n\ndat &lt;- bind_cols(dat, yhat = result.lm$fitted.values, residuals = result.lm$residuals)\nsummary(dat)\n\n       x                  y                yhat            residuals       \n Min.   :-9.99069   Min.   :-28.216   Min.   :-27.9721   Min.   :-2.82796  \n 1st Qu.:-5.08007   1st Qu.:-13.074   1st Qu.:-13.2294   1st Qu.:-0.61831  \n Median :-0.46887   Median :  0.301   Median :  0.6143   Median : 0.03553  \n Mean   :-0.09433   Mean   :  1.739   Mean   :  1.7387   Mean   : 0.00000  \n 3rd Qu.: 4.65795   3rd Qu.: 15.963   3rd Qu.: 16.0060   3rd Qu.: 0.69367  \n Max.   : 9.98809   Max.   : 32.638   Max.   : 32.0081   Max.   : 2.68062  \n\n\n予測値\\(\\hat{y}\\)はfitted.valuesとして保存されている。これの平均値が被説明変数\\(y\\)の平均値に一致していることが確認できる。回帰分析は説明変数\\(x\\)を伸ばしたり(\\(\\beta_1\\)倍する)ズラしたり(\\(\\beta_0\\)を加える)しながら，被説明変数\\(y\\)に当てはめるのであり，位置合わせがなされた予測値の中心が被説明変数の中心と一致することは理解しやすいだろう1。\n次に，残差の平均が0になっていることも確認しておこう。これが0でない\\(c\\)であれば，回帰係数が常に\\(c\\)だけズレていることになるので，そのような系統的ズレは最適な線形の当てはめにおいて除外されているべきだからである2。\nまた，回帰分析において残差は正規分布に従うことが仮定されていた。これを検証するにはQ-Qプロットを見ると良い。\n\ndat %&gt;%\n  ggplot(aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\n\nQ-Qプロットとは2つの確率分布を比較するためのグラフであり，横軸には理論的分布の分位点が，縦軸に実データが並ぶもので，右上がりの直線上にデータが載っていれば分布に従っている，と判断するものである。直線から逸脱している点は理論的分布からの逸脱と考えられる。今回の結果はほとんどが正規分布の直線上にあることから，大きな逸脱がないことが認められる。\nデータ生成メカニズムによっては，被説明変数が二値的であったり，順序的であったり，カウント変数であったり，と正規分布がそぐわないものもあるだろう。そのようなデータに無理やり回帰分析を当てはめることは適切ではない。いかなる時もデータは可視化して，モデルを当てはめることの適切さをチェックすることを忘れたはならない。\nちなみに出力結果を直接plot関数に入れてもよい。ここから残差と予測値の相関関係や，Q-Qプロット，標準化残差のスケールロケーションプロット，レバレッジと標準化残差3 などがプロットされる。\n\nplot(result.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差と予測値のプロットからも想像できるが，両者の相関はゼロである。図で確認しておこう。\n\nlibrary(GGally) # 必要ならインストールしよう\nggpairs(dat)\n\n\n\n\n\n\n\n\nこの関係から明らかなように，残差は説明変数や予測値と相関しない4。説明変数と残差に相関関係があるとすると，説明変数でまだ説明できていない分散が残っていることになるし，予測値と残差に相関がないことは予測値が高いか低いかにかかわらず，残差が一様に分布していることを意味する。このことを踏まえて，重回帰分析の特徴を理解していこう。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#重回帰分析の特徴",
    "href": "chapter11.html#重回帰分析の特徴",
    "title": "11  重回帰分析の基礎",
    "section": "11.3 重回帰分析の特徴",
    "text": "11.3 重回帰分析の特徴\n重回帰分析においては，回帰係数は偏回帰係数partial regression coefficientsと呼ばれる。この「偏」の一文字が意味することを考えていこう。\n\n11.3.1 回帰係数と偏回帰係数\n単回帰分析の回帰係数は，説明変数\\(x\\)が一単位上昇した時の被説明変数の変化量，と解釈すればよい。これに対して重回帰分析の偏回帰係数を，「説明変数\\(x_1\\)が一単位上昇した時の被説明変数の変化量」とすることはできない。というのも，説明変数が複数(\\(x_2,x_3,\\ldots\\))あり，他の説明変数の次元についての変化を考慮していない変化量になっているからである。\n重回帰分析において，説明変数が完全に無相関で直交しているのであれば，\\(x_1\\)の変化と\\(x_2\\)の変化を独立して説明できるが，往々にしてそのようなことはない。偏回帰係数は当該変数以外の変動を統制した回帰係数である。\n上で単回帰係数において，説明変数と残差が相関しないことを確認した。言い換えれば，説明変数で説明でき分散は全て説明し尽くされており，残差は説明変数で説明できない被説明変数の分散，つまり説明変数の影響を除外した被説明変数の分散と考えることができる(被説明変数の分散=説明変数が説明する分散+残差の分散)。\nここで第二の変数\\(x_2\\)があったとする。第一の変数\\(x_1\\)で\\(y\\)を説明した残差\\(e_y\\)と，第一の変数で第二の変数を説明した残差\\(e_{x2}\\)との相関を偏相関partial correlationという。これは第一の変数\\(x_1\\)からの影響を両者から取り除いているので，\\(x_1\\)で統制した相関係数ということができる。偏相関は単純な相関が「見せかけの関係」でないことを検証するための重要な指標である。\n偏相関係数を計算してみよう。\n\nlibrary(MASS)\nlibrary(psych)\nSigma &lt;- matrix(c(1, 0.3, 0.5, 0.3, 1, 0.8, 0.5, 0.8, 1), ncol = 3)\nX &lt;- mvrnorm(1000, c(0, 0, 0), Sigma, empirical = TRUE) %&gt;% as.data.frame()\n## 相関行列\ncor(X)\n\n    V1  V2  V3\nV1 1.0 0.3 0.5\nV2 0.3 1.0 0.8\nV3 0.5 0.8 1.0\n\n## 回帰分析をして残差を求める\nresult.lm1 &lt;- lm(V2 ~ V1, data = X)\nresult.lm2 &lt;- lm(V3 ~ V1, data = X)\ncor(result.lm1$residuals, result.lm2$residuals)\n\n[1] 0.7867958\n\n## 偏相関を求めるR関数で確認\npsych::partial.r(X)[2, 3]\n\n[1] 0.7867958\n\n\n最後はpsychパッケージの偏相関行列を求める関数で検証した。確かに残差同士の相関係数が偏相関係数になっていることが確認できたと思う。\nそして，ここでは残差同士の相関係数として算出しているが，残差をつかった回帰分析の係数が偏回帰係数になるのである。このデータセットの第一変数を従属変数にした重回帰分析の結果から，これを確認してみよう。\n\nresult.mra &lt;- lm(V1 ~ V2 + V3, data = X)\n# 回帰係数を取り出す\nresult.mra$coefficients\n\n  (Intercept)            V2            V3 \n 1.171322e-17 -2.777778e-01  7.222222e-01 \n\n# 残差をつかって偏回帰係数を確認する\nresult.lm3 &lt;- lm(V1 ~ V3, data = X)\nresult.lm4 &lt;- lm(V2 ~ V3, data = X)\nresult.lm5 &lt;- lm(result.lm3$residuals ~ result.lm4$residuals)\n#\nresult.lm5$coefficients\n\n         (Intercept) result.lm4$residuals \n        5.381952e-18        -2.777778e-01 \n\n\n重回帰分析の結果result.mraのV2からV1への回帰係数は-0.2778である。また，V3でV1,V2を統制した残差同士をつかい，回帰係数を求めた結果は-0.2778と，同じ値になっていることが確認できただろう。\nV3からV1への偏回帰係数も同様で，V2で両者を統制した残差同士による回帰係数になっている。このように，重回帰分析の回帰係数は，他の説明変数で統制した値になっており，日本語で説明するなら「他の変数の値が同じであると想定した条件つきの，当該変数の影響力」とでもいうべき値になっている。\nなぜこのような持って回った説明をするかというと，つい「条件付きの」という話を忘れて報告，解釈してしまうことが多いからで，吉田 and 村井 (2021) の論文での指摘は議論を呼んだのは記憶に新しい5。 たとえば今回の例でも，回帰係数が-0.2778であったのに対し，V1とV2の単相関が0.3 であったことを思い出そう。符号が反転しているため，解釈は真逆になってしまう。実際の単相関は正の関係であるから，条件付きであることを忘れて「V2は負の影響，V3は正の影響」と表現してしまうと，ミスリーディングなことになるからである。\nまた，豊田 (2017) は重回帰分析のこうした誤用を避けるために，独立変数を事前に直交化したデザインで行うコンジョイント分析の積極的な利用を提案している。我々が重回帰分析をうまく使いこなせないのであれば，そうした手法も有用であるだろう。\n\n\n11.3.2 多重共線性\n偏回帰係数の解釈が難しい理由の一つは，説明変数同士に相関関係がみられることにある。 特に，説明変数間の相関関係が高くなることは多重共線性Multicollinearityの問題という。この問題は，回帰係数の標準誤差がインフレを起こすことを指す。\n例えば先ほどの例で，説明変数V2とV3は相関係数\\(0.8\\)を持っていた。この時の回帰係数の標準誤差を確認しておこう。\n\nsummary(result.mra)\n\n\nCall:\nlm(formula = V1 ~ V2 + V3, data = X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.59118 -0.54717 -0.03692  0.55044  2.90735 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.171e-17  2.690e-02   0.000        1    \nV2          -2.778e-01  4.486e-02  -6.192 8.65e-10 ***\nV3           7.222e-01  4.486e-02  16.100  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8507 on 997 degrees of freedom\nMultiple R-squared:  0.2778,    Adjusted R-squared:  0.2763 \nF-statistic: 191.7 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n標準誤差は0.0449であり，それほど大きくない標準誤差で問題がないようである。しかし両者の相関係数がより高くなり，一方が他方に線形的に従属してしまうと係数の推定値が不安定になるため，注意が必要である。\nこのインフレを確認するための指標がVariance Infration Factor: VIFである。Rではcarパッケージにあるvif関数に重回帰モデルを入れることでこの指標が算出される。一般にVIFが3，あるいは10を超えると多重共線性が生じており，解釈に注意が必要と言われている6。\n\nlibrary(car) # なければ入れておこう\nvif(result.mra)\n\n      V2       V3 \n2.777778 2.777778 \n\n\n幸い，今回の値はこれらの基準を下回っていたので許容範囲内である。\n\n\n11.3.3 変数の投入順序\n重回帰分析の場合は複数の説明変数があるが，これを投入するときに全ての変数を同時に投入するか，順番をつけて投入するかといった手法の違いがある。前者を強制投入法と呼ぶこともある。 順番をつけて投入する方法は，逐次投入と呼ばれる。この場合は，適合度指標などを参考に変数を追加あるいは削除して，適合度が統計的に有意に向上するかどうかを考えながら進めていく。\n重回帰係数の予測値\\(\\hat{y}\\)と，被説明変数\\(y\\)の相関係数\\(R_{y\\hat{y}}\\)は重相関係数と呼ばれ，予測がうまくいっているかどうかを表す適合度の一つである。相関係数なので\\(-1\\)から\\(+1\\)までの値を取りうるが，\\(-1\\)は逆に完全に合致していることになるので，この相関係数の符号は大して情報を持たない。そこでこれを二乗した\\(R_{y\\hat{y}}^2\\)を考える。これは決定係数とも呼ばれ，説明変数の分散のうち予測値の分散が占める割合を表している。7\n説明変数の逐次投入は，説明変数を持たないヌルモデルから一つずつ追加していくForward Selection，全ての変数を投入してから一つずつ減らしていくBackward Selectionがある。Forwardのほうは追加することによって\\(R^2\\)が有意に増加するか，Backwardのほうは削除しても有意に\\(R^2\\)が減らないか，を確認しながら進めることになる。この方法は手元のデータに最も適した説明変数のペアを選出できる方法ではあるが，検定を繰り返していることの問題と，手元のデータ以外に一般化する時の根拠の乏しさから，用いられないこともある。\n逐次投入法には別の観点からの手法もある。それが階層的回帰分析である。この手法は，重回帰分析における交互作用項の投入を検討する文脈で発展した。重回帰分析では，説明変数同士の相関がない，もしくは小さい方が望ましい。しかし，交互作用とは分散分析における組み合わせの効果を表すものであり，実験デザインによっては交互作用効果が重要な変動であることも少なくない。回帰分析と分散分析は，一般線形モデルという形で統一的に理解されるが，回帰分析でも連続的に変化する組み合わせの効果を考えることができる。交互作用があるということは説明変数間に相関があることを意味するため，回帰分析の大前提に抵触する可能性があり，その投入には慎重を期する必要がある。\nこうした文脈から，まずは要因の効果を投入し，次に交互作用項を投入してモデル適合度の有意な改善がみられるかどうかを検証する手順が推奨されている。この逐次投入法を特に階層的回帰分析と呼ぶ。ここでの「階層」とは，手順が重要度順に進められていることを意味し，データの特徴に関するものではないことに注意が必要である8。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#係数の標準誤差と検定",
    "href": "chapter11.html#係数の標準誤差と検定",
    "title": "11  重回帰分析の基礎",
    "section": "11.4 係数の標準誤差と検定",
    "text": "11.4 係数の標準誤差と検定\n\n11.4.1 係数の検定\nサンプルが母集団から得られた確率変数であるのだから，(偏)回帰係数もまた確率変数である。すなわち，サンプルが変わるごとに変化し，その揺らぎがある確率分布に従うと考えられる。これを確認するためには，データ生成過程をモデリングし，反復することで近似させて理解するのがいいだろう。\n\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データ生成関数\ndataMake &lt;- function(n, beta0, beta1, sigma) {\n  x &lt;- runif(n, -10, 10)\n  e &lt;- rnorm(n, 0, sigma)\n  y &lt;- beta0 + beta1 * x + e\n  dat &lt;- data.frame(x, y)\n  return(dat)\n}\n\n# 結果オブジェクトの準備\niter &lt;- 2000\nbeta0.est &lt;- rep(NA, iter)\nbeta1.est &lt;- rep(NA, iter)\n# simulation\nfor (i in 1:iter) {\n  sample &lt;- dataMake(n, beta0, beta1, sigma)\n  result.lm &lt;- lm(y ~ x, data = sample)\n  beta0.est[i] &lt;- result.lm$coefficients[1]\n  beta1.est[i] &lt;- result.lm$coefficients[2]\n}\n\ndata.frame(x = beta0.est) %&gt;% ggplot(aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\ndata.frame(x = beta1.est) %&gt;% ggplot(aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n図から明らかなように，回帰係数も確率的に分布する。ただしその平均は理論値に近似している。\n\nmean(beta0.est)\n\n[1] 1.999257\n\nmean(beta1.est)\n\n[1] 2.999798\n\n\nこの分布の幅が回帰係数の標準誤差である。\n\nsd(beta0.est)\n\n[1] 0.04580387\n\nsd(beta1.est)\n\n[1] 0.007659277\n\n\n回帰係数はt分布に従い，その自由度はサンプルサイズからモデルで用いる係数の数を引いたものになる。先ほどのヒストグラムを基準化し，理論分布を重ねて描画してみることで確認しておこう。\n\ndata.frame(x = beta1.est) %&gt;%\n  scale() %&gt;%\n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 100) +\n  stat_function(fun = function(x) dt(x, df = n - 2), color = \"red\", linewidth = 2)\n\n\n\n\n\n\n\n\nこのt分布を用いて，係数が0の母集団から得られたサンプルなのかどうかの検定が行われる。\n\n\n11.4.2 モデル適合度の検定\n一方で，出力の最後にはF統計量による検定も行われていたことを確認しておこう。次に示すのは重回帰分析の例である。\n\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 0\nbeta2 &lt;- 0\nsigma &lt;- 1\nx1 &lt;- runif(n, -10, 10)\nx2 &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + e\nsample &lt;- data.frame(y,x1,x2)\nresult.lm &lt;- lm(y ~ x1 + x2, data = sample)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = sample)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85235 -0.68275 -0.01436  0.67809  2.70488 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.996999   0.045263  44.120   &lt;2e-16 ***\nx1          -0.006453   0.007970  -0.810    0.418    \nx2          -0.003928   0.007795  -0.504    0.615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.012 on 497 degrees of freedom\nMultiple R-squared:  0.001893,  Adjusted R-squared:  -0.002124 \nF-statistic: 0.4713 on 2 and 497 DF,  p-value: 0.6245\n\n\n上の例では，統計量\\(F\\)が，自由度\\(F(\\) 2,497 \\()\\)のもとで，0.4713であり，統計的に有意ではないと判断される(p=0.6245,n.s.)。\nこれは重相関係数に対する検定であり，母集団においてモデル全体としての説明力が0である，という帰無仮説を検証しているものである。この有意性検定には，説明変数の数\\(p\\)，サンプルサイズ\\(n\\)，重相関係数\\(R^2\\)を用いて，以下の式で用いられる検定統計量Fを利用する(南風原 2014)。\n\\[ F= \\frac{R^2}{1-R^2}\\cdot\\frac{n-p-1}{p} \\]\nここで右辺第一項目はCohenの効果量(\\(f^2=\\frac{R^2}{1-R^2}\\)) といわれ，サンプルサイズ設計においてはこの指標が利用される。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#サンプルサイズ設計",
    "href": "chapter11.html#サンプルサイズ設計",
    "title": "11  重回帰分析の基礎",
    "section": "11.5 サンプルサイズ設計",
    "text": "11.5 サンプルサイズ設計\n重回帰分析のサンプルサイズ設計は，変数の効果の大きさ(回帰係数)が事前にわかっているのであれば，nを徐々に増やしていくシミュレーションによって行える。しかしそのようなケースは稀であり，実際には\\(R^2\\)の検定を用いて，ある効果量と検出力の下で，正しく検出できるサイズを算出することになる。\nサンプルサイズの算出には，非心F分布を用いる。この時の非心度は，効果量\\(f^2\\)に\\(n\\)をかけたものになる。これを使ってサンプルサイズ設計をする例は以下のとおりである。\n\nf2 &lt;- 0.15      # 効果量\nalpha &lt;- 0.05   # タイプ1エラー率\nbeta &lt;- 0.2     # タイプ2エラー率\np &lt;- 5         # 説明変数の数\n\nfor(n in 10 : 500){\n    lambda &lt;- f2 * n\n    df1 &lt;- p\n    df2 &lt;- n - p - 1\n    cv &lt;- qf(p = 1- alpha, df1, df2)\n    t2error &lt;- pf(q = cv, df1, df2, ncp  = lambda)\n    if(t2error &lt; beta){\n        break\n    }\n}\n\nprint(n)\n\n[1] 92\n\n\nこの設定では，n = 92以上であればモデルとして影響力がないとは言えない，ということがわかる。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#まとめ",
    "href": "chapter11.html#まとめ",
    "title": "11  重回帰分析の基礎",
    "section": "11.6 まとめ",
    "text": "11.6 まとめ\n重回帰分析は，人文社会科学で多用される技術ではあるが，技術が先行して理解が伴わないまま利用されているケースも少なくない。繰り返しになる点もあるが，以下に注意点をまとめておく。\n\n偏回帰係数の意味；重回帰分析における偏回帰係数は，ほかの変数を統制した上での値であり，あたかも各係数が独立直交しているかのように解釈するのは適切ではない。\n誤差の正規性；誤差は正規分布に従っているという仮定があり，二値データや整数しかとらないカウントデータなどに盲目的にモデルを適用してはならない。誤差の正規性が満たされているかどうかは，分析後にQ-Qプロットを用いて確認する。\n誤差の均質性；誤差はモデル全体にわたって同じ正規分布に従っているというのもモデルの仮定である。すなわち，独立変数に応じて誤差分散が変わるといった均質でないデータの場合は，正しく推定されない。誤差の均質性については，分析後のQ-Qプロットを用いて確認する。\n誤差間の独立性；誤差はモデル全体にわたって同じ正規分布から独立に生成されている(i.i.d)というのがモデルの仮定である。時系列データのように，誤差間に対応(自己回帰)がみられるデータの場合は回帰分析は適切な手法とならない。状態空間モデルなど，誤差間関係を適切にモデリングしたものを当てはめる必要がある。\nモデルの適切な定式化；モデルには被説明変数に影響を与えるすべての変数が正しく含まれている必要がある。例えば，影響を与えることがわかっている変数\\(X_o\\)を意図的に除外して分析をしたとする。そのモデルに含まれる変数\\(X_a\\)が被説明変数\\(y\\)に影響を与えていたとしても，\\(X_a\\)と\\(X_o\\)に相関があれば，\\(X_o\\)の影響力が\\(X_a\\)を通じて\\(y\\)に伝播するkから，\\(X_a\\)の影響力が課題に評価されることになる。自らの仮説のために，意図的に変数を選択するのはQRPsに該当する。\n説明変数間の相関関係；説明変数のうちにあまりにも相関関係が高い変数ペアがあれば，多重共線性の疑いが生じる。多重共線性は推定値の不安定さとなって現れる。このような場合は，説明変数を主成分分析で合成変数にまとめるといった対応が考えられる。また，高い相関ではないが交互作用効果が見たいといった場合は，逐次投入など慎重に個々の影響を考えながら投入するようにする(階層的重回帰分析)。なお交互作用項は，各変数の平均からの偏差をかけ合わせたものにすることが一般的である。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#課題",
    "href": "chapter11.html#課題",
    "title": "11  重回帰分析の基礎",
    "section": "11.7 課題",
    "text": "11.7 課題\n\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。画面には一部しか表示しておらず，全体(\\(n=100\\))はこちらex_regression1.csvからダウンロード可能です。このデータセットを用いて重回帰分析を行い，結果を出力してください。\n\n\n\n           y        x1         x2\n1  1.8685595 -4.248450  1.9997792\n2 -0.5728781  5.766103 -3.3435292\n3  1.0321850 -1.820462 -0.2277393\n4 10.0468488  7.660348  9.0894765\n5 -1.1968078  8.809346 -0.3419521\n6  9.6719213 -9.088870  7.8070044\n\n\n\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。画面には一部しか表示しておらず，全体(\\(n=300\\))はこちらex_regression2.csvからダウンロード可能です。このデータセットを用いて重回帰分析を行ってください。結果のプロットから，上に挙げた重回帰分析の仮定に反しているところを指摘してください。\n\n\n\n          y         x1           x2\n1  3.586304 -0.4248450  0.132767341\n2  8.599252  0.5766103  0.922713561\n3  2.397115 -0.1820462  0.053684622\n4  3.505236  0.7660348  0.007801881\n5  6.517720  0.8809346  0.633076091\n6 -1.394231 -0.9088870 -0.895346802\n\n\n\n\\(R^2=0.3\\)を目標として，説明変数の数\\(p=10\\)の重回帰分析を行う際に，必要なサンプルサイズはいくつになるか，計算してみましょう。ここで，\\(\\alpha = 0.05,\\beta=0.2\\)とします。\n\n\n\n\n\n南風原朝和. 2014. 心理統計学の基礎: 続・統合的理解のために. 有斐閣.\n\n\n吉田寿夫, and 村井潤一郎. 2021. “心理学的研究における重回帰分析の適用に関わる諸問題.” 心理学研究 92 (3): 178–87. https://doi.org/10.4992/jjpsy.92.19226.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房. http://ci.nii.ac.jp/ncid/BB27527420.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.\n\n\n西内啓. 2017. 統計学が最強の学問である[数学編]: データ分析と機械学習のための新しい教科書. ダイヤモンド社.\n\n\n豊田秀樹. 2017. もうひとつの重回帰分析. 東京図書.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#footnotes",
    "href": "chapter11.html#footnotes",
    "title": "11  重回帰分析の基礎",
    "section": "",
    "text": "もちろん証明できる。\\(\\beta_0 = \\bar{y} - \\beta_1\\bar{x},\\beta_1 = r_{xy} \\frac{s_y}{s_x}\\)より，\\(\\bar{\\bar{y}} = \\frac{1}{n}\\sum(\\bar{y} - \\beta_1\\bar{x} + \\beta_1x_i) = \\bar{y} - \\beta_1\\bar{x} + \\beta_1\\frac{1}{n}\\sum x_i = \\bar{y}\\)である。↩︎\nもちろん証明できる。\\(\\bar{e} = \\frac{1}{n}\\sum e_i = \\frac{1}{n} \\sum (y_i - \\hat{y}_i) = \\bar{y} = \\bar{\\hat{y}} = 0\\)である。↩︎\n縦軸の標準化された残差の大きな値は解釈に注意が必要な外れ値である可能性が高い。レバレッジも同様に回帰係数に大きな影響を与える値の指標であり，この図の端に位置する変数は注意が必要，と考える。↩︎\n論文が早期公開された後，心理学会が主催するオンラインシンポジウムでは著者とこの論文で取り上げられた論文の著者が登場して，議論が交わされた(日本心理学会YouTubeライブ・話題の論文について著者と語るシリーズ,2021年7月2日20時-21時40分)。平日の夜という設定，早期公開版における議論であったにも関わらず，1700名近い視聴者が参加した。↩︎\n論文が早期公開された後，心理学会が主催するオンラインシンポジウムでは著者とこの論文で取り上げられた論文の著者が登場して，議論が交わされた(日本心理学会YouTubeライブ・話題の論文について著者と語るシリーズ,2021年7月2日20時-21時40分)。平日の夜という設定，早期公開版における議論であったにも関わらず，1700名近い視聴者が参加した。↩︎\n2変数重回帰分析モデルで，VIFが3であれば説明変数間の相関は\\(r=0.81\\)程度である。VIFが10であれば\\(r=0.97\\)にもなる。詳しくは 小杉, 紀ノ定, and 清水 (2023) を参照。↩︎\nもちろん証明できる。小杉 (2018) を参照。↩︎\nこれに対して，データの階層性(ex.学級\\(\\subset\\)市区町村\\(\\subset\\)都道府県)を考慮する線形モデルのことを，階層線形モデルHierarichal Linear Model:HLM という。↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter12.html",
    "href": "chapter12.html",
    "title": "12  線型モデルの展開",
    "section": "",
    "text": "12.1 一般線型モデル",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter12.html#一般化線型モデル",
    "href": "chapter12.html#一般化線型モデル",
    "title": "12  線型モデルの展開",
    "section": "12.2 一般化線型モデル",
    "text": "12.2 一般化線型モデル",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter12.html#階層線型モデル",
    "href": "chapter12.html#階層線型モデル",
    "title": "12  線型モデルの展開",
    "section": "12.3 階層線型モデル",
    "text": "12.3 階層線型モデル",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html",
    "href": "chapter13.html",
    "title": "13  多変量解析の入り口",
    "section": "",
    "text": "13.1 因子分析",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>多変量解析の入り口</span>"
    ]
  },
  {
    "objectID": "chapter13.html#構造方程式モデリング",
    "href": "chapter13.html#構造方程式モデリング",
    "title": "13  多変量解析の入り口",
    "section": "13.2 構造方程式モデリング",
    "text": "13.2 構造方程式モデリング",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>多変量解析の入り口</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bernaards, Coen A., and Robert I. Jennrich. 2005. “Gradient\nProjection Algorithms and Software for Arbitrary Rotation Criteria in\nFactor Analysis.” Educational and Psychological\nMeasurement 65: 676–96. https://doi.org/10.1177/0013164404272507.\n\n\nGabry, Jonah, Rok Češnovar, and Andrew Johnson. 2023. Cmdstanr: R\nInterface to ’CmdStan’.\n\n\nHadley, Wickham. 2014. “Tidy Data.” Journal of\nStatistical Software 59: 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nRevelle, William. 2021. Psych: Procedures for Psychological,\nPsychometric, and Personality Research. Evanston, Illinois:\nNorthwestern University. https://CRAN.R-project.org/package=psych.\n\n\nRosseel, Yves. 2012. “lavaan: An\nR Package for Structural Equation Modeling.”\nJournal of Statistical Software 48 (2): 1–36. https://doi.org/10.18637/jss.v048.i02.\n\n\nStevens, Stanley Smith. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nZeileis, Achim. 2005. “CRAN Task Views.” R\nNews 5 (1): 39–40. https://CRAN.R-project.org/doc/Rnews/.\n\n\nキーラン・ヒーリー. (2018) 2021.\nデータ分析のためのデータ可視化入門. Translated by 瓜生真也,\n江口哲史, and 三村喬生. 講談社.\n\n\nクルシュケJ. K. (2014) 2017. ベイズ統計モデリング: R, JAGS,\nStanによるチュートリアル 原著第2版. Translated by 前田和寛 and\n小杉考司. 共立出版.\n\n\nシ. 2016. 計算機言語のまとめノート. 暗黒通信団.\n\n\nランダー，J.P. (2017) 2018. みんなのr 第2版. Translated by\n高柳慎一, 津田真樹, 牧山幸史, 松村杏子, and 簑田高志. マイナビ出版.\n\n\n佐藤坦. 1994. はじめての確率論: 測度から確率へ. 共立出版.\n\n\n南風原朝和. 2014. 心理統計学の基礎: 続・統合的理解のために.\n有斐閣.\n\n\n吉田伸生. 2021. 確率の基礎から統計へ. 新装版. 日本評論社.\n\n\n吉田寿夫, and 村井潤一郎. 2021.\n“心理学的研究における重回帰分析の適用に関わる諸問題.”\n心理学研究 92 (3): 178–87. https://doi.org/10.4992/jjpsy.92.19226.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房.\nhttp://ci.nii.ac.jp/ncid/BB27527420.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023.\n数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計.\n技術評論社.\n\n\n平岡和幸, and 堀玄. 2009. プログラミングのための確率統計.\nオーム社. http://amazon.co.jp/o/ASIN/4274067750/.\n\n\n松村優哉, 湯谷啓明, 紀ノ定保礼, and 前田和寛. 2021. 改訂2版\nRユーザのためのRStudio[実践]入門:\nTidyverseによるモダンな分析フローの世界. 技術評論社.\n\n\n株式会社ホクソエム, trans. (2016) 2017. Rプログラミング本格入門:\n達人データサイエンティストへの道. 単行本. 共立出版.\n\n\n永田靖, and 吉田道弘. 1997. 統計的多重比較法の基礎.\nサイエンティスト社.\n\n\n池田功毅, and 平石界. 2016.\n“心理学における再現可能性危機：問題の構造と解決策.”\n心理学評論 59 (1): 3–14. https://doi.org/10.24602/sjpr.59.1_3.\n\n\n河野敬雄. 1999. 確率概論. 京都大学学術出版会.\n\n\n石田基広, 市川太祐, 高柳慎一, and 福島真太朗, trans. (2015) 2016.\nR言語徹底解説. 共立出版.\n\n\n総務省. 2020.\n“統計表における機械判別可能なデータ作成に関する表記方法.”\n統計企画会議申し合わせ. https://www.soumu.go.jp/main_content/000723697.pdf.\n\n\n西内啓. 2017. 統計学が最強の学問である[数学編]:\nデータ分析と機械学習のための新しい教科書. ダイヤモンド社.\n\n\n豊田秀樹. 2009. 検定力分析入門: Rで学ぶ最新データ解析ー.\n東京図書.\n\n\n———. 2017. もうひとつの重回帰分析. 東京図書.\n\n\n高橋康介. 2018. 再現可能性のすゝめ. Edited by 石田基広. Vol. 3.\nWonderful r. 共立出版.",
    "crumbs": [
      "References"
    ]
  }
]