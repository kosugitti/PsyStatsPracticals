[
  {
    "objectID": "chapter16.html",
    "href": "chapter16.html",
    "title": "16  ベイジアンモデリング",
    "section": "",
    "text": "16.1 ベイジアンモデリングの学習方法\nここまでは定型的な統計モデルをいろいろ紹介してきた。定型的といったのは，モデルの形や求めるパラメータの数，その解釈の仕方が決まっていて，データの種類や型に合えば適用できるモデルという意味である。これに対してベイジアンモデリングは，データに合う形のモデルを形作る＝モデリングするということであり，その推定方法としてベイズ法をつかうというものである。推定法は必ずしもベイズ法である必要はなく，最尤法でも最小二乗法でも良いのだが，これらの手法による推定は推定手順も自らで開発しなければならない。これに対し，すでにみた確率的プログラミング言語によるベイズ推定は，確率モデルさえ記述できれば推定結果が得られる。これにより，研究者は自らのデータとその背景に合ったモデルを考えて記述するだけでよく，テクニカルな推定手順を考える必要がなくなる。確率的プログラミング言語は，その言葉にあるようにプログラミングの知識を必要とするが，逆に言えばこの知識・技能さえ習得しておけば，あとは分析者のアイデア次第で、自身のオリジナルな分析ができる。\n以下ではStanによるプログラミングと，その特徴的な利用例についてみていくが，その前にベイジアンモデリングを学ぶ上での指針を示しておく。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ベイジアンモデリング</span>"
    ]
  },
  {
    "objectID": "chapter16.html#ベイジアンモデリングの学習方法",
    "href": "chapter16.html#ベイジアンモデリングの学習方法",
    "title": "16  ベイジアンモデリング",
    "section": "",
    "text": "16.1.1 学習のステップ1;確率的プログラミング言語Stanの学習\nここではRを使った統計分析を扱ってきたので，改めてプログラミングとは，という話をする必要はないだろう。ただ，確率的プログラミング言語としてここで取り上げるStanは，Rよりもやや上級者向けの，C++と呼ばれる言語に基づいたものである。初学者にとって大きな違いは，「インタプリタ型とコンパイル型」，および「型宣言」の2点だろう。\n\n16.1.1.1 インタプリタ型とコンパイル型\nRはインタプリタ型言語と呼ばれる。個人的には「一問一答型」と呼んでいる。コマンドプロンプト&gt;が表示されている時，Rは入力を待って聞き耳を立てているのであった。ここに計算式や命令文を入れると，結果を計算して返す。つまり，問いに対して答えが返ってくる，という形式の繰り返しである。\nこれに対してコンパイル型言語というのがある。C言語やJava，Python，そしてStanはこの形である。すなわち，命令文全体をまず書いて，その文書(スクリプトファイル)全体を機械語に翻訳する。この作業をコンパイルという。コンパイルされたものを実行すると，その文書の内容が実行される。ここで命令文に誤りがある場合，1.コンパイルできないというエラーが表示される，2.コンパイルはできるが，実行時にエラーが表示される，という2つのケースがある。エラーは大抵，XX行目がおかしい，という形で表示される。インタプリタ型であれば，書いて実行した行でエラーだと言われるので気づきやすいが，コンパイル型は一旦書き切ってからでないとエラーかどうかわからないので1，不便に感じるかもしれない。\nコンパイル型の利点は，一旦機械語に翻訳し，計算機は計算機自身の母語(機械語)で計算をするので，計算速度が速いという点にある。この利点のために必要なこととして理解して欲しい。また，コンパイルは専用のツールを使い，そのツールによってコンパイルされたものは，そのツールの環境でしか動かないという制約がある。Windowsの場合はRtools，Macの場合はcommand line toolsを導入する必要がある。これらは計算機のより根源的なところにアクセスする。一般的なアプリケーションを使うのとは違い，むしろMCMCサンプリングを行うアプリケーションを作るようなものだから，ウィルス対策ソフトがその実行を妨げるようなことがある。環境の構築はすでに済んでいるものとして話を進めるが，その準備に一苦労する可能性があることは覚えておくと良い。困ったことがあれば，自身で検索するなどして対応する必要があるだろう。\nさて，Stanを使った分析では，Rファイルとは別に命令文全体をStanの言語で書いたStanファイルを準備することになる。このファイルをRの命令文で「Stanを使ってコンパイルせよ」と指示する。コンパイルが終われば，これまたR側から，「そのコンパイルされたオブジェクトを使ってMCMCサンプリングをせよ」と指示する。計算結果はRのオブジェクトとして環境に保存されるから，あとはRによるデータハンドリングの作業になってくる。StanファイルもRファイルもRStudioのエディタ機能を利用すれば良いが，両者を混ぜるようなことのないよう，この仕組みを理解して進めてほしい。\n\n\n16.1.1.2 型宣言\n聞きなれない言葉かもしれないが，型宣言とは，変数の型を宣言することである。例えば，int x;というコードがあったとき，intは整数型を表している。整数型は整数であり，xに代入可能なのは1.0(実数)でも1+0i(複素数)でもなく，1(整数)である。\nこのように，変数を使う前にその変数がどの型なのかを宣言することを型宣言という。このような型宣言は，コンパイル型言語では必須である。このように宣言しておくことで，本来整数しか入らないところに実数を入れてしまう，といったエラーが生じないように工夫されている。Rでは変数を事前に宣言する必要がなく，ただx &lt;- 1と書き始めると，xが整数であれ実数であれ，自由に扱うことができた。このことに慣れていると，事前に宣言しなければならないことが非常に不便に思えるかもしれないが，型宣言をすることで言語の堅牢性を高めているという利点がある。\nStanはこの型宣言をブロックごとに行う必要がある。ブロックとは，中括弧{}で囲われる領域のことであり，次の6つのブロックがある。\n\ndataブロック\ntransformed dataブロック\nparametersブロック\ntransformed parametersブロック\nmodelブロック\ngenerated quantitiesブロック\n\nもっともよく使われるのは1.dataブロックと，3.parametersブロック，5.modelブロックである。dataブロックはStan外部とのやりとり，すなわちStanが外部から受け取るデータを宣言，記述するところである。ここで型が異なるデータが与えられるとエラーになる。すなわち，Stanの側でint x;と宣言してあるのに対し，R側からx &lt;- 1.2のような実数が与えられると，実行時にエラーになる。このように，型宣言をすることで，エラーを防ぐものであると理解してほしい。\nparametersブロックは推定したいパラメータを宣言するものであり，ここで宣言されたパラメータについて，Stanはサンプリング結果を返すことになる。modelブロックは確率モデルを記述するところ(尤度関数を記述するところ)であるので，もっとも重要なブロックであると言えるだろう。\nそのほかのブロックは捕捉的なものであり，必ずしも使う必要があるわけではない。transformed dataブロックは，dataブロックで宣言されたデータを変換するところであり，transformed parametersブロックは，parametersブロックで宣言されたパラメータを変換するところである。なぜそのような変換をするかといえば，内部で以後の計算をやりやすくするためである。例えば複数のパラメータを組み合わせて，確率分布に与える場合は一旦返還しておいた方が可読性が高い。具体例として回帰分析のことを考えると，パラメータは切片\\(\\beta_0\\)と傾き\\(\\beta_1\\)であり，これが説明変数\\(x_i\\)と組み合わさって予測値\\(\\hat{y}_i\\)を作るのであった。パラメータブロックには\\(\\beta_0\\)と\\(\\beta_1\\)を宣言するが，transformed parametersブロックでyhatを宣言して\n\\[ yhat = \\beta_0 + \\beta_1 x\\]\nとかいておくと，モデルブロックではyhatを使って記述できる。このように，あるパラメータがほかのパラメータの組み合わせで作られる場合などは，一旦その置き換えられる形を書いておいた方がわかりやすだろう。\ngenerated quantitiesブロックは，サンプリングされた値を加工して使う場合に用いる。サンプリングされたものの加工は，結果を受け取ったRの側でも可能なので，このブロックは必ずしも必要ではない。しかし，サンプリングが終わった時に自分に必要な加工された値も(コンパイルして高速で)計算しておいてくれると便利である。他にも色々な用途があるので，このブロックに関しては続く実践例のところでみていこう。\n\n\n16.1.1.3 そのほかの細かな違い\nあとは，行の終わりにセミコロンをつける必要があるとか，コメントを書くときに//を使うとか，そういった細かなところが違うだけである。\nプログラミングの基本は思った通りに動くのではなく，書いた通りに動くことである。もし思い通りにいかず，エラーが表示されれば，それもあなたが書いたコードに原因がある2。そのためエラーがでたら恐れ慄くのではなく，解決のためのヒントが表示されたぐらいに理解すれば良い。問題点を一つ一つ解決していけば，必ず望むところに到達できるはずである。最近は生成AIが発達しているので，エラーメッセージを丸ごと生成AIに与えて，どこにどのような問題があるかを聞くという方法があるので，それを利用すると良い。\n\n\n\n16.1.2 学習のステップ2;これまでの分析方法を書き直してみる\nここまで様々な統計モデルを見てきた。ベイジアンモデリングの学習のステップ2は，これまでの分析方法をStanに書き直してみることである。\n例えば回帰分析を，重回帰分析を，階層線形モデルをStanの言葉で書いたらどうなるだろうか。もちろんbrmsパッケージを使うとこうした苦労は必要ないのだが，改めて自分で既知のモデルを描いてみると，どのようなモデルがどのように記述されるかがわかるだろう。\nこの時のポイントは，分析に際してデータ生成メカニズムという視点を持つことである。我々はつい，データがあってそれに合う分析方法を探す，という発想になってしまう。あるいは分析方法のバリエーションがないばあい，分析方法に合うようなデータを取る，という考え方になってしまう。これはおかしなことだとは思わないだろうか。自分の購入した統計ソフトが回帰分析しかできないので，離散変数は諦めて研究計画を練り直そう，というのは大変貧しい話である。\n本来，自然な人間の振る舞いや反応の仕方を数値におとして，そこから意味を読み取ることが統計学であり，予算や環境の問題で人間の振る舞いの方を変えさせるというのはおかしいのである。なるべくデータは生のままで，これをどのように分析するかを考えるべきである。その時，「このデータはどのようなメカニズムで生まれてきたのか」という視点からアプローチする。ここでのメカニズムは確率分布と言い換えても良いかもしれない。すなわち，個々の反応は確定した一つの値しか取らないわけがないので，その反応のあり得るほかの値をかんがえ，その総覧を確率分布として表現するのである。その上で，その確率分布が持つパラメータが，どのような仕組みを持っているかを数式で記述する。\n回帰分析は，個々の値\\(y_i\\)が，本来取りうる値\\(\\hat{y}_i\\)に誤差\\(e_i\\)がついて生じた，と考えている。この誤差は正規分布に従うから，\\(y_i \\sim N(\\hat{y}_i, \\sigma^2)\\)と記述される。この\\(\\hat{y}_i\\)は説明変数\\(x_i\\)の線形結合で表現されるから，\\(y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\)とする，といった具合である。\n回帰分析がこのようなメカニズムであったように，t検定や分散分析なども同様に記述することができる。こうした既存のモデルを改めて記述すると，これまで意識していなかったモデルの性質が見えてくる。例えば，確率分布として何を仮定していたのか，パラメータの制約として何をおいていたのか，事前分布として何を考えていたのか，といったことが，Stanの言語で逐一記述することでわかるようになる。これが，既存のモデルをStanの言語で書き直すことによる学習の利点である。\n\n様々なモデルを試してみる\n\n既存のモデルが確率的プログラミング言語で表現できることがわかれば，つぎは確率的プログラミング言語でないと表現できないことに目を向けてみよう。\n例えばt検定や分散分析は「平均値の差の検定」である。ここで行われていたことは，正規分布に従うデータの，平均値の差があるかどうか，全ての群間に差がないと言って間違える可能性はどれぐらいあるか，ということであった。データ生成メカニズムの観点から見ると，この手法はごく限定的な一部分しか見ていなかったことに気づく。\n平均値以外のパラメータを考えることはできないのだろうか。特定の群間の差だけを考えることはできないのだろうか。差があるかないかだけではなく，どれぐらい差があるのかとか，一方が他方より大きい確率はどれぐらいか，といったことを考えることはできないのだろうか。\nこれらの疑問に対して，ベイジアンモデリングは答えを与える。実験計画法によって得られたデータであっても，これまで以上に多角的な視点，様々な仮説を持って考えることができる。\nもちろん実験計画法による要因効果の特定だけがベイジアンモデリングではない。正規分布ではないデータに対しても，特定の離散的な区別をしていないデータに対しても，データ生成の観点からモデルを組み込んでいくことができる。以下ではこうした例をいくつか見ていくが，これらを見ることで統計分析の視点が一変することを感じてほしい。統計分析は，与えられたデータに既存の分析を制約の中で考えるのではなく，データの生成メカニズムをクリエイティブに考える楽しい営みなのである。\n\n限界について知っておこう\n\nベイジアンモデリングの自由さ，創造性に目覚めてしばらくすると，その限界に気づくこともあるだろう。まずは楽しんでいってほしいというところだが，先にどのような壁に直面しがちなのか，みておこう。\n一つはモデル評価の問題である。例えば帰無仮説検定の場合，これは評価・判断をするための技術であるから，「設定した有意水準を下回る\\(p\\)値を得れば差があると言って良い」といった評価基準が明確であった。これに対して，ベイジアンモデリングを行うと，こうした「YesかNoか」といった答えは出しにくい。帰無仮説と対立仮説というモデル，あるいは自分が開発したモデルが既存のモデルに比べて，良いのか悪いのかといった判断基準をどう持てば良いのか。\nこれについての答えは明確で，ベイズファクター(Bayes Factor)をみよ，というのがそれである。ベイズ的モデル評価はこのBFに一元化できるといっても過言ではない。BFはモデルとデータの当てはまりの良さを，モデル同士の相対比較で表現するものであるから，対立仮説よりも帰無仮説のほうが良い，という結論を出すこともできる。ただし，この「当てはまりのよさ」(周辺対数尤度)を計算するプロセスが少し複雑で，またモデルによっては解析的に計算できず推定するしかないこともある3。この点については，今後の計算機科学の発展が望まれる。\nt検定や分散分析など，定型的なモデルについてはBFを自動的に算出してくれるパッケージやアプリケーションがある。JASP(JASP Team 2025)はその代表的なもので，GUIを備えた統計ソフトウェアでありながら，既存の分析結果と同時にベイズ推定の結果も出力し，BFも自動的に計算してくれる。\nただし，BFも「3.0より大きければ優っていると判断して良い」という数値基準もあるが，こうした「YesかNoか！」という二値判断が，過大な解釈を許したり基準を超えるための不正を生んだりしてきたという歴史を鑑みると，使い方には注意が必要である。またBFは事前分布の置き方によっては同じモデルでも大きく値を変えることが知られており，客観的な事前分布の置き方については様々な議論がある。\nBFを離れてモデルを評価するのであれば，得られた事後分布やパラメータを見て色々判断するしかないだろう。Kruschke (2018) は事前に判断するパラメータの領域を宣言しておく方法を考えているし，豊田 (2020) は事後分布の関数の形で判断する方法を提示したりしている。これらは帰無仮説検定に対する代案として提示されているものである。今後どのような形で議論が進むのか，まだ確定していないというのが現状である。\nモデルの評価はBFでできるとして，次に初学者が直面する問題は，「自分でモデルを作るのが難しい」というものである。以下に続く様々なモデルはどれも魅力的であるが，自分では思いつかないよ，と思って挫けそうになるという相談をよく耳にすることがある。これについては特効薬があるわけではないが，そもそもゼロから全てのモデルを作り上げよう，とするのが大きすぎる野望のように思われる。まずは色々なモデルを知って，このモデルを自分のデータにはこのように応用できそうだ，と想像力を働かせるところから進めよう。あるいは非常に限定的な，小さなおもちゃのようなモデル(Toyモデル)を作って，それを徐々に発展させていくことで大きなモデルに育てる，という観点を持つことである。浜田 (2018) や 浜田 (2020) を読むと，このステップの重要さがよくわかるだろう。\nそもそも，線形モデルでも十分なシーンというのも結構あるものである。ベイジアンモデリングで自分似合ったデータをカスタマイズする，と豪語しておいた後で言うのもおかしいが，はっきりした傾向があるのであれば線形モデルで大体うまくいく。線形モデルはピッタリとは言わないが大体当てはまっていて理解できるモデルであり，モデリングは細かな違いに当てはめていこうとする「作り込み」の技術であるから，実践的にはそこまで必要のないことも少なくない。もちろん線形モデルといってもただの単回帰分析で良い，といってるのではなく，データの生成メカニズムにあった一般化線形モデル，混合モデルなど工夫できるところは色々あるのだから。\nこうした問題やぶつかりそうな壁があると知ってもなお，ベイジアンモデリングはおすすめできる。この自由で創造的な世界を知らずして，統計分析が苦手だと思ってしまうのは非常にもったいないからである。以下の用例で，ベイジアンモデリングの様々な可能性を味わっていただきたい。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ベイジアンモデリング</span>"
    ]
  },
  {
    "objectID": "chapter16.html#正規分布を使ったモデル",
    "href": "chapter16.html#正規分布を使ったモデル",
    "title": "16  ベイジアンモデリング",
    "section": "16.2 正規分布を使ったモデル",
    "text": "16.2 正規分布を使ったモデル\nまずはこれまでもよく用いられてきた，正規分布を使ったモデルについて見てみよう。\n\n16.2.1 分散を推定する\n正規分布を使ったモデルといえば，一般線形モデルのような平均値に関するモデルがほとんどである。正規分布は位置パラメータ\\(\\mu\\)と，スケールパラメータすなわち幅のパラメータ\\(\\sigma\\)でその形状が定まるが4，後者はブレの大きさ，誤差の大きさに関するものと考えられるから，そうした指標としてモデルを考えてみよう。\nカバーストーリーとして次のようなシーンを考える5。 ある牛丼チェーン店では，並盛り一人前につき150gの肉を載せて提供すること，という決まりになっている。ここである店舗で社員による抜き打ち検査があり，提供係10名に牛丼を作らせ，その肉の量を計測した。10名のうち2名はまだ日の浅いアルバイトであることがわかっている。計測した肉の量は以下の通りであった。\n\ny &lt;- c(151, 149, 152, 150, 151, 148, 151, 150, 221, 245)\n\nここから次のようなモデルを考えよう。全員平均\\(\\mu=150\\)のつもりで提供しているが，誤差が個人ごとに異なるものとする。これを数式で次のように表現する。\n\\[ y_i \\sim N(\\mu, \\sigma_i) \\]\n添字を注意深く見るとわかるが，個人を識別する\\(i\\)がデータ\\(y_i\\)と標準偏差\\(\\sigma_i\\)についていて，平均値\\(\\mu\\)にはついていない。つまり平均値は全員で共通していると仮定し，そこからのブレが個人ごとに違うというモデルになっている。\nこれがデータ\\(y_i\\)を生成するデータ生成メカニズムである。ここでの未知数\\(\\mu,\\sigma_i\\)をデータから推測するために，StanによるMCMC法を用いる。Stanのコードは次のようになる。このコードを，例えばgyudon10.stanとでも名前をつけて保存しておこう。\ndata{\n    int N;\n    array[N] real Y;\n}\nparameters{\n    real mu;\n    array[N] real&lt;lower=0&gt; sigma;\n}\nmodel{\n    // likelihood\n    for(i in 1:N){\n        Y[i] ~ normal(mu, sigma[i]);\n    }\n    // prior\n    mu ~ uniform(0, 200);\n    sigma ~ cauchy(0,5);\n}\nコードがdataとparameters，modelの3つのブロックに分かれていることに注意してほしい。また，各ブロックでintやarrayなどの型宣言があることに注意してほしい。\nまずはdataブロックを見よう。intは整数型で，まずデータのサイズを外部から入力するようにしている。これで，例えばデータが7件とか50件と変わった時でも同じコードが使えるようにしている。またarrayは配列の型宣言であり，同じ変数名で複数の値が入るようになっている。ここではN人分のデータを扱うためにarray[N] real Y;というように宣言している。\n続くparamtersブロックでは，知りたい未知数の\\(\\mu\\)と\\(\\sigma_i\\)をそれぞれreal mu;とarray[N] real&lt;lower=0&gt; sigma;としている。realは実数型，arrayはすでに述べたように配列で，\\(\\sigma_i\\)の\\(i\\)によって異なる\\(\\sigma\\)である様を表現している。&lt;lower=0&gt;としているのは変数に対する制約で，この\\(\\sigma_i\\)は下限が\\(0\\)，すなわち正の数しかとらないことにしている。分散は負になることがないので，こうしておくとStanがMCMCサンプリングにおいて可能な値の候補を探す領域が適切に制限されることになる。\n最後のmodelブロックは，確率モデルを記述する。ベイズ的確率モデルは尤度と事前分布が必要で，まず尤度を記述している。 Y[i] ~ normal(mu, sigma[i])のところがそれで，\\(i\\)がfor文によって繰り返されている。数式で言えば，次の計算と同じである。\n\\[ \\prod_{i=1}^{N} N(Y_i | \\mu, \\sigma_i) \\]\n実際の計算は対数尤度をとって，次の計算を行っているが，Stanでは「データが次の確率分布に従う」という形で書けるので，確率分布を使ったモデルさえかければ誰でも推定ができる。\n\\[ \\sum _{i=1}^{N} \\log\\{N(Y_i | \\mu, \\sigma_i)\\} \\]\nまた今回はそれぞれの事前分布として，\\(\\mu\\)に\\(0\\)から\\(200\\)までの一様分布を，\\(\\sigma\\)にコーシー分布を置いた。どのような事前分布をおくかは自由だし，特段指定がなければStanは十分にひろい一様分布を自動的に設定する。分散(標準偏差)パラメータの事前分布には裾の重い分布をおくことが一般的で，コーシー分布やStudentのt分布，指数分布などがよく用いられる。\nさてこのコードを，次のRコードから呼び出して実行する。手順は(必要なライブラリを読み込んだ上で)，stanファイルをコンパイルし，できたオブジェクトにサンプリングの設定を与えて出力する，というものである。\nサンプリングに際してはデータを外部から与える必要があるため，dataSetオブジェクトを作って渡すようにした。与えるデータはリスト型であり，stanファイル側のdataブロックと同じ変数名をつける必要がある。サンプリングのその他の設定は次のとおりである。\n\ndata…データを与える\nchains…MCMCチェインを走らせる数\nparallel_chains…MCMCチェインのうち，並列で走らせるチェインの数。実行環境のCPUが持っているコア数-1程度にするのが良い。\niter_warmup…サンプリングに入る前の調整期間。詳しくは@sec-mcmc-evaluation を参照\niter_sampling…サンプリングの数。詳しくは@sec-mcmc-evaluation を参照\nshow_message…メッセージの出力を抑制。必ずしも設定しなくても良い。\nreflesh…サンプリングの途中経過をどの程度の頻度で出力するかの設定。必ずしも設定しなくとも良い。\n\n\npacman::p_load(cmdstanr)\nmodel &lt;- cmdstan_model(\"gyudon10.stan\")\ndataSet &lt;- list(\n    N = length(y),\n    Y = y\n)\nresult &lt;- model$sample(\n    data = dataSet,\n    chains = 4,\n    parallel_chains = 4,\n    iter_warmup = 2000,\n    iter_sampling = 5000,\n    show_messages = FALSE,\n    refresh = 0\n)\n\nWarning: 2138 of 20000 (11.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\nprint(result)\n\n variable   mean median    sd  mad     q5    q95 rhat ess_bulk ess_tail\n lp__     -18.60 -18.21  2.29 2.15 -22.94 -15.58 1.00     2296     2354\n mu       150.59 150.78  0.66 0.51 149.43 151.44 1.00     2550     4070\n sigma[1]   2.91   1.48  5.96 1.65   0.11   9.91 1.00      754      346\n sigma[2]   4.92   2.93 10.16 2.25   0.74  13.75 1.00     3087     3725\n sigma[3]   4.18   2.61  6.39 2.06   0.70  11.79 1.00     1874     4869\n sigma[4]   3.20   1.75  7.31 1.62   0.21   9.67 1.00     2627     2302\n sigma[5]   2.96   1.40 10.09 1.58   0.08   9.48 1.01      632      234\n sigma[6]   5.95   3.88 10.00 2.67   1.32  16.04 1.00     2734     5426\n sigma[7]   2.80   1.42  6.86 1.58   0.08   9.27 1.00      742      314\n sigma[8]   3.36   1.91  8.41 1.80   0.22   9.98 1.00     2688     2434\n\n # showing 10 of 12 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\n結果を見ると，\\(\\mu\\)は平均して150程度であり，店舗マニュアルに沿った結果が出ているといえよう。注目すべきは個々人の誤差であり，可視化してみるとその特徴がわかりやすい。\n\npacman::p_load(bayesplot)\ndraws &lt;- result$draws()\n# sigmaパラメータのみを可視化\nbayesplot::mcmc_areas(draws, regex_pars = \"sigma\")\n\n\n\n\n\n\n\n\nこれをみると明らかなように，最後の2人が大きな値であり，明らかに未熟であることがわかる。 数値で出力するために，bayestestRパッケージの力も借りてみよう。\n\npacman::p_load(bayestestR)\n# sigmaパラメータのみを選択\nsigma_draws &lt;- draws[, , grepl(\"sigma\", dimnames(draws)$variable)]\n# EAP(平均), MAP(最頻値), 中央値, 95%HDI\nbayestestR::describe_posterior(sigma_draws, centrality = c(\"mean\", \"median\", \"MAP\"), \n                              ci = 0.95, ci_method = \"hdi\", test = NULL)\n\nSummary of Posterior Distribution\n\nParameter | Median |   Mean |   MAP |          95% CI\n-----------------------------------------------------\nsigma[1]  |   1.48 |   2.91 |  0.27 | [ 0.02,   9.91]\nsigma[2]  |   2.93 |   4.92 |  1.36 | [ 0.05,  13.75]\nsigma[3]  |   2.61 |   4.18 |  1.14 | [ 0.04,  11.80]\nsigma[4]  |   1.75 |   3.20 |  0.93 | [ 0.03,   9.67]\nsigma[5]  |   1.40 |   2.96 |  1.04 | [ 0.02,   9.48]\nsigma[6]  |   3.88 |   5.95 |  1.86 | [ 0.29,  16.26]\nsigma[7]  |   1.42 |   2.80 |  0.58 | [ 0.02,   9.27]\nsigma[8]  |   1.91 |   3.36 |  0.62 | [ 0.04,   9.99]\nsigma[9]  |  60.03 |  87.28 | 40.32 | [18.24, 210.92]\nsigma[10] |  80.14 | 114.08 | 58.77 | [26.19, 280.65]\n\n\nここにあるように，結果は確率分布の形で得られるので，平均値(Mean, EAP)でみるのか，中央値(Median)でみるのか，密度が最大になるところ(MAP)でみるのかによって大きく値が異なる。特に分散パラメータは左右対称ではなく歪んだ分布になるので，EAP推定値は適切ではない。 また，確信区間は今回HDIを指定した。HDIはHighest Density Intervalsの略で，最高密度を含む95%の領域を指す。いわゆるパーセンタイルのように，上下2.5%を除外した領域を取る方法はETI(Equal-Tailed Intervals)と呼ばれるが，これだと歪んだ分布に対してやや偏った結果になることがある。詳しくは クルシュケ ([2014] 2017) や Makowski, Ben-Shachar, and Lüdecke (2019) ，あるいは単にbayestestRのサイトを参照して欲しい。\nさて，ともあれこのように幅のパラメータを推定するモデルを描くことができた。今回はたった10件の数字でもモデリングができること，平均パラメータ以外もモデリングの対象になることを確認してもらいたい。心理学において，特に反復測定を行う場合の個人の分散は，その人の持っている精度・誤差の大きさを表すと言える。今回のようにこの幅が，熟練度と解釈できるようなシーンであれば，立派に解釈可能なパラメータである。例えばこの\\(\\sigma_i\\)に，経験日数\\(z_i\\)をつかって\\(\\sigma_i = \\beta_0 + \\beta_1 z_i\\)のようなモデリングをすれば，どの程度経験によって誤差がなくなっていくかといった意味のある考察もできるだろう。このように，心理学において考察できるパラメータは平均だけではなく，我々は自由にその発想で考えることができるということを味わってもらいたい。\n\n\n16.2.2 欠測のあるデータを有効に使う\n続いては相関係数のモデリングを見てみよう。 例えば大学の入試の成績と，入学後の成績の間にはそれほど高い相関が見られない，という現象がある。 これは大学の入試が適切に学力を測定していない，という問題ではなく，いわゆる「選抜効果」とよばれるものだ。大学に入学できているのは一定のスコアを超えた者だけなので，得られたデータが本来の「入試の成績」の情報を含んでいないという問題である。\n具体的に数字で見てみよう。まずある程度の相関関係を持つデータを作り，その一部を欠損させてみよう。\n\npacman::p_load(MASS,tidyverse)\nN &lt;- 200\nmu &lt;- c(50,60)\nsd &lt;- c(10,10)\nrho &lt;- 0.7\nSig &lt;- matrix(nrow=2,ncol=2)\nSig[1,1] &lt;- sd[1]*sd[1]\nSig[1,2] &lt;- sd[1]*sd[2]*rho\nSig[2,1] &lt;- sd[2]*sd[1]*rho\nSig[2,2] &lt;- sd[2]*sd[2]\n# 乱数の発生\nset.seed(17)\nX &lt;- mvrnorm(N,mu,Sig,empirical=T)\n\ndat &lt;- data.frame(X)\ndat$FLG &lt;- factor(ifelse(dat$X1&gt;60,1,2),labels=c(\"pass\",\"fail\"))\n# 描画\ng &lt;- ggplot(dat,aes(x=X1,y=X2,group=FLG,color=FLG)) + geom_point()\ng\n\n\n\n\n\n\n\n\nここでは60点以上の者が入学したとして切断してみた。この場合の相関係数を，欠測値を除くuse=complete.obsのオプションをつけて確認しておく。\n\n# フルデータの場合\ncor(dat$X1, dat$X2)\n\n[1] 0.7\n\n# 欠測値に置き換える\ndat[dat$FLG==\"fail\",]$X2 &lt;- NA\n# 改めて相関係数を算出\ncor(dat$X1, dat$X2, use=\"complete.obs\")\n\n[1] 0.2493492\n\n\n不合格者が欠測値だとすると，相関係数は0.249にまで落ちてしまった。\nさてこれをベイジアンモデリングでできるだけ補正してみよう。コードは次のようなものになる。\ndata{\n    int&lt;lower=0&gt; Nobs;\n    int&lt;lower=0&gt; Nmiss;\n    vector[2] obsX[Nobs];\n    array[Nmiss] real missX;\n}\n\nparameters{\n    vector[2] mu;\n    real&lt;lower=0&gt; sd1;\n    real&lt;lower=0&gt; sd2;\n    real&lt;lower=-1,upper=1&gt; rho;\n}\n\ntransformed parameters{\n    cov_matrix[2] Sig;\n    Sig[1,1] = sd1 * sd1;\n    Sig[1,2] = sd1 * sd2 * rho;\n    Sig[2,1] = Sig[1,2];\n    Sig[2,2] = sd2 * sd2;\n}\n\nmodel{\n    //likelihood\n    obsX ~ multi_normal(mu, Sig);\n    missX ~ normal(mu[1], sd1);\n    //prior\n    mu[1] ~ uniform(0,100);\n    mu[2] ~ uniform(0,100);\n    sd1 ~ cauchy(0,5);\n    sd2 ~ cauchy(0,5);\n    rho ~ lkj_corr(1);\n}\nこのコードでは，まずデータブロックでNobsとNmissという二つの整数値をとっている。これは2つの変数が両方とも観測されたケースの数と，一方が欠測値であったケースの数である。StanではNAを直接扱うことができず，有効なデータの数だけ渡す必要があり，冒頭にその数を明示しておいた。\n次にvector型でobsX[Nobs]を宣言している。vector[2]とあるのは2つの要素を持つベクトルで1セットの変数であることを意味し，それがNobs個あることを表している。最後に，一方の変数が欠落していたデータも活用するために，Nmiss個の配列変数を用意した。こちらはベクトルではなく，サイズをしていした実数の配列の扱いである。\n今回推定したいのは相関係数rhoだが，これは2次元の多変量正規分布を想定した時の分散共分散行列の中に現れる。尤度のところにあるように，平均ベクトル\\(\\mathbb{\\mu}\\)と，分散共分散行列\\(\\mathbb{\\Sigma}\\)からデータは生成されている。\n\\[ obsX \\sim MN(\\mathbb{\\mu},\\mathbb{\\Sigma})\\]\nここで分散共分散行列の要素を紐解くと， \\[ \\mathbb{\\Sigma} = \\begin{pmatrix}\n\\sigma_1^2 & \\sigma_{12}\\\\\n\\sigma_{21} & \\sigma_2^2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho \\\\\n\\sigma_1 \\sigma_2 \\rho & \\sigma_2^2\n\\end{pmatrix}\\]\nとなる。ここで\\(\\sigma_1, \\sigma_2\\)はそれぞれの変数の標準偏差であり，\\(\\rho\\)は相関係数である。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ベイジアンモデリング</span>"
    ]
  },
  {
    "objectID": "chapter16.html#正規分布以外の分布を使う",
    "href": "chapter16.html#正規分布以外の分布を使う",
    "title": "16  ベイジアンモデリング",
    "section": "16.3 正規分布以外の分布を使う",
    "text": "16.3 正規分布以外の分布を使う\n\n16.3.1 項目反応理論\n\n\n16.3.2 再捕獲法による全体の推論",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ベイジアンモデリング</span>"
    ]
  },
  {
    "objectID": "chapter16.html#分布を混ぜる",
    "href": "chapter16.html#分布を混ぜる",
    "title": "16  ベイジアンモデリング",
    "section": "16.4 分布を混ぜる",
    "text": "16.4 分布を混ぜる\n\n16.4.1 分布を混ぜる(変化点を検出する)\n\n\n16.4.2 分布を混ぜる(0過剰ポアソン分布)\n\n\n\n\nJASP Team. 2025. “JASP (Version 0.95.1)[Computer software].” https://jasp-stats.org/.\n\n\nKruschke, John K. 2018. “Rejecting or Accepting Parameter Values in Bayesian Estimation.” Advances in Methods and Practices in Psychological Science 1 (2): 270–80.\n\n\nMakowski, Dominique, Mattan S. Ben-Shachar, and Daniel Lüdecke. 2019. “: Describing Effects and Their Uncertainty, Existence and Significance Within the Bayesian Framework.” Journal of Open Source Software 4 (40): 1541. https://doi.org/10.21105/joss.01541.\n\n\nクルシュケJ. K. (2014) 2017. ベイズ統計モデリング: R, JAGS, Stanによるチュートリアル 原著第2版. Translated by 前田和寛 and 小杉考司. 共立出版.\n\n\nリーM.D, and ワゲンメーカーズE-J. (2013) 2017. ベイズ統計で実践モデリング: 認知モデルのトレーニング. Translated by 井関龍太.\n\n\n浜田宏. 2018. その問題、数理モデルが解決します. ベレ出版. http://amazon.co.jp/o/ASIN/4860645685/.\n\n\n———. 2020. その問題、やっぱり数理モデルが解決します. ベレ出版.\n\n\n浜田宏, 石田淳, and 清水裕士. 2019. 社会科学のためのベイズ統計モデリング. 朝倉書店. http://amazon.co.jp/o/ASIN/4254128428/.\n\n\n豊田秀樹. 2020. 瀕死の統計学を救え！. 朝倉書店.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ベイジアンモデリング</span>"
    ]
  },
  {
    "objectID": "chapter16.html#footnotes",
    "href": "chapter16.html#footnotes",
    "title": "16  ベイジアンモデリング",
    "section": "",
    "text": "エディタがその言語に対応していたら，おかしな記述に下線が引かれるなど注意を促してくる機能もある。またRStudioでStan言語を書いていると，コンパイルの前に文法のチェックをする機能もある。↩︎\nとはいえ，コードに原因がない場合もある。それはStanを導入する際のシステム的なエラーであり，書かれた内容ではなく動かす環境全体の問題である。解決策としては，表示されるエラーを解読して問題を解決するか，環境を再構築する(Stanを再インストールする，最新バージョンに入れ替える等)必要がある。この場合も，生成AIが助力してくれるだろう。↩︎\n詳しくは(浜田, 石田, and 清水 2019)を参照してほしい↩︎\n正規分布の幅のパラメータは，テキストによっては分散\\(\\sigma^2\\)で記載されていることが多いが，ここでは標準偏差\\(\\sigma\\)で記述する。Stanでは標準偏差で記述するようになっているので，それに合わせたいからである。標準偏差を二乗したものが分散であるから，意味するところは本質的に変わりがない。↩︎\nこのカバーストーリーとモデルは リー and ワゲンメーカーズ ([2013] 2017) の「七人の科学者」P.48–49を参考に作ったものである。↩︎",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ベイジアンモデリング</span>"
    ]
  },
  {
    "objectID": "chapter07.html",
    "href": "chapter07.html",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "",
    "text": "7.1 帰無仮説検定の理屈と手続き\n帰無仮説検定は，心理学における統計の利用シーンの代表的なものだろう。 その手順は形式化されており，統計パッケージによってはデータの種類を指定するだけで自動的に結果の記述までしてくれるものもあるほどである。誰がやっても同じ結果になり，また，機械的に手続きを自動化できることは大きな利点ではある。欠点は，初学者がそのメカニズムを十分に理解せずに誤った結果を得たり，悪意のある利用者が自分に都合の良い数字を出させたりすることにある。科学的営みは悪意をもった実践者を想定しておらず，もしそのような悪例が露見した場合には事後的に摘発・対処するしかない。しかし残念なことながら，初学者の浅慮や意図せぬ悪用も多くみられる。\n心理学において，先行研究の結果が再現しないことを再現性問題というが，そのひとつは統計的手法の誤った使い方にあるとされる(池田 and 平石 2016)。改めて，丁寧に帰無仮説検定の手続きやロジックを見ていくことにしよう。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#帰無仮説検定の理屈と手続き",
    "href": "chapter07.html#帰無仮説検定の理屈と手続き",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "",
    "text": "7.1.1 帰無仮説検定の目的\n帰無仮説検定は，実験や調査で得たデータから得られた知見が意味のあるものかどうか，母集団の性質として一般化可能かどうかを判定するための枠組みである。手法と判断基準が明確なゲームの一種だと考えたよう。というのも，帰無仮説検定は有意水準という基準を設けて，帰無仮説と対立仮説という2つの考え方(モデル)を対決させ，勝敗を決するものだからである。勝敗を決するとしたのは，帰無仮説と対立仮説は排他的な関係にあるからであり，どちらも正しいとかどちらも間違っているという結末にはならないからである。ただし，あくまでも推測統計的なロジックに基づく判定であるから，判定結果にも確率的な要素が含まれる。本当は帰無仮説が正しい時に，間違って「対立仮説が正しい」と判定してしまう確率はゼロではない。逆に帰無仮説が正しくない時に，間違って「対立仮説が正しくない(帰無仮説が正しい)」と判定してしまう可能性もある。前者をタイプ1エラー，後者をタイプ2エラーという。どちらの確率もゼロであってほしいが，そうはならないので，前者を\\(\\alpha\\)，後者を\\(\\beta\\)としたときに，それぞれを一定の水準以下に抑えたい。この目的のために手順を整え，一般化したのが帰無仮説検定である。なお，先に述べた有意水準は，この\\(\\alpha\\)の許容される水準であり，心理学では一般に5%に設定する。\nこのように帰無仮説検定という考え方は，エラーの統制が本来の狙いであるから，「有意になるように工夫する」という発想は根本的に間違っている。また，統計的推定という数学的手続きに，人間が納得しやすい判定を下すという人為的手続きが組み合わさったものであるから，帰無仮説検定の結果に過剰な意味を持たせたり一喜一憂したりすることがないように注意しよう。\n\n\n7.1.2 帰無仮説検定の手続き\n帰無仮説検定の手続きを一般化すれば，次のようになる。\n\n帰無仮説と対立仮説を設定する。\n検定統計量を選択する。\n判定基準を決定する。\n検定統計量を計算する。\n判定する。\n\n帰無仮説検定は，群間の平均値に差があるかどうか，相関係数に統計的な意味があるかどうかといった事例に対して適用される。 当然のことながら，これは標本から母集団を推定するという文脈における話で，物理学的な真偽を理論的に判断するとか，全数調査のように母集団全体の情報が手に入る場合といった場合の話ではない。また，標本のサンプルサイズが小さく，標本統計量の信頼区間が大きいことから，枠組みなしには判定できないという背景があることも再確認しておこう。\n母集団の状態がわからないので，仮説を設定する。帰無仮説Null Hypothesisは空っぽの仮説という意味で，母平均差がない(差がゼロ，\\(\\mu_1 - \\mu_2 = 0\\))とか，母相関がゼロ(\\(\\rho = 0\\))である，とされる。対立仮説Alternative Hypothesisは帰無仮説と排他的な関係にある仮説としてつくられるから，「差が無くはない(\\(\\mu_1 - \\mu_2 \\neq 0\\))」「相関がゼロではない(\\(\\rho \\neq 0\\))」という表現になる。なぜ帰無仮説がゼロであることから始められるかといえば，ふたつの排他的な仮説を考えた時にゼロでない状態というのは無数にあり得るので，仮説として特定できないからである(差が1のとき，1.1のとき，1.11のとき・・・と延々と検定し続けるわけにもいくまい)。\n検定統計量の選択は，二群の平均値差のときは\\(t\\)，三群以上の時は\\(F\\)，相関係数の検定も\\(t\\)，と天下り的に示されることが一般的である。もちろんこれらの統計量が選ばれるのは，数理統計的な論拠に基づいている。判定基準は5%水準とすることが一般的だし，検定統計量の計算はアルゴリズムに沿って機械的に可能である。判定は客観的な指標に基づいて行われるから，「どの状況でどのような帰無仮説をおくか」が類型化できれば，この手続き全体が自動的に進められる。\nしかしここでは改めて，丁寧に手順を追いながらみてみよう。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#相関係数の検定",
    "href": "chapter07.html#相関係数の検定",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "7.2 相関係数の検定",
    "text": "7.2 相関係数の検定\nここでは相関係数の検定を例に取り上げる。俗に「無相関検定」と呼ばれるように，相関がどれほど大きいとかどれほど意味があるということをチェックするのでは無く，無相関ではない，ということをチェックする。もちろん標本相関は計算してゼロでなければ，それは無相関ではない。ここで考えたいのは，母相関がゼロではないということである。言い換えると，母相関がゼロの状態であっても，標本相関がゼロでないことは，小標本のサンプリングという背景のもとでは当然のことである。\n確認してみよう。まず，無相関なデータセットを作ることを考える。RのMASSパッケージを使い，多変量正規分布の確率分布関数から乱数を生成しよう。\n\npacman::p_load(MASS)\nset.seed(12345)\nN &lt;- 100000\nX &lt;- mvrnorm(N,\n  mu = c(0, 0),\n  Sigma = matrix(c(1, 0, 0, 1), ncol = 2),\n  empirical = TRUE\n)\nhead(X)\n\n           [,1]        [,2]\n[1,] -0.4070308 -0.72271139\n[2,] -0.5774631 -0.57075167\n[3,]  0.2312929 -0.42458994\n[4,]  0.6242499 -0.55522146\n[5,] -0.7791585  0.55004824\n[6,]  1.8995860 -0.04899946\n\n\nここでは10^{5}個の乱数を生成した。つくられたオブジェクトXは表示されているように，2変数からなる。ここでは相関のある2変数を想定しており，各変数がそれぞれ標準正規分布に従っているという設定である。rnorm関数を2つ使って2変数をつくっても良いのだが，2変数セットで取り出すことを考えると多変量正規分布をかんがえることになる。多変量正規分布は，ひとつひとつの変数については正規分布として平均とSDをもち，かつ，変数の組み合わせとして共分散をもつものである。mvrnormの引数をみると，muは平均ベクトルであり，Sigmaが分散共分散行列である。分散共分散行列とは，ここでは\\(2\\times 2\\)の正方行列であり，対角項に分散を，非対角項に共分散をもつ行列である。共分散は標準偏差と相関係数の積で表される。\n分散\n\\[ s_x^2 = \\frac{1}{n}\\sum (x_i - \\bar{x})^2 =  \\frac{1}{n}\\sum (x_i - \\bar{x})(x_i - \\bar{x})\\]\n標準偏差\n\\[ s_x = \\sqrt{s_x^2} = \\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\]\n共分散\n\\[ s_{xy} = \\frac{1}{n}\\sum (x_i - \\bar{x})(y_i - \\bar{y})\\]\n相関係数\n\\[r_{xy} = \\frac{s_{xy}}{s_xs_y} = \\frac{\\frac{1}{n}\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\sqrt{\\frac{1}{n}\\sum (y_i - \\bar{y})^2}}\\]\n分散共分散行列\n\\[\\Sigma = \\begin{pmatrix} s_x^2 & s_{xy} \\\\ s_{yx} & s_y^2 \\end{pmatrix}\n= \\begin{pmatrix} s_x^2 & r_{xy}s_xs_y \\\\ r_{xy}s_xs_y & s_y^2 \\end{pmatrix}\\]\n今回Sigma = matrix(c(1,0,0,1),ncol = 2)としたのは，この2変数が無相関であること(SDはそれぞれ1であること)を指定している。ちなみにempirical = TRUEのオプションは，生成された乱数が設定した分散共分散行列のもつ相関係数と一致するように補正することを意味している。\n可視化しておこう。つくられた乱数が無相関であることを，散布図を使って確認する。\n\npacman::p_load(tidyverse)\nX %&gt;%\n  as.data.frame() %&gt;%\n  ggplot(aes(x = V1, y = V2)) +\n  geom_point()\n\n\n\n\n\n\n\n\n数値的にも確認しておこう。\n\ncor(X) %&gt;% round(5)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nつくられた乱数が無相関であることが確認できた。さてこれが母集団であったとして，ここからたとえばn = 20のサンプルをとったとする。この時の相関はどうなるだろうか。 Rで計算してみよう。sample関数をつかって抜き出す行を決めて，該当する行だけs1オブジェクトに代入する。その上で相関係数を計算してみよう。\n\nselected_row &lt;- sample(1:N, 20)\nprint(selected_row)\n\n [1]  9647 80702 57543 93179 99032 82624 32672 53670 69698 42383 23801 69303\n[13]  9816 61803 69464 23107 76958 44447    10 27292\n\ns1 &lt;- X[selected_row, ]\ncor(s1)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.1431698\n[2,] 0.1431698 1.0000000\n\n\n今回の相関係数は0.1431698となった。母集団の相関係数が0であっても，適当に抜き出した20点が相関係数を持ってしまう(0でない)ことはあり得ることなのである。問題は，これがどの程度あり得ることなのか，である。いいかえると，研究者が\\(n=20\\)のサンプルをとって相関を得た時，それが\\(r = 0.14\\)であったとしても，母相関\\(\\rho = 0.0\\)からのサンプルである可能性がどれぐらいあるか，ということである。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#標本相関係数の分布と検定",
    "href": "chapter07.html#標本相関係数の分布と検定",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "7.3 標本相関係数の分布と検定",
    "text": "7.3 標本相関係数の分布と検定\n標本相関係数は確率変数なので，毎回標本を取る度に値が変わるし，どの実現値がどの程度出現するかは標本分布で表現できる。 ではどのような標本分布に従うのだろうか。先ほどのサンプリングを繰り返して，乱数によって近似してみよう1。\n\niter &lt;- 10000\nsamples &lt;- c()\nfor (i in 1:iter) {\n  selected_row &lt;- sample(1:N, 20)\n  s_i &lt;- X[selected_row, ]\n  cor_i &lt;- cor(s_i)[1, 2]\n  samples &lt;- c(samples, cor_i)\n}\ndf &lt;- data.frame(R = samples)\n# ヒストグラムの描画\ng &lt;- df %&gt;%\n  ggplot(aes(x = R)) +\n  geom_histogram(binwidth = 0.01)\nprint(g)\n\n\n\n\n\n\n\n\nヒストグラムを見ると，サンプルサイズが20の場合，母相関係数\\(\\rho = 0.0\\)であっても\\(r = 0.3\\)や\\(r=0.4\\)程度の標本相関が出現することはある程度みられることである。\nまた，標本分布は左右対称の何らかの理論分布に従っていそうだ。数理統計学の知見から，相関係数の場合，標本相関係数を次の式によって変換することで，自由度が\\(n-2\\)の\\(t\\)分布に従うことが知られている。\n\\[ t = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} \\]\n\ndf %&gt;%\n  mutate(T = R * sqrt(18) / sqrt(1 - R^2)) %&gt;%\n  ggplot(aes(x = T)) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 0.1) +\n  # 自由度18のt分布の確率密度関数を追加\n  stat_function(fun = dt, args = list(df = 18), color = \"red\", linewidth = 2) +\n  # Y軸のラベルを変更\n  ylab(\"Density\")\n\n\n\n\n\n\n\n\nこれを利用して相関係数の検定が行われる。以下，サンプルサイズ20で標本相関係数が\\(r=0.5\\)だったとして，手順に沿って解説する。\n\n帰無仮説は母相関\\(\\rho = 0.0\\)とする。対立仮説は\\(\\rho \\neq 0.0\\)である。\n検定統計量は相関係数\\(r\\)を変換した\\(t\\)とする。\n判定基準として，\\(\\alpha = 0.05\\)とする。すなわち，母相関が0であるという仮説を棄却して間違える確率を5%以下に制御したい。\n検定統計量を計算する。\\(n=20,r=0.5\\) より， \\[t = \\frac{0.5\\times(\\sqrt{18})}{\\sqrt{1-0.5^2}} = 2.449\\]\n標本相関係数の絶対値が0.5を超える確率は，\\(t\\)分布の理論値から，次のように計算できる。あるいは，\\(t\\)分布の両端5%を切り出す臨界値 を次のように計算できる。\n\n\n(1 - pt(0.5 * sqrt(18) / sqrt(1 - 0.5^2), df = 18)) * 2\n\n[1] 0.02476956\n\nqt(0.975, df = 18)\n\n[1] 2.100922\n\n\nここで注意してほしい点は，今回の検定の目的が「母相関が0であるという帰無仮説を棄却できるかどうか」であり，相関係数の符号については関心がなく絶対値で考える点である。pt関数は，ある確率点までの累積面積であるから，1から引くことでその確率点以上の値がでる確率が示される。\\(t\\)分布は左右対称の分布なので，これを2倍した値が絶対値で考えた時の出現確率である。これが5%よりも小さければ，有意であると判断できる。今回は，統計的に有意であるといって良い。\nなお，表現上の細かい注意点になるが，この確率は今回の実現値「以上」のより極端な値が出る確率であり，この実現値が出る確率という言い方はしない。確率は面積であり，点に対する面積はないからである。\nqt関数で示されるのは確率点なので，これ以上の値を今回の実現値が出していたら，統計的に有意であると判断できる。今回の実現値から算出した値は\\(t(18)=2.449\\)であり，臨界値の\\(2.100\\)よりも大きな値なので，有意であると判断できる。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#種類の検定のエラー確率",
    "href": "chapter07.html#種類の検定のエラー確率",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "7.4 2種類の検定のエラー確率",
    "text": "7.4 2種類の検定のエラー確率\n上では丁寧に計算過程をみてきたが，実践場面ではサンプルはひとつであり，標本統計量もひとつ算出されるだけである。自分の大切なデータであるから，標本分布から得られた特定のケースにすぎないことが直感的にわかりにくいかもしれない。\n相関係数の検定をするときは，Rの関数cor.testを使って次のように行う。ここではmvrnorm関数を使って，相関係数0.5の仮想データを作っている。\n\nset.seed(17)\nn &lt;- 20\nsampleData &lt;- mvrnorm(n,\n  mu = c(0, 0),\n  Sigma = matrix(c(1, 0.5, 0.5, 1), ncol = 2),\n  empirical = TRUE\n)\ncor.test(sampleData[, 1], sampleData[, 2])\n\n\n    Pearson's product-moment correlation\n\ndata:  sampleData[, 1] and sampleData[, 2]\nt = 2.4495, df = 18, p-value = 0.02477\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.07381057 0.77176071\nsample estimates:\ncor \n0.5 \n\n\n結果として示されている，tの値や自由度，\\(p\\)値が先ほど示した例と対応していることを確認できる。さらに，相関係数の信頼区間や標本相関係数そのものも示されている。この信頼区間が0を跨いでいないことからも，帰無仮説が棄却されることが見て取れるだろう。\nわれわれは既に，母相関が0のデータセットの一部を取り出すと，その相関係数が0ではなく0.5のような数字になることも知っている。もちろん母相関が0であれば標本相関も0近い値が出やすいとしても，である。つまり標本から得られた値をあまり大事に考えすぎない方が良い(もちろん一般化を念頭においている時は，である)。 また，帰無仮説は「母相関が0である」なので，これが棄却されたとしても「母相関が0であるとは言えない」のに過ぎない。ここから，母相関も\\(r=0.5\\)付近にあるはずだとか，\\(p\\)値が2.4%なので5%よりもずいぶん低いのは証拠の重要さを物語っているのだ，と論じるのは適切ではない。母相関が0という仮想的な状況のもとでの話であって，母相関が実際にどの程度なのかを検討しているわけではない。この点が誤解されやすいので特に注意してほしい。\nここに来るとタイプ1エラー，タイプ2エラーがより具体的に理解できるようになってきたではないだろうか。タイプ1エラーはこの帰無仮説が正しい時に，標本相関から計算した統計量で判断する確率であるから，上の手続きで見たことそのものである。\n別の角度で見てみよう。cor.testをつかうと標本統計量の信頼区間が算出できる。この信頼区間が母相関–ここでは帰無仮説である\\(\\rho =0\\)を「正しく」含んでいる割合を見てみよう。 cor.test関数が返すオブジェクトには，conf.intという名前のものがあり，デフォルトではここで95%の信頼区間が含まれている。 シミュレーションに先立って，結果を格納する2列のデータフレームを作っておき，シミュレーション後にifelse関数で母相関が含まれているかどうかの判定をした。\n\nset.seed(42)\niter &lt;- 10000\nintervals &lt;- data.frame(matrix(NA, nrow = iter, ncol = 2))\nnames(intervals) &lt;- c(\"Lower\", \"Upper\")\nfor (i in 1:iter) {\n  selected_row &lt;- sample(1:N, 20)\n  s_i &lt;- X[selected_row, ]\n  cor_i &lt;- cor.test(s_i[, 1], s_i[, 2])\n  intervals[i, ] &lt;- cor_i$conf.int[1:2]\n}\n#\ndf &lt;- intervals %&gt;%\n  mutate(FLG = ifelse(Lower &lt;= 0 & Upper &gt;= 0, 1, 0)) %&gt;%\n  summarise(type1error = mean(FLG)) %&gt;%\n  print()\n\n  type1error\n1       0.95\n\n\n今回の例では，95%の割合で正しく判断できていた。言い換えると，エラーが生じる割合は5%だったので，タイプ1エラー確率を5%以下にするという目的はしっかり達成できていたことが確認できた。\n同様に，タイプ2エラーは，帰無仮説が正しくないときに帰無仮説を採択する確率だから，シミュレーションするなら次のようになる。まず母相関が0でない状況を作り出そう。今回は母相関が0.5であるとして，母集団分布を描いてみよう。\n\nset.seed(12345)\nN &lt;- 100000\nX &lt;- mvrnorm(N,\n  mu = c(0, 0),\n  Sigma = matrix(c(1, 0.5, 0.5, 1), ncol = 2),\n  empirical = TRUE\n)\n\nX %&gt;%\n  as.data.frame() %&gt;%\n  ggplot(aes(x = V1, y = V2)) +\n  geom_point()\n\n\n\n\n\n\n\n\n今度は，ここからサンプルサイズ20のデータセットを取り出し，検定することにしよう。検定の結果，有意になれば1，ならなければ0というオブジェクトを作って，判定の正しさを考えてみることにする。\n\niter &lt;- 10000\njudges &lt;- c()\nfor (i in 1:iter) {\n  selected_row &lt;- sample(1:N, 20)\n  s_i &lt;- X[selected_row, ]\n  cor_i &lt;- cor.test(s_i[, 1], s_i[, 2])\n  judges &lt;- c(judges, cor_i$p.value)\n}\ndf &lt;- data.frame(p = judges) %&gt;%\n  mutate(FLG = ifelse(p &lt;= 0.05, 1, 0)) %&gt;%\n  summarise(\n    sig = sum(FLG == 1),\n    non.sig = sum(FLG == 0),\n    type2error = non.sig / iter\n  ) %&gt;%\n  print()\n\n   sig non.sig type2error\n1 6442    3558     0.3558\n\n\n今回は母相関が0.5であり，帰無仮説は棄却されて然るべきなのだが，有意でないと判断された割合が35.58%あったことになる。心理学の研究などでは，この確率\\(\\beta\\)が0.2未満，逆にいうと検出が0.8以上あることが望ましいとされているので，今回のこの事例では十分な件出力がなかった，と言えるだろう。\nもちろん実際には，母相関がどれぐらいなのかわからない。\\(0.3\\)なのかもしれないし，\\(-0.5\\)であるかもしれない。つまりタイプ2エラーは研究者が制御できるところではなく，せいぜい大きな相関が見込めそうな変数について標本を取ろうと心がけるだけである。\nタイプ1,2エラーの確率は，サンプルサイズや効果量(ここでは母相関の大きさ)の関数である。サンプルサイズは研究者が決定することができるので，効果を見積もり，制御したいエラー確率の基準を決めて，合理的にサンプルサイズを決めるべきである。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#課題",
    "href": "chapter07.html#課題",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "7.5 課題",
    "text": "7.5 課題\n\n母相関が0の母集団から，サンプルサイズ10の標本を取り出して標本相関を見た時の標本分布を，乱数のヒストグラムで近似してみましょう。\n同じく，サンプルサイズ50の標本を取り出して標本相関を見た時の標本分布を，乱数のヒストグラムで近似してみましょう。サンプルサイズが20や10の時と比べてどういう違いがあるでしょうか。\nサンプルサイズ50の標本相関が\\(r=-0.3\\)のとき，統計的に有意と言えるでしょうか。cor.test をつかって検定し，検定結果と判断結果を記述してください。\n標本相関が\\(r=-0.3\\)だとします。サンプルサイズが10,20,50,1000のとき，統計的に有意と言えるでしょうか。cor.testを使って検定し，検定結果を一覧にしてみましょう。ここから何がわかるでしょうか。\n母相関が\\(\\rho = -0.3\\)だったとします。サンプルサイズ20のとき，どの程度の検出力があると見込めるでしょうか。シミュレーションで近似してください。\n\n\n\n\n\n池田功毅, and 平石界. 2016. “心理学における再現可能性危機：問題の構造と解決策.” 心理学評論 59 (1): 3–14. https://doi.org/10.24602/sjpr.59.1_3.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter07.html#footnotes",
    "href": "chapter07.html#footnotes",
    "title": "7  統計的仮説検定(Null Hypothesis Statistical Testing)",
    "section": "",
    "text": "このような二度手間を取らず，mvrnormからサンプルサイズ20の乱数を反復生成しても良い。母集団を具体的なものとしてイメージするために，母相関が0の母集団からサンプリングを繰り返す方法をとった。↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>統計的仮説検定(Null Hypothesis Statistical Testing)</span>"
    ]
  },
  {
    "objectID": "chapter08.html",
    "href": "chapter08.html",
    "title": "8  平均値差の検定",
    "section": "",
    "text": "8.1 一標本検定\n平均値差の検定は，実験計画の結論を出すために用いられる手段である。無作為割り当てによって個人差や背景要因が相殺され，平均的な因果効果を検証することができるからである。 その結果を一般化するためには，やはり推測統計学の知見が必要であり，サンプルサイズやタイプ1,2エラーが関わってくることに変わりはない。\nまず配置標本検定の例から始める。母平均がわかっている，あるいは理論的に仮定される特定の値に対して，標本平均が統計的に有意に異なっていると言って良いかどうかの判断をするときに用いる。 たとえば7件法のデータを取ったときに，ある項目の平均が中点4より有意に離れていると言って良いかどうか，といった判定をするときに用いる。かりに，サンプルサイズ10で7件法のデータが得られたとしよう。ここでは平均4,SD1の正規乱数を10件生成することで表現する。実際にはこの値を，人に対する尺度カテゴリへの反応として得ているはずである。\npacman::p_load(tidyverse)\nset.seed(17)\nn &lt;- 10\nmu &lt;- 4\nX &lt;- rnorm(n, mean = mu, sd = 1)\nprint(X)\n\n [1] 2.984991 3.920363 3.767013 3.182732 4.772091 3.834388 4.972874 5.716534\n [9] 4.255237 4.366581\n今回，標本平均は4.177であり，これより極端な値が\\(\\mu = 4\\)の母集団から得られるかどうかを検定する。帰無仮説検定の手順にそって進めていくと，以下のようになる。\nこのあと，検定統計量の計算と判定である。これをRはt.test関数で一気に処理できる。\nresult &lt;- t.test(X, mu = mu)\nprint(result)\n\n\n    One Sample t-test\n\ndata:  X\nt = 0.6776, df = 9, p-value = 0.5151\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 3.585430 4.769131\nsample estimates:\nmean of x \n 4.177281\n結果として，今回の検定統計量の実現値は0.678であり，自由度9のt分布からこれ以上の値が出てくる確率は，0.515であることがわかる。これは5%水準と見比べてより大きいので，レアケースではないと判断できる。つまり，母平均4の正規母集団から，4.177の標本平均が得られることはそれほど珍しいものではなく，統計的に有意に異なっていると判断するには及ばない，ということである。\nレポートなどに記載するときは，これら実現値やp値を踏まえて「\\(t(9)=0.66776,p=0.5151 ,n.s.\\)」などとする。ここでn.s.はnot significantの略である。\nさてこの例では，母平均4の正規乱数を生成し，その平均が4と異なるとはいえない，と結論づけた。これは一見，当たり前のことのようであり，無意味な行為におもえるかもしれない。しかし次の例を見てみよう。\nn &lt;- 3\nmu &lt;- 4\nX &lt;- rnorm(n, mean = mu, sd = 1)\nmean(X) %&gt;%\n  round(3) %&gt;%\n  print()\n\n[1] 5.04\n\nresult &lt;- t.test(X, mu = mu)\nprint(result)\n\n\n    One Sample t-test\n\ndata:  X\nt = 5.1723, df = 2, p-value = 0.03541\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 4.174825 5.904710\nsample estimates:\nmean of x \n 5.039768\nここではサンプルサイズ\\(n=3\\)であり，標本平均が5.04であった。このときt値は5%臨界値を上回っており，「母平均4のところから得られる値にしては極端」であるから，統計的に有意に異なる，と判断することになる。乱数生成時は平均を確かに4に設定したが，母平均から取り出したごく一部が，そこから大きく離れてしまうことはあり得るのである。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#一標本検定",
    "href": "chapter08.html#一標本検定",
    "title": "8  平均値差の検定",
    "section": "",
    "text": "帰無仮説は母平均が理論的な値(ここでは尺度の中点4)であること，すなわち\\(\\mu =4\\)であり，対立仮説は\\(\\mu \\neq 4\\) である。\n検定統計量は，正規母集団から得られる標本平均が従う標本分布であり，母分散が未知の場合の区間推定に用いたT統計量になる。\n判断基準は心理学の慣例に沿って5%とする。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#二標本検定",
    "href": "chapter08.html#二標本検定",
    "title": "8  平均値差の検定",
    "section": "8.2 二標本検定",
    "text": "8.2 二標本検定\n続いて二標本の検定について考えよう。実験群と統制群のように，無作為割り当てをすることで平均因果効果をみる際に行われるのが，この検定である。帰無仮説は「群間差はない」であり，対立仮説はその否定である。また，正規母集団からの標本を仮定するので，検定統計量はここでもt分布に従う値になる。帰無仮説検定の手順に沿って，改めて確認しておこう。\n\n帰無仮説は「二群の母平均に差がない」である。二群の母平均をそれぞれ\\(\\mu_1,\\mu_2\\)とすると，帰無仮説は\\(\\mu_1 = \\mu_2\\)，あるいは\\(\\mu_1 - \\mu_2 = 0\\)と表される。対立仮説は\\(\\mu_1 \\neq \\mu_2\\)あるいは\\(\\mu_1-\\mu_2 \\neq 0\\)である。\n検定統計量は，正規母集団から得られる標本平均が従う標本分布であり，母分散が未知の場合の区間推定に用いたT統計量になる。\n判断基準は心理学の慣例に沿って5%とする。\n\nこれを検証するために，サンプルデータを乱数で生成しよう。 まず，各群のサンプルサイズをn1,n2とする。ここでは話を簡単にするため，サンプルサイズは両群ともに10とした。つぎに両群の母平均だが，群1の母平均を\\(\\mu_1\\)，群2の母平均を\\(\\mu_2 = \\mu_1 + \\delta\\)で表現した。この\\(\\delta\\)は差分であり，これが\\(\\delta=0\\)であれば母平均が等しいこと，\\(\\delta \\neq 0\\)であれば母平均が異なることになる。最後に両群の母SDを設定した。\nここでの検定は，この差分\\(d\\)が母平均0の母集団から得られたと判断して良いかどうか，という形で行われる。検定統計量\\(T\\)は，次式で算出されるものである。\n\\[ T = \\frac{d - \\mu_0}{\\sqrt{U^2_p/\\frac{n_1n_2}{n_1+n_2}}}\\]\nここで\\(d\\)は二群の標本平均の差であり，\\(U^2_p\\)はプールされた不偏分散と呼ばれ，二群を合わせて計算された全体の母分散推定量である。各群の標本分散をそれぞれ\\(S^2_1, S^2_2\\)とすると，次式で算出される。\n\\[ U^2_p = \\frac{n_1S^2_1+ n_2S^2_2}{n_1 + n_2 -2} \\]\nこれらの式はつまり，サンプルサイズの違いを考慮するため，一旦両群の標本分散に各サンプルサイズを掛け合わせ，プールした全体のサンプルサイズから各々\\(-1\\)をすることで全体として不偏分散にしている。\nこれを踏まえて，具体的な数字で見ていこう。 その上で乱数でデータを生成し，その標本平均を確認した上で，t.test関数によって検定を行っている。\n\nn1 &lt;- 10\nn2 &lt;- 10\nmu1 &lt;- 4\nsigma &lt;- 1\ndelta &lt;- 1\nmu2 &lt;- mu1 + (sigma * delta)\n\nset.seed(42)\nX1 &lt;- rnorm(n1, mean = mu1, sd = sigma)\nX2 &lt;- rnorm(n2, mean = mu2, sd = sigma)\n\nX1 %&gt;%\n  mean() %&gt;%\n  round(3) %&gt;%\n  print()\n\n[1] 4.547\n\nX2 %&gt;%\n  mean() %&gt;%\n  round(3) %&gt;%\n  print()\n\n[1] 4.837\n\nresult &lt;- t.test(X1, X2, var.equal = TRUE)\nprint(result)\n\n\n    Two Sample t-test\n\ndata:  X1 and X2\nt = -0.49924, df = 18, p-value = 0.6237\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.506473  0.927980\nsample estimates:\nmean of x mean of y \n 4.547297  4.836543 \n\n\n今回の母平均は\\(\\mu_1 = 4, \\mu_2 = 4+1\\)にしているが，標本平均は4.547と4.837であり，標本上では大きな差が見られなかった。結果として，t値は0.4992369であり，自由度18のもとでのp値は0.6236593である。5%水準を上回る値であるから，結論としては対立仮説を採択するには至らない，差があるとはいえない，である。\n今回の設定では母平均に差があるはず(\\(4 \\neq 4 + 1\\))なのだから，これは誤った判断で，タイプ2エラーが生じているケースということになる。研究実践場面では，母平均やその差については知り得ないのだから，このような判断ミスが生じていたかどうかは分かり得ないことに留意しよう。\nなお，ここではわかりやすく2群であることを示すためにX1,X2と2つのオブジェクトを用意したが，実践的にはデータフレームの中で群わけを示す変数があり，formulaの形で次のように書くことが多いだろう。\n\ndataSet &lt;- data.frame(group = c(rep(1, n1), rep(2, n2)), value = c(X1, X2)) %&gt;%\n  mutate(group = as.factor(group))\nt.test(value ~ group, data = dataSet, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  value by group\nt = -0.49924, df = 18, p-value = 0.6237\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.506473  0.927980\nsample estimates:\nmean in group 1 mean in group 2 \n       4.547297        4.836543",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#二標本検定ウェルチの補正",
    "href": "chapter08.html#二標本検定ウェルチの補正",
    "title": "8  平均値差の検定",
    "section": "8.3 二標本検定(ウェルチの補正)",
    "text": "8.3 二標本検定(ウェルチの補正)\n先ほどのt.test関数には，var.equal = TRUEというオプションが追加されていた。これは2群の分散が等しいと仮定した場合の検定になる。t検定は歴史的にこちらが先に登場しているが，2群の分散が等しいかどうかはいきなり前提できるものでもない。等分散性の検定は，Levene検定を行うのが一般的であり，R においては，carパッケージやlawstat パッケージが対応する関数を持っている。ここではcarパッケージの leveneTest関数を用いる例を示す。\n\npacman::p_load(car)\nleveneTest(value ~ group, data = dataSet, center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  1  2.9405 0.1035\n      18               \n\n\nこの結果を見ると，p値から明らかなように，2群の分散が等しいという帰無仮説が棄却できなかったので，等しいと考えてt検定に進むことができる。もしこれが棄却されてしまったら，2群の分散が等しいという帰無仮説が成り立たないのだから，等分散性の仮定を外す必要がある。実行は簡単で，var.equalをFALSEにすれば良い。\n\nresult2 &lt;- t.test(value ~ group, data = dataSet, var.equal = FALSE)\nprint(result2)\n\n\n    Welch Two Sample t-test\n\ndata:  value by group\nt = -0.49924, df = 13.421, p-value = 0.6257\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.5369389  0.9584459\nsample estimates:\nmean in group 1 mean in group 2 \n       4.547297        4.836543 \n\n\nよく見ると，タイトルがWelch Two Sample t-testに変わっている。Welchの補正が入ったt検定という意味である。また自由度が実数(13.421)になっているが，このようにt分布の自由度を調整することで等分散性の仮定から逸脱した場合の補正となる。もちろん報告する際は「\\(t(\\) 13.421 \\()=\\) -0.499, \\(p=\\) 0.626」のように書くことになるから，自由度が実数であれば補正済みであると考えられるだろう。\nしかし，分散が等しいという仮定は，等しくない場合の特殊な場合であるから，最初からWelchの補正がはいった検定だけで十分である。このような考え方から，Rにおけるt.test 関数のデフォルトではvar.equal = FALSEとなっており，特段の指定をしなければ等分散性の仮定をしない。こちらの方が検定を重ねることがないので，より望ましい。\n\n8.3.1 効果量の算出\n今回の例は，仮想データとして\\(\\mu_1 = 4,\\mu_2 = \\mu_1 + \\sigma d\\)であり，明らかに\\(\\mu_1 \\neq \\mu_2\\)なのだが，有意差を検出するには至らなかった。統計的な有意差はあくまでも「統計的な」観点からのものであり，我々が現実に検証したいのは本当に差があるかどうか，いわば「実質的な差」があるかどうかであるのだから，統計的な有意差を得ることを目的にするのははっきりと不適切な目標設定であると言えるだろう1。\nところで，統計的に差があるとはっきり言えるのはどのような時だろうか。これは次の4つのデータの分布を見てもらうとわかりやすい。\n\n\n\n\n\n\n\n\n\n左列は平均差が大きいデータ，右列は小さいデータである。 上段は分散が小さいデータ，下段は大きいデータである。 この4つそれぞれのシーンにおいて，「差がある」と判断しやすいのはどれかを考えてみるとよい。当然，左上のシーンが最も明確に差があると言えるであろう。なぜなら，両群が明確に分かれており，群間の重複がないからである。左下は同じ平均値差であっても，群内の広がりが大きいから群間の重複がみられるため，「差がある」という判断を受けても各群の中には該当しないケースがちらほらみられることだろう。右上パネルのようなケースでは，重複は少ないが差が小さいため，「差がある」と判断できるかどうかが微妙である。右下に至っては，差も小さく分布の重複も大きいから，「差がある」と判断しても該当しないケースが多くなる。たとえば「男性は女性よりも力が強い(体力・筋力に差がある)」というデータがあったとしても，「女性より非力な男性」もかなり多く存在するだろう。そういう反例が多くみられるような場合，統計的に差があるという結果が示されたとしても，受け入れられないのではないだろうか。\nここから明らかなように，差の判断には平均値差だけでなく分散も関わってくる。そこで平均値差を標準偏差で割った，標準化された差が重要になってくるのであり，これが効果量と呼ばれるものである2。\n今回2群の差のデータを作る時に，\\(\\sigma d\\) としたが，平均値差の効果量esは， \\[ es = \\frac{\\mu_1 - \\mu_2}{\\sigma} \\]\nで表現されるから，\\(d\\)が効果量を表していたのである。もちろん我々は母平均，母SDなどを知り得ないのでこれもデータから推定する他ない。幸いRにはeffsizeパッケージなど，効果量を算出するものが用意されている。\n\npacman::p_load(effsize)\ncohen.d(value ~ group, data = dataSet)\n\n\nCohen's d\n\nd estimate: -0.2232655 (small)\n95 percent confidence interval:\n    lower     upper \n-1.165749  0.719218 \n\ncohen.d(value ~ group, data = dataSet, hedges.correction = TRUE)\n\n\nHedges's g\n\ng estimate: -0.2138318 (small)\n95 percent confidence interval:\n     lower      upper \n-1.1162608  0.6885973 \n\n\n平均値差の検定の後は，ここに示したCohenのdやHedgesのgといった効果量を添えて報告することが一般的である。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#対応のある二標本検定",
    "href": "chapter08.html#対応のある二標本検定",
    "title": "8  平均値差の検定",
    "section": "8.4 対応のある二標本検定",
    "text": "8.4 対応のある二標本検定\n実験群と統制群のように異なる2群ではなく，プレポスト実験のように対応がある2群の場合は，t検定の定式化が異なる。対応がないt検定の場合は，群平均の差\\(\\mu_1 - \\mu_2\\)の分布を考えたが，対応がある場合は個々の測定の差，つまり\\(X_{i1} - X_{i2} = D_i\\)を考える。この一つの標本統計量を検定するのだから，一標本検定の一種であるとも言える。またこのDの分布の標準誤差は，標本標準誤差\\(U_D\\)を使った\\(U_D/\\sqrt{n}\\)を使って推定する3。検定統計量\\(T\\)は，次式で算出される。\n\\[ T = \\frac{\\bar{D}}{U_D/\\sqrt{n}} = \\frac{\\sum D_i/n}{\\sqrt{\\frac{\\frac{1}{n-1}\\sum(D_i-\\bar{D})^2}{n}}}\\]\n検定にあたっては，t.test関数の引数pairedをTRUEにするだけで良い。\n\n8.4.1 仮想データの組成\n仮想データを作って演習してみよう。データの組成については，2種類のアプローチで説明が可能である。ひとつは次のシミュレーションで表されるような形である。\n\nn &lt;- 10\nmu1 &lt;- 4\nsigma &lt;- 1\nd &lt;- 1\nX1 &lt;- rnorm(n, mu1, sigma)\nX2 &lt;- X1 + sigma * d + rnorm(n, 0, sigma)\nt.test(X1, X2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  X1 and X2\nt = -1.8036, df = 9, p-value = 0.1048\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.4339112  0.1617193\nsample estimates:\nmean difference \n     -0.6360959 \n\n\nすなわち，第一の群が\\(\\mu_1\\)を平均にばらついた実現値として得られ，第二の群はその実現値に一定の効果\\(\\sigma *d\\)が加わり，その測定にさらに誤差がつく形である。この方法は具体的なデータ生成プロセスをそのまま模したような形でデータを作っているが，測定誤差を二重に計上している点が気になるかもしれない。\nもう一つの考え方は，プレポスト型のデータに限らず，何らかの形で「対応がある」ことも表現できるものである。対応があるということは，2つのデータがそれぞれ独立した一変数正規分布から得られているのではなく，二変数正規分布から得られると考えるのである。二変数正規分布は，それぞれの変数は正規分布しているが，両者の間に相関があると考えるものである。変数が一つだけの正規分布は \\[X \\sim N(\\mu,\\sigma)\\] で表現されているのに対し，複数の変数を同時に生成する多変数(多次元)正規分布Multivariate Normal Distributionは，以下のように表現される。 \\[ \\mathbf{X} \\sim MVN(\\mathbf{\\mu},\\mathbf{\\Sigma})\\]\nここで\\(\\mathbf{X}\\)や\\(\\mathbf{\\mu}\\)は\\(n\\)次元ベクトルであり，\\(\\mathbf{\\Sigma}\\)は分散共分散行列を表している。二変数の場合は以下のように書くことができる。\n\\[\\mathbf{\\Sigma} = \\begin{pmatrix} \\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2 \\end{pmatrix} = \\begin{pmatrix} \\sigma_1^2 & \\rho_{12}\\sigma_1\\sigma_2 \\\\ \\rho_{21}\\sigma_2\\sigma_1 & \\sigma_2^2 \\end{pmatrix}\\]\n共分散\\(\\sigma_{ij}\\)は相関係数\\(\\rho_{ij}\\)を用いて書けることからわかるように，変数間に相関があることを想定してデータを生成するのである。この組成に従った仮想データの作成は以下のとおりである。\n\npacman::p_load(MASS) # 多次正規乱数を生成するのに必要\nn &lt;- 10\nmu1 &lt;- 4\nsigma &lt;- 1\nd &lt;- 1\nmu &lt;- c(mu1, mu1 + sigma * d)\nrho &lt;- 0.4\nSIG &lt;- matrix(c(sigma^2, rho * sigma * sigma, rho * sigma * sigma, sigma^2), ncol = 2, nrow = 2)\nX &lt;- mvrnorm(n, mu, SIG)\nt.test(X[, 1], X[, 2], paired = TRUE)\n\n\n    Paired t-test\n\ndata:  X[, 1] and X[, 2]\nt = -2.4313, df = 9, p-value = 0.0379\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.96934592 -0.07095313\nsample estimates:\nmean difference \n       -1.02015 \n\n\n効果量については，対応のないt検定の場合と同じで良い。\n\ncohen.d(X[, 1], X[, 2])\n\n\nCohen's d\n\nd estimate: -1.04088 (large)\n95 percent confidence interval:\n      lower       upper \n-2.04204357 -0.03971697 \n\ncohen.d(X[, 1], X[, 2], hedges.correction = TRUE)\n\n\nHedges's g\n\ng estimate: -0.9968994 (large)\n95 percent confidence interval:\n     lower      upper \n-1.9510179 -0.0427809 \n\n\n\n\n8.4.2 検定の方向性\nここまでの検定では，主に「差があるかどうか」といった仮説に対応するものを扱ってきた。差があるかどうか，というのはその差がプラスの方向にでているのか，マイナスの方向に出ているのかといったことを問題にしていない。そこで検定統計量の分布についても，分布の両裾を考えて有意水準を設定していた。\nしかしプレポスト実験などでは，効果が「上がった」のか「下がった」のか，ということが大きな関心時でもあることが多いだろう。効果がある，ただし逆効果である，というのでは意味がないからである。このように方向性をもった仮説を検証する場合は，検定統計量の分布も一方向だけ考えればよく，t.test関数にはalternativeオプションをつかって表現する。\nt.test(x,y,alternatives = \"less\") とすると\\(x &lt; y\\)の帰無仮説を検証することになるし，alternatives = \"greater\"とすると\\(x &gt; y\\)の帰無仮説を検証することになる。デフォルトではalternatives = \"two.sided\"であり，両側検定が選ばれている。\nただし，両裾から片裾にかわるということは，検定統計量が超えるかどうかの判断をする臨界値が小さくなることでもある。必然的に，片裾(片側検定)のほうが緩やかな基準で検定をしていることにもなる。デフォルトで普段から厳しく検定しているから大丈夫だろう，というのも一つの考え方だが，やはり本来の研究仮説に適した帰無仮説の設定をするべきだろう。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#課題",
    "href": "chapter08.html#課題",
    "title": "8  平均値差の検定",
    "section": "8.5 課題",
    "text": "8.5 課題\n\n平均が50、標準偏差が10の正規分布からランダムに選んだ30個のサンプルを用意し，このサンプルの平均が母集団の平均と異なるかどうかを検定してください。検定結果を，心理学のフォーマット(心理学会編「論文執筆投稿の手引き」)に準拠した書き方で，結果を記述してください。\n以下のデータセットを使用して，2つの独立した群の平均に差があるかどうかをt検定してください。検定結果を，心理学のフォーマット(心理学会編「論文執筆投稿の手引き」)に準拠した書き方で，結果を記述してください。 \\[ group1 =\\{45, 50, 55, 60, 65 \\} \\] \\[ group2 = \\{57, 60, 62, 77, 75 \\} \\]\n多次元正規分布を用いた仮想データ生成方で，対応のあるt検定の練習をしましょう。サンプルサイズを\\(n=20\\)とし, 平均ベクトル\\(\\mu = (12, 15)\\), 分散共分散行列\\(\\Sigma = \\begin{pmatrix} 4 & 2.8 \\\\ 2.8 & 4\\end{pmatrix}\\)の多次元正規分布から作られた乱数を使って，対応のあるt検定をしてください。検定結果を，心理学のフォーマット(心理学会編「論文執筆投稿の手引き」)に準拠した書き方で，結果を記述してください。\n自由度が10, 20, 30のt分布のグラフを，標準正規分布のグラフとともに描画してください。自由度が増えるとt分布がどのように変化するでしょうか。\n自由度が15のt分布において、有意水準5%の片側検定と両側検定の臨界値(検定の判断基準となる理論値)を求めてください。\n\n\n\n\n\n豊田秀樹. 2009. 検定力分析入門: Rで学ぶ最新データ解析ー. 東京図書.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#footnotes",
    "href": "chapter08.html#footnotes",
    "title": "8  平均値差の検定",
    "section": "",
    "text": "たとえば物理学などのシーンでは，測定の精度が高く，単一の物理世界を対象にした検証を行うのだから，予測が真であるか偽であるかを確率的に考えるような必要はない。そのような世界における検証–あえて理論的な正しさが明確な世界，と表現するが–であれば，統計的な差があるかどうかの情報はあくまでも理論を支持するおまけ情報にすぎない。いわば統計的検定の結果を報告するのは，論文を書くためのレトリックである。例えば，ニュートンの運動法則やアインシュタインの相対性理論などの物理法則の検証では，測定誤差の範囲内で理論値と実験値が一致するかどうかが重要であり，統計的な有意性は二次的な情報に過ぎない。翻って，人間を対象にした小サンプルの科学である心理学は，統計的な判断に頼らざるを得ないという側面はあるだろう。しかしだからと言って，実質的な差が本質的であることを忘れてしまっては本末転倒である。↩︎\n統計的な有意差よりも効果量，効果量よりも実質的な差のほうが意味のある差であることを忘れてはならない。詳しくは 豊田 (2009) を参照。↩︎\n対応があるケースを考えているので，当然\\(n\\)は前後の群で同数である。↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html",
    "href": "chapter09.html",
    "title": "9  多群の平均値差の検定",
    "section": "",
    "text": "9.1 分散分析の基礎\n心理学実験においては，古典的に分散分析モデルが多用されてきた。平均値差を見ることで実験の効果，因果関係を明らかにできるように，巧妙に実験デザインが組み立てられる。その精緻さは理論的一貫性という意味である種の美しさを持ち，多くの研究者が魅了されてきた。\n分散分析にのせることを目的にした実験計画であり，実験デザインの不自由さ(とにかく分散分析をしなければならない!)が批判的に論じられることもあるが，心理学が測定している対象が平均値差以上の精度で議論できる性質でないという反論もあるだろう。\n今や分散分析を超えたより高度な統計モデルがあり，現在の研究においては分散分析はもはや過去のものにすぎないかもしれないが，以後のモデルも分散分析を基本としたその発展系であるので，改めて基本を押さえておくことも重要である。\n分散分析は「分散」の分析であるかのような名称であるが，平均値差を検定するためのものである。なぜ「分散」を冠するかといえば，効果量のところで見たように，平均値差の判断には群内分散の情報が必要だからである。\n多群の平均値の差，その散らばりを群間分散といい，群に含まれるデータの散らばりを群内分散とよぶ。分散分析は群内分散に対する群間分散の比が十分に大きいと考えられる場合，群間に統計的な有意差があると判断する。分散の比を表す確率分布はF分布と呼ばれる。F分布は群間・群内それぞれの自由度を母数にもつ。\nまた，実験計画はBetweenデザインとWithinデザインに区分される。t検定でみたような，対応のない独立した群を対象にしたデザインがBetween，群間に相関が想定される対応のある群を対象にしたデザインがWithinである。Withinデザインは同じ個体から複数回の反応を得る(ex. period 1-2-3…)ため，反復測定デザインRepeated measured designともよばれることがある。この場合，群内分散から個人内の分散すなわち個人差を取り出すことができるため，これが分離できないBetweenデザインよりも基本的にWithinデザインのほうが目的となる変動を捉えやすい。ただし，反復測定による個体への負担を考えると，毎回Withinデザインでいいというわけにもいかないところが難点である。\n\\[Between Design: \\text{全変動} = \\text{群間変動}+ \\text{群内変動(誤差)}\\] \\[Within Design:  \\text{全変動} = \\text{群間変動}+ \\text{個人差変動} + \\text{誤差}\\]\n分散分析は要因が複数ある場合も考えられるから，要因AがBetween，要因BがWithinといった場合は混合計画と呼ばれることがある。慣例的に，要因Factorとその要因に含まれる水準Levelを同時に表現し，\\(\\text{間}2 \\times \\text{間}3\\)の分散分析(二要因の分散分析で，いずれもBetweenデザインであり，水準数がそれぞれ2と3)，といった言い方をすることがある。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#分散分析のステップ",
    "href": "chapter09.html#分散分析のステップ",
    "title": "9  多群の平均値差の検定",
    "section": "9.2 分散分析のステップ",
    "text": "9.2 分散分析のステップ\nt検定において等分散性の仮定が成立するかどうかが事前に問題になったように，Betweenデザインにおいても分散の等質性は仮定されており，Leveneの検定などで事前に検証しておくべきである。またWithinデザインにおいては，データの組成に関わる分散共分散行列の非対角要素が全て等しいことが望ましいが，実践的にはそこまでの仮定が成立しているとは考えにくい。ただし分散分析としては，等分散性の仮定よりも，より緩やかな球面性の仮定が成立していればよいとされており，これを事前に検定することが一般的である。Welchの補正のように，球面性の仮定が成立していない場合は，自由度を補正することで検定の精度が維持される。\n分散分析は多要因・多水準の平均値差の検定である。各水準ごとにt検定を繰り返せば良いのではないか，というアイデアは誰しも思いつくことであろうが，この方法は検定の目的である\\(\\alpha\\)水準の制御ができなくなるという問題を含む。そこで多水準の場合は分散分析を行うことで，すべての要因・水準の母平均が同じであるという帰無仮説を検定し，効果の有無をまず明確にする。この帰無仮説が棄却されたらどこかに差があるわけだから，以後は慎重に\\(\\alpha\\)水準を制御しつつ事後的な検定にすすむ。\n水準間の差をみるための事後的な検定は，下位検定とも呼ばれる。その方法は多岐に渡り，ゴールドスタンダードは存在せず，往々にして分析者が利用しているソフトウェアが対応する手法が選択される。要因・水準が多くなると検証すべき組み合わせも多くなり，下位検定の手続きも非常に煩雑になる。統計ソフトウェアはそれこそ機械的に，幾重にも細かく分散分析表を分解して下位検定をつづけていってくれるが，いくら制御されているとはいえ検定を繰り返していることに変わりはないし，各下位検定の結果を一貫した総合的解釈をするのは困難である。実験計画はシンプルであるほうが望ましいし，複雑なモデルになるようであれば分散分析を超えた，階層線型モデルやベイジアンアプローチなどを取る方が良いだろう。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#anova君を使う",
    "href": "chapter09.html#anova君を使う",
    "title": "9  多群の平均値差の検定",
    "section": "9.3 ANOVA君を使う",
    "text": "9.3 ANOVA君を使う\n分散分析をRで実行するには，基本関数であるaovやcarパッケージなどを用いることができる。 もっとも，その出力は必ずしも親切ではないし，下位検定や効果量の算出などは別のパッケージ，別の関数を用いる必要がある。\n筆者がお勧めするのは，大正大学の井関龍太が開発したanovakunである。パッケージ化されていないので，リンク先からソースコードを読み込んでanovakun関数を実行する必要があるが，さまざまな実験デザインに対応し，また下位検定や効果量，球面性の補正などおよそ分散分析で必要な手法は網羅されている。以下ではこれを用いた実践を行う。\nanovakunの読み込みは，ソースコードをプロジェクトフォルダにダウンロードしてsource関数で読み込むか，インターネットに繋がっている状態でリンク先から直接ソースファイル(anovakun_489.txt)1をsource関数で読み込むといいだろう。\n\nsource(\"https://riseki.cloudfree.jp/?plugin=attach&refer=ANOVA%E5%90%9B&openfile=anovakun_489.txt\")\n\n読み込みが終わるとEnvironタブにanovakun関数が含まれていることを確認しよう。\n\n9.3.1 ANOVA君の入力とデータ\nANOVA君は伝統的にワイド型データから読み込むようになっている。すなわち，一行に1オブザベーション入っている形式である。Between計画の場合は，データの前に水準数を表すインデックスと最終的な従属変数の形に整形したデータが必要である。Within計画の場合は1行に1Obs.なのだから，反復した水準の数だけ右にデータを入れていく形に整形する。\nしかしChapter 3.7 で述べたように，昨今は計算機にとって優しい型，ロング型での入力もおおく，ANOVA君もversion 4.4.0からロング型での入力も許すようになった。その場合はオプションlong=TRUE とロング型であることを明記する必要がある。\nANOVA君を使う時は，関数anovakunに，データ，要因計画の型，各要因の水準の順で入力する。ここで要因計画の型とは，文字列でBetween/Withinの違いを明示することになる。被験者のラベルを表す小文字のsを挟んで，左側に間(Between)要因，右側に内(Within)要因を入れる。例えば一要因Between計画の場合は\"As\"，二要因Within計画の場合は\"sAB\"，間1内2の混合計画であれば\"AsBC\"のようにする。\n続いて入力する水準数は，要因の数だけ必要である。ただし，ロング型で入力した場合は自動的に水準数が計算されるので入力の必要がない。\nこのテキストでは，データの持ち替えについてすでに触れているので，色々扱いやすいロング型に整形して利用していくものとする。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#betweenデザイン",
    "href": "chapter09.html#betweenデザイン",
    "title": "9  多群の平均値差の検定",
    "section": "9.4 Betweenデザイン",
    "text": "9.4 Betweenデザイン\n\n9.4.1 1way-ANOVA\nもっとも単純な一要因3水準，Between計画の例から始めよう。仮想データの生成を行うことで，分散分析のメカニズムと共に見ていくことにする。\n\nset.seed(123)\n# 各群のサンプルサイズ\nn1 &lt;- 5\nn2 &lt;- 4\nn3 &lt;- 6\n# 母平均，効果量，母SD\nmu &lt;- 10\ndelta &lt;- 1\nsigma &lt;- 3\n# 群平均\nmu1 &lt;- mu - (delta * sigma)\nmu2 &lt;- mu\nmu3 &lt;- mu + (delta * sigma)\n# データセット\nX1 &lt;- rnorm(n1, mu1, sigma)\nX2 &lt;- rnorm(n2, mu2, sigma)\nX3 &lt;- rnorm(n3, mu3, sigma)\n## 組み上げる\ndat &lt;- data.frame(\n  ID = 1:(n1 + n2 + n3),\n  group = as.factor(rep(LETTERS[1:3], c(n1, n2, n3))),\n  value = c(X1, X2, X3)\n)\n## データの確認\ndat\n\n   ID group     value\n1   1     A  5.318573\n2   2     A  6.309468\n3   3     A 11.676125\n4   4     A  7.211525\n5   5     A  7.387863\n6   6     B 15.145195\n7   7     B 11.382749\n8   8     B  6.204816\n9   9     B  7.939441\n10 10     C 11.663014\n11 11     C 16.672245\n12 12     C 14.079441\n13 13     C 14.202314\n14 14     C 13.332048\n15 15     C 11.332477\n\n### 実行\nanovakun(dat, \"As\", long = TRUE, peta = TRUE)\n\n\n[ As-Type Design ]\n\nThis output was generated by anovakun 4.8.9 under R version 4.5.1.\nIt was executed on Wed Sep 17 18:01:53 2025.\n\n \n&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;\n\n------------------------------\n group   n     Mean    S.D. \n------------------------------\n     A   5   7.5807  2.4331 \n     B   4  10.1681  3.9548 \n     C   6  13.5469  1.9483 \n------------------------------\n\n\n&lt;&lt; ANOVA TABLE &gt;&gt;\n\n== This data is UNBALANCED!! ==\n== Type III SS is applied. ==\n\n--------------------------------------------------------------\n Source       SS  df      MS  F-ratio  p-value      p.eta^2 \n--------------------------------------------------------------\n  group  98.3840   2 49.1920   6.5897   0.0117 *     0.5234 \n  Error  89.5804  12  7.4650                                \n--------------------------------------------------------------\n  Total 187.9644  14 13.4260                                \n                  +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\n\n&lt;&lt; POST ANALYSES &gt;&gt;\n\n&lt; MULTIPLE COMPARISON for \"group\" &gt;\n\n== Shaffer's Modified Sequentially Rejective Bonferroni Procedure ==\n== The factor &lt; group &gt; is analysed as independent means. == \n== Alpha level is 0.05. == \n \n------------------------------\n group   n     Mean    S.D. \n------------------------------\n     A   5   7.5807  2.4331 \n     B   4  10.1681  3.9548 \n     C   6  13.5469  1.9483 \n------------------------------\n\n-------------------------------------------------------\n Pair     Diff  t-value  df       p   adj.p          \n-------------------------------------------------------\n  A-C  -5.9662   3.6062  12  0.0036  0.0108  A &lt; C * \n  B-C  -3.3789   1.9159  12  0.0795  0.0795  B = C   \n  A-B  -2.5873   1.4117  12  0.1834  0.1834  A = B   \n-------------------------------------------------------\n\n\noutput is over --------------------///\n\n\n出力結果は大きく分けて記述統計&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;と，分散分析表&lt;&lt; ANOVA TABLE &gt;&gt;，下位検定&lt;&lt; POST ANALYSES &gt;&gt;に分けられる。記述統計はデータが正しく読み込めているかどうかのチェックに使おう。\n一番のメインは分散分析表であり，平方和sum of squaresを自由度dfで割った，1自由度あたりのデータの散らばりを，群間と群内(誤差)との比で検証しているのが見て取れる。群間平方和が\\(98.38\\)，群内平方和が\\(89.58\\)であり，それぞれ自由度\\(2\\)(\\(3\\)水準\\(-1\\))と\\(12\\)(\\(\\sum_{j=1}^3 n_j-1\\))から生じているので，平均平方Mean Squaresがそれぞれ\\(49.19\\)と\\(7.47\\)である。この比が\\(6.5897\\)で，自由度\\(F(2,12)\\)のF分布においてこの値以上の極端な数字が出る確率が5%を下回っている(実に\\(p=0.0117\\)である)ため，統計的に有意であると判断できる。 分散分析表のTotalのところで，全体のSSが群間SS+群内SSに一致していること，自由度も全体df=群間df+群内dfになっていることを確認しておこう。\nまた，anovakun関数の引数としてpeta = TRUEを指定したが，これは偏\\(\\eta^2\\)(partial eta)と呼ばれる効果量を出力するためのオプションである。\n今回は分散分析の時点で統計的な有意差が認められたため(\\(F(2,12)=6.59, p &lt; 0.05, \\eta^2=0.52\\))，続いて下位検定が表示されている。ANOVA君は下位検定についても複数のオプションを持っているが，デフォルトではShafferの修正Bonferroni検定が行われる。詳しくは専門書(永田 and 吉田 1997)を参照してほしいが，概略を説明すると，検証すべき仮説の数で有意水準を分割するというBonferroniの方法を，競合する仮説の数も考慮して分母を調整するというものである。\nこの計算の結果，A群とC群の間にのみ統計的な有意差が確認された(\\(t(12)=3.61,p&lt;0.05\\))と言える。\n\n\n9.4.2 2way-ANOVA\n二要因の場合も見ておこう。ANOVA君の表記方法は要因計画の型が変わるだけで大きな変更はないが，交互作用interactionを考える必要があるところがポイントである。これも仮想データの組成を見ることでその意義がわかりやすくなるだろう。間2\\(\\times\\)間2の実験デザインを例に，まずは各水準の理論的平均値がどのようにつくられるかをみておこう。\n\nset.seed(123)\n# 各群のサンプルサイズ\nn &lt;- 10\n# 全体平均，効果量，母SD\nmu &lt;- 10\ndelta1 &lt;- 1\ndelta2 &lt;- 0 # ここではあえて要因Bの効果を0にしている\ndelta3 &lt;- 2\nsigma &lt;- 3\n# 効果の計算\neffectA &lt;- delta1 * sigma # Factor A\neffectB &lt;- delta2 * sigma # Factor B\neffectAB &lt;- delta3 * sigma # interaction\n# 各群の平均\nmu11 &lt;- mu + effectA + effectB + effectAB\nmu12 &lt;- mu + effectA - effectB - effectAB\nmu21 &lt;- mu - effectA + effectB - effectAB\nmu22 &lt;- mu - effectA - effectB + effectAB\n\n効果の現れ方は相対的だから，要因Aが第一水準に+effectAの形で現れたら，第二水準には-effectA の形で現れる。要因Bについても同様である。交互作用については組み合わせにおいて生じるから，要因Aの第一水準と要因Bの第一水準の組み合わせのところに+effectABを充てる。ここでも効果は相対的に現れるという条件を守るために，要因Aの第一水準の中で+effectABの効果を相殺するために，要因Aの第一水準と要因Bの第二水準の組み合わせの符号が反転する。同様に，要因Bの第一水準の中で相殺するために要因Aの第二水準と要因Bの第一水準には-effectABが加わる。\nこのようにして考えられる理論的平均値に対して，外乱要因である誤差が生じて実現値が得られる。 組み上げて得られたデータを確認しておこう。\n\nX11 &lt;- rnorm(n, mean = mu11, sd = sigma)\nX12 &lt;- rnorm(n, mean = mu12, sd = sigma)\nX21 &lt;- rnorm(n, mean = mu21, sd = sigma)\nX22 &lt;- rnorm(n, mean = mu22, sd = sigma)\ndat &lt;- data.frame(\n  ID = 1:(n * 4),\n  FactorA = rep(1:2, each = n * 2),\n  FactorB = rep(rep(1:2, each = n), 2),\n  value = c(X11, X12, X21, X22)\n)\ndat\n\n   ID FactorA FactorB      value\n1   1       1       1 17.3185731\n2   2       1       1 18.3094675\n3   3       1       1 23.6761249\n4   4       1       1 19.2115252\n5   5       1       1 19.3878632\n6   6       1       1 24.1451950\n7   7       1       1 20.3827486\n8   8       1       1 15.2048163\n9   9       1       1 16.9394414\n10 10       1       1 17.6630141\n11 11       1       2 10.6722454\n12 12       1       2  8.0794415\n13 13       1       2  8.2023144\n14 14       1       2  7.3320481\n15 15       1       2  5.3324766\n16 16       1       2 12.3607394\n17 17       1       2  8.4935514\n18 18       1       2  1.1001485\n19 19       1       2  9.1040677\n20 20       1       2  5.5816258\n21 21       2       1 -2.2034711\n22 22       2       1  0.3460753\n23 23       2       1 -2.0780133\n24 24       2       1 -1.1866737\n25 25       2       1 -0.8751178\n26 26       2       1 -4.0600799\n27 27       2       1  3.5133611\n28 28       2       1  1.4601194\n29 29       2       1 -2.4144108\n30 30       2       1  4.7614448\n31 31       2       2 14.2793927\n32 32       2       2 12.1147856\n33 33       2       2 15.6853770\n34 34       2       2 15.6344005\n35 35       2       2 15.4647432\n36 36       2       2 15.0659208\n37 37       2       2 14.6617530\n38 38       2       2 12.8142649\n39 39       2       2 12.0821120\n40 40       2       2 11.8585870\n\n\nもちろん実際には，計画に応じたデータセットが得られているはずであり，各群のサンプルサイズが異なるなどの事情もあるだろう。しかしこうして，理論的にデータの組成を見ておくことで，サンプルサイズを変えたり効果量を変えたりしながら，どのように結果が変わってくるかを確認しながら進めることができる2。\nそれではこの仮想データを分析してみよう。\n\nanovakun(dat, \"ABs\", long = TRUE, peta = TRUE)\n\n\n[ ABs-Type Design ]\n\nThis output was generated by anovakun 4.8.9 under R version 4.5.1.\nIt was executed on Wed Sep 17 18:01:53 2025.\n\n \n&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;\n\n-----------------------------------------\n FactorA  FactorB   n     Mean    S.D. \n-----------------------------------------\n       1        1  10  19.2239  2.8614 \n       1        2  10   7.6259  3.1142 \n       2        1  10  -0.2737  2.7924 \n       2        2  10  13.9661  1.5819 \n-----------------------------------------\n\n\n&lt;&lt; ANOVA TABLE &gt;&gt;\n\n---------------------------------------------------------------------------\n           Source        SS  df        MS  F-ratio  p-value      p.eta^2 \n---------------------------------------------------------------------------\n          FactorA  432.7854   1  432.7854  61.4190   0.0000 ***   0.6305 \n          FactorB   17.4478   1   17.4478   2.4761   0.1243 ns    0.0644 \nFactorA x FactorB 1668.9825   1 1668.9825 236.8545   0.0000 ***   0.8681 \n            Error  253.6721  36    7.0464                                \n---------------------------------------------------------------------------\n            Total 2372.8878  39   60.8433                                \n                               +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\n\n&lt;&lt; POST ANALYSES &gt;&gt;\n\n&lt; SIMPLE EFFECTS for \"FactorA x FactorB\" INTERACTION &gt;\n\n----------------------------------------------------------------------\n      Source        SS  df        MS  F-ratio  p-value      p.eta^2 \n----------------------------------------------------------------------\nFactorA at 1 1900.7730   1 1900.7730 269.7492   0.0000 ***   0.8823 \nFactorA at 2  200.9950   1  200.9950  28.5243   0.0000 ***   0.4421 \nFactorB at 1  672.5693   1  672.5693  95.4480   0.0000 ***   0.7261 \nFactorB at 2 1013.8610   1 1013.8610 143.8826   0.0000 ***   0.7999 \n       Error  253.6721  36    7.0464                                \n----------------------------------------------------------------------\n                          +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\noutput is over --------------------///\n\n\n基本的な結果の見方については，一要因のときと同じである。今回は要因Aと交互作用の効果を作り，正しく検出されている。下位検定については，要因Aが2水準であったためこちらの主効果の検証は必要なく(記述統計を見て群平均比較をすればよい)，交互作用についての単純効果の検証が行われている。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#withinデザイン",
    "href": "chapter09.html#withinデザイン",
    "title": "9  多群の平均値差の検定",
    "section": "9.5 Withinデザイン",
    "text": "9.5 Withinデザイン\nWithinデザインは対応のあるt検定の時と同じように，多次元正規分布からの生成として考えよう。すなわち各個体から得られるデータが相関しているという仮定をおくのである。以下のサンプルコードを読んで，データ生成過程を確認しよう。なお共分散は\\(\\rho_{xy}=\\frac{s_{xy}}{s_xs_y}\\)より\\(s_{xy}=\\rho_{xy}s_xs_y\\)として整形している。\n\npacman::p_load(tidyverse)\npacman::p_load(MASS)\nset.seed(42)\n# 各群のサンプルサイズ\nn &lt;- 10\n# 全体平均，効果量，母SD\nmu &lt;- 10\ndelta &lt;- 1\ns1 &lt;- s2 &lt;- s3 &lt;- 1\nrho12 &lt;- 0.1\nrho13 &lt;- 0.3\nrho23 &lt;- 0.8\nmus &lt;- c(mu, mu + s1 * delta, mu - s1 * delta)\n# 共分散行列の生成\nSigma &lt;- matrix(NA, ncol = 3, nrow = 3)\nSigma[1, 1] &lt;- s1^2\nSigma[2, 2] &lt;- s2^2\nSigma[3, 3] &lt;- s3^2\nSigma[1, 2] &lt;- Sigma[2, 1] &lt;- rho12 * s1 * s2\nSigma[1, 3] &lt;- Sigma[3, 1] &lt;- rho13 * s1 * s3\nSigma[2, 3] &lt;- Sigma[3, 2] &lt;- rho23 * s2 * s3\n# データの生成\nX &lt;- mvrnorm(n, mus, Sigma) %&gt;% as.data.frame()\n# データの確認\nX\n\n          V1        V2       V3\n1  10.625304  9.418518 7.493325\n2  12.437964 11.249993 8.806719\n3   8.604481 11.182418 8.722798\n4   9.390742 10.181310 8.786312\n5   9.567609 10.147592 9.194809\n6  10.651739 11.005419 8.917299\n7   9.125913  9.805634 7.511082\n8   7.770294 12.462671 8.790231\n9   6.909722  9.863405 7.429485\n10 11.267590 10.798088 8.754522\n\n# Long型に整形\nX &lt;- X %&gt;%\n  rowid_to_column(\"ID\") %&gt;%\n  pivot_longer(-ID) %&gt;%\n  print()\n\n# A tibble: 30 × 3\n      ID name  value\n   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1 V1    10.6 \n 2     1 V2     9.42\n 3     1 V3     7.49\n 4     2 V1    12.4 \n 5     2 V2    11.2 \n 6     2 V3     8.81\n 7     3 V1     8.60\n 8     3 V2    11.2 \n 9     3 V3     8.72\n10     4 V1     9.39\n# ℹ 20 more rows\n\n# 分析の実行\nanovakun(X, \"sA\", long = TRUE, peta = TRUE, GG = TRUE)\n\n\n[ sA-Type Design ]\n\nThis output was generated by anovakun 4.8.9 under R version 4.5.1.\nIt was executed on Wed Sep 17 18:01:54 2025.\n\n \n&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;\n\n-----------------------------\n name   n     Mean    S.D. \n-----------------------------\n   V1  10   9.6351  1.6609 \n   V2  10  10.6115  0.9057 \n   V3  10   8.4407  0.6777 \n-----------------------------\n\n\n&lt;&lt; SPHERICITY INDICES &gt;&gt;\n\n== Mendoza's Multisample Sphericity Test and Epsilons ==\n\n-------------------------------------------------------------------------\n Effect  Lambda  approx.Chi  df      p         LB     GG     HF     CM \n-------------------------------------------------------------------------\n   name  0.0068      8.8720   2 0.0118 *   0.5000 0.5988 0.6392 0.5547 \n-------------------------------------------------------------------------\n                              LB = lower.bound, GG = Greenhouse-Geisser\n                             HF = Huynh-Feldt-Lecoutre, CM = Chi-Muller\n\n\n&lt;&lt; ANOVA TABLE &gt;&gt;\n\n--------------------------------------------------------------\n  Source      SS  df      MS  F-ratio  p-value      p.eta^2 \n--------------------------------------------------------------\n       s 16.4609   9  1.8290                                \n--------------------------------------------------------------\n    name 23.6422   2 11.8211  10.7022   0.0009 ***   0.5432 \ns x name 19.8819  18  1.1045                                \n--------------------------------------------------------------\n   Total 59.9849  29  2.0684                                \n                  +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\n\n&lt;&lt; POST ANALYSES &gt;&gt;\n\n&lt; MULTIPLE COMPARISON for \"name\" &gt;\n\n== Shaffer's Modified Sequentially Rejective Bonferroni Procedure ==\n== The factor &lt; name &gt; is analysed as dependent means. == \n== Alpha level is 0.05. == \n \n-----------------------------\n name   n     Mean    S.D. \n-----------------------------\n   V1  10   9.6351  1.6609 \n   V2  10  10.6115  0.9057 \n   V3  10   8.4407  0.6777 \n-----------------------------\n\n----------------------------------------------------------\n  Pair     Diff  t-value  df       p   adj.p            \n----------------------------------------------------------\n V2-V3   2.1708   9.5342   9  0.0000  0.0000  V2 &gt; V3 * \n V1-V3   1.1945   2.3896   9  0.0406  0.0406  V1 &gt; V3 * \n V1-V2  -0.9764   1.6250   9  0.1386  0.1386  V1 = V2   \n----------------------------------------------------------\n\n\noutput is over --------------------///\n\n\n上記コードについていくつか解説をしておこう。 今回，各群の分散は同じにしつつ，変数間相関に大きな違いを持たせた。あえて球面性の仮定が成立しないような例を得たかったからで，出力の&lt;&lt; SPHERICITY INDICES &gt;&gt;をみると統計量\\(\\lambda\\)のあとの\\(p\\)値が5%を下回っており，「球面性が成立している」という帰無仮説が棄却されていることがわかる。この時いくつかの補正法があるが，今回はGreenhouse-Geisserの補正を当てることにしている。それがanovakun関数のなかのGG=TRUEの箇所である。\nこれを踏まえて分散分析表が示されている。ここでも全体平方和SS，自由度dfが各行の要素の和になっていることが確認できるが，その因子名のところにsが含まれていることが確認できる。これが個体ごとの変動を表しており，誤差から個人差を取り除いて効果の検証ができていることがわかる。\n分散分析は加法的，線型的な分解であるからわかりやすく，要因が複雑に組み合わさることがあっても基本的に今回のパーツを組み上げることで理解できる。言い換えると，データが先にある実践的な場合には平方和をひとつひとつ丁寧に紐解いていくことで理解できる。実にanovakunの前進であるanova4では4要因，anovakunでは26要因までのデザインを分析することが可能である。もっとも4要因計画にもなると3次の交互作用まで考えられ，主効果と合わせてこれらの交互作用効果を解釈するのは困難である。anoakunは2次以上の交互作用が見られた場合，自動的に下位検定を行ってくれないので，要因の水準ごとにデータを分割して，分散分析表を解体しつつ分析する必要がある。3\nしかしもちろん，これには検定の多重性の問題が関わってくるから，あまり推奨される手法ではない。ごく少ない要因で，主効果の有無を検証することを主眼においた丁寧な実験デザインを組み立てることを試みるべきである。\nまたここでは，仮想データを作ることで，得られたデータの背後にある生成メカニズムに注目した。「与えられたデータを分解する」のが分散分析であるのに対し，リバースエンジニアリングからアプローチしたのである。こうすることで，分散分析の見えない仮定に注意が向くことを期待している。簡便のために，いくつかのパラメータを均質化するなどしたが，実践的には群ごとのサンプルサイズが異なることも少なくないだろうし，群間の分散や共分散が均質であることを前提とするのは難しいだろう。これを考慮した細かい作り込みも，リバースエンジニアリングによって生成メカニズムがわかっている場合には応用が可能である。さらに，どこの水準間にどのような効果があると仮定されるか，といった精緻な仮説があるのなら，そこだけをターゲットにした分析を行うことも可能である。\n分散分析は，あくまでも大雑把な全体的傾向を見るためのものであることに留意しよう。心理学のデータがより精緻な仮定に耐えうる精度を持つものになれば，分散分析は過去の遺物となるかもしれない。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#課題",
    "href": "chapter09.html#課題",
    "title": "9  多群の平均値差の検定",
    "section": "9.6 課題",
    "text": "9.6 課題\n\n以下のデータセットは一要因4水準Between計画で得られたものです。分散分析を行って，要因の効果があるかどうか，水準間に差があるとすればどこに見られるかを報告してください。なおこのデータセットはこちらex_anova1.csvからダウンロード可能です。\n\n\n\n   ID group value\n1   1     A 14.37\n2   2     A 15.11\n3   3     A 16.11\n4   4     A 11.17\n5   5     A 14.51\n6   6     A  7.85\n7   7     A 10.65\n8   8     B 16.45\n9   9     B 11.76\n10 10     B 19.11\n11 11     B 19.62\n12 12     C  2.92\n13 13     C  6.27\n14 14     C  1.82\n15 15     C -0.10\n16 16     C  5.30\n17 17     C  1.57\n18 18     D  8.33\n19 19     D  2.71\n20 20     D  5.97\n21 21     D  4.97\n22 22     D  1.65\n23 23     D  8.73\n24 24     D  5.93\n25 25     D  4.27\n\n\n\n以下のデータセットは一要因4水準Within計画で得られたものです。分散分析を行って，要因の効果があるかどうか，水準間に差があるとすればどこに見られるかを報告してください。なおこのデータセットはこちらex_anova2.csvからダウンロード可能です。\n\n\n\n      V1    V2    V3    V4\n1  11.32 12.99  9.34 -0.14\n2  10.77 13.84 14.74  3.52\n3   9.86 12.26 12.56  2.60\n4   8.74 11.59 14.27  0.68\n5  11.12 12.93 12.92  1.13\n6   9.65 16.55 12.60  2.32\n7   9.72 14.64  9.69 -1.34\n8  12.02 11.18 14.43  2.64\n9  10.00 10.79  9.19 -1.09\n10 10.04 15.53 13.38  1.82\n11 10.20 11.56 11.02 -0.05\n12  7.81  9.29 12.20 -3.25\n\n\n\n間(3)\\(\\times\\)間(3)の分散分析モデルの仮想データセットを作りましょう。そのデータに分散分析を適用し，仮定した要因の効果がみられるか(あるいは効果がないと仮定した場合に正しく検出されないか)を確認しましょう。\n【発展課題】二要因混合計画分散分析(間\\(\\times\\)内)の仮想データセットを作りましょう。そのデータに分散分析を適用し，仮定した要因の効果がみられるか(あるいは効果がないと仮定した場合に正しく検出されないか)を確認しましょう。\n\n\n\n\n\n永田靖, and 吉田道弘. 1997. 統計的多重比較法の基礎. サイエンティスト社.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#footnotes",
    "href": "chapter09.html#footnotes",
    "title": "9  多群の平均値差の検定",
    "section": "",
    "text": "2024.03.17時点での最新バージョンが4.8.9である。リンク先URLは，公式サイトからソースファイルのリンクをコピーして貼り付けると良い。↩︎\nかつては分散分析は手計算でできる分析モデルであり，得られたデータを平方和に分解していくプロセスをたどりながら分散分析のメカニズムが体得されるという教育が多く見られた。ただしその方法は計算に時間がかかること，ミスが混在しやすいことに加え，手元のデータが唯一無二のものであるという印象を強くすることが懸念される。推測統計学においては，手元のデータはあくまでも実現値に過ぎないと考えるのであり，乱数を生成して幾つでも自在に作り出せる経験を得た方が教育効果として良いのではないか，と筆者は考えている。↩︎\nanovakunの補助関数anovatanを用いることで，注目したい要因ごとにデータを分割してくれる。詳しくは公式サイトのマニュアルを参照してほしい。↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter10.html",
    "href": "chapter10.html",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "10.1 疑わしき研究実践 Questionable Research Practicies\nここまでシミュレーションを通じて仮想データを生成し，帰無仮説検定のステップをリバースエンジニアリングしながら検定を「モデル」の観点から確認してきた。\nシミュレーションは仮想世界を作ることであり，いかようにもデータを作ることができるのだから，たとえば実践的にタブーとされていることを仮想的に検証してみることができる。このアプローチで，QRPsが具体的にどのように問題になるのかを体験してみよう。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#疑わしき研究実践-questionable-research-practicies",
    "href": "chapter10.html#疑わしき研究実践-questionable-research-practicies",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "10.1.1 検定の繰り返し\n帰無仮説検定は確率を伴った判断なので，「差がないのにあると判断してしまった(タイプ1エラー)」とか，「差があるのに検出できなかった(タイプ2エラー)」といった問題が生じうる。すでに述べたように，タイプ2エラーの方は本質的に知り得ないので(差がどの程度あるか，事前にわかっていることがない)，せめてタイプ1エラーは制御することを目指すことになる。\nこうした検定は合理的に行われるべきもので，なんとか有意に「したい」といった研究者のお気持ちとは独立しているはずである。しかし(もしかすると)意図せぬところで，この制御に失敗してしまっている可能性がある。\nひとつは検定の繰り返しに関する問題である。たとえば分散分析において，「主効果が出てから下位検定で各ペアの検証をするんだから，最初から各ペアのt検定を繰り返せばいいじゃないか」と考える人がいるかもしれない。これで本当に問題ないのか，シミュレーションで確認してみよう。\n以下のコードは，有意差のないデータセットを作り，1.分散分析を行なって有意になるかどうか，2.各ペアについて繰り返しt検定を行い，どこかに有意差が検出されるかどうか，を比較している。分散分析はANOVA君ではなく，R固有のaov関数を用いた1。また，「どこかに有意差が検出される」をif文を使って書いているところを，注意深く確認しておいてほしい。\n\npacman::p_load(tidyverse)\npacman::p_load(broom) # 分析結果をtidyに整形するパッケージ。ない場合はinstallしておこう\n\n\nalpha &lt;- 0.05 # 有意水準を0.05に設定\nn1 &lt;- n2 &lt;- n3 &lt;- 10 # 各グループのサンプルサイズを10に設定\nmu &lt;- 10 # 平均値を10に設定\nsigma &lt;- 2 # 標準偏差を2に設定\n\nmu1 &lt;- mu2 &lt;- mu3 &lt;- mu # 各グループの平均値を同じに設定\n\nset.seed(12345) # 乱数のシードを設定して再現性を確保\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\n\nanova.detect &lt;- rep(NA, iter) # ANOVA検出結果の保存用ベクトルを初期化\nttest.detect &lt;- rep(NA, iter) # t検定検出結果の保存用ベクトルを初期化\n\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  X1 &lt;- rnorm(n1, mu1, sigma) # グループ1のデータを生成\n  X2 &lt;- rnorm(n2, mu2, sigma) # グループ2のデータを生成\n  X3 &lt;- rnorm(n3, mu3, sigma) # グループ3のデータを生成\n\n  dat &lt;- data.frame( # データフレームを作成\n    group = c(rep(1, n1), rep(2, n2), rep(3, n3)), # グループ番号を追加\n    value = c(X1, X2, X3) # データを追加\n  )\n  result.anova &lt;- aov(value ~ group, data = dat) %&gt;% tidy() # ANOVAを実行し結果を整形\n  anova.detect[i] &lt;- ifelse(result.anova$p.value[1] &lt; alpha, 1, 0) # 有意差があるかを判定して保存\n\n  # t検定を繰り返す\n  ttest12 &lt;- t.test(X1, X2)$p.value # グループ1と2のt検定\n  ttest13 &lt;- t.test(X1, X3)$p.value # グループ1と3のt検定\n  ttest23 &lt;- t.test(X2, X3)$p.value # グループ2と3のt検定\n\n  ttest.detect[i] &lt;- ifelse(ttest12 &lt; alpha | ttest13 &lt; alpha | ttest23 &lt; alpha, 1, 0) # いずれかのt検定で有意差があれば保存\n}\n\nttest.detect %&gt;% mean() # t検定で有意差が検出された割合を計算\n\n[1] 0.109\n\nanova.detect %&gt;% mean() # ANOVAで有意差が検出された割合を計算\n\n[1] 0.04\n\n\n結果を見ると，t検定で有意差が検出された確率が0.109であり，設定した\\(\\alpha\\)水準を大きく上回っていることがわかる。有意でないところに有意差を見出しているのだから，これはタイプ1エラーのインフレである。分散分析で検出された結果は0.04であり，正しく\\(\\alpha\\)水準がコントロールできている。\n検定を繰り返すことの問題は，確率的判断にある。5%の水準でタイプ1エラーが起こるということは，95%の確率で正しく判断できるということだが，2回検定を繰り返すとその精度は\\((1-0,05)^2=0.9025\\)であり，3回検定を繰り返すと\\((1-0.05)^3=0.857375\\)と，どんどん小さくなっていってしまう。検定はタイプ1エラーのハンドリングが目的であったことを忘れてはならない。\n\n\n10.1.2 ボンフェロー二の方法\n一つの論文のなかに複数の研究(Study1, Study2,…)があり，それぞれで検定による確率的判断を行っているとしよう。それぞれ別のデータセットに対する検定であっても，一つの露文の中で確率的判断が繰り返されていることに違いはない。このような場合は，どのようにして有意水準をコントロールすれば良いのだろうか。\n最も単純明快な方法のひとつは，分散分析の下位検定でもみられたBonferroniの補正である。すなわち，検定の回数で有意水準を割ることで，検定を厳しくするのである。5%水準の検定を5回繰り返すのなら，\\(0.05/5=0.01\\)とすることで全体的なタイプ1エラー率を抑制するのである。これが正しく機能するかどうか，シミュレーションで確認してみよう。\n反復してデータを生成することになるので，仮想データ生成関数を別途事前に準備しておこう。\n\n# シミュレーション用の関数を定義\nstudyMake &lt;- function(n, mu, sigma, delta) {\n  X1 &lt;- rnorm(n, mu, sigma) # グループ1のデータを生成\n  X2 &lt;- rnorm(n, mu + sigma * delta, sigma) # グループ2のデータを生成（平均値が異なる）\n  dat &lt;- data.frame( # データフレームを作成\n    group = rep(1:2, each = n), # グループ番号を追加\n    value = c(X1, X2) # データを追加\n  )\n  result &lt;- t.test(X1, X2)$p.value # グループ間のt検定を実行\n  return(result) # p値を返す\n}\n\nこの関数は，引数としてサンプルサイズn，平均値mu，標準偏差sigma，効果量deltaをとり，2群のt検定の結果である\\(p\\)値を返す関数である。\n\n# 使用例；t検定の結果のp値が戻ってくる\nstudyMake(n = 10, mu = 10, sigma = 1, delta = 0)\n\n[1] 0.9444895\n\n\nこれ一回で1分析するので，これを複数回行って一つの研究とし，一つの論文のなかでnum_studies回の研究を行ったとしよう。今回はnum_studies = 3としている。Rのreplicate関数で研究回数繰り返した\\(p\\)値ベクトルを得て，どこかに差が検出されるかどうかをチェックする。「どこかに」を表現するためにany関数を使って判定する。判定する有意水準として，\\(\\alpha\\)と補正をかけた\\(\\alpha_{adj}\\)の2つを用意した。\n\nset.seed(12345) # 乱数のシードを設定して再現性を確保\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\nalpha &lt;- 0.05 # 有意水準を0.05に設定\nnum_studies &lt;- 3 # 研究の数を3に設定\nalpha_adjust &lt;- alpha / num_studies # 多重検定補正後の有意水準を計算\n\nFLG.detect &lt;- rep(NA, iter) # 検出結果を保存するベクトルを初期化\nFLG.detect.adj &lt;- rep(NA, iter) # 補正後の検出結果を保存するベクトルを初期化\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  p_values &lt;- replicate(num_studies, studyMake(n = 10, mu = 10, sigma = 1, delta = 0)) # 各研究のp値を生成\n  FLG.detect[i] &lt;- ifelse(any(p_values &lt; alpha), 1, 0) # 補正前の有意差検出を判定して保存\n  FLG.detect.adj[i] &lt;- ifelse(any(p_values &lt; alpha_adjust), 1, 0) # 補正後の有意差検出を判定して保存\n}\n\nFLG.detect %&gt;% mean() # 補正前の有意差検出率を計算\n\n[1] 0.145\n\nFLG.detect.adj %&gt;% mean() # 補正後の有意差検出率を計算\n\n[1] 0.049\n\n\n結果を見ると，\\(\\alpha\\)水準のまま検定を行うと，論文全体でのタイプ1エラー率が0.145と5%を上回っており，3つの研究のどこかで間違った判断をしていることがわかる。補正すると0.049と正しく制御されている。\n一連の研究をまとめた一つの論文に，複数の研究が含まれていることは少なくない。各検定結果をまとめて総合考察とすることも一般的である。総合考察は各分析結果から全体的な結論を導くのだが，その要素のどこかに間違いがあると，全体の論立てが崩れてしまうことにもなりかねない。いわば腐った支柱が紛れ込んでいる土台の上に家屋を建てるようなもので，研究の積み重ねを目的とする科学活動の一環である以上，正しく制御されていることは重要である。\n\n\n10.1.3 N増し問題\n人間を対象にした研究を行って，データを一生懸命取る。その結果，効果があると見られた操作/介入から統計的な有意差が検出されなければ，「悔しい」という心情になることは理解できる。もう少し実験を工夫すれば，もう少しデータが違えばよかったのでは，と思うかもしれない。ではもう少し頑張ってデータを増やしてみればどうだろうか。\n実はこの考え方はQRPsのひとつである。検定は真偽判定をする競技のようなものなので，ゲームの途中でプレイヤーの人数が変わるのはよろしくない。このことをシミュレーションで確認してみよう。\n以下のコードは，t検定のデータを最初n1=n2=10で作成して行っている。タイプ1エラーの検証をするので，効果量は\\(0\\)である。ここでt検定を行い，もしその\\(p\\)値が\\(\\alpha\\)よりも大きかったら，つまり有意であると判断されなかったら，同じ方法でデータを1件追加する。そしてまたt検定を行う。このサンプルの追加は，効果量\\(0\\)なので，偶然のお許しが出るまでいつまで経っても終わることがないため，上限を100にしてある。上限に達したら流石に諦めてもらうとして，さてそうしたQRPsな努力の結果，\\(\\alpha\\)水準はどれぐらいに保たれているだろうか。\n\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\nalpha &lt;- 0.05 # 有意水準を0.05に設定\np &lt;- rep(0, iter) # p値を保存するベクトルを初期化\nadd.vec &lt;- rep(0, iter) # 増やした人数を保存するベクトルを初期化\n\nset.seed(123) # 乱数のシードを設定して再現性を確保\n\nn1 &lt;- n2 &lt;- 10 # 各グループのサンプルサイズを10に設定\nmu &lt;- 10 # 平均値を10に設定\nsigma &lt;- 2 # 標準偏差を2に設定\ndelta &lt;- 0 # 平均の差を0に設定\n\n## シミュレーション本体\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  # 最初のデータを生成\n  Y1 &lt;- rnorm(n1, mu, sigma) # グループ1のデータを生成\n  Y2 &lt;- rnorm(n2, mu + sigma * delta, sigma) # グループ2のデータを生成\n  p[i] &lt;- t.test(Y1, Y2)$p.value # t検定を実行しp値を保存\n  # データを追加する\n  count &lt;- 0 # 追加したデータの数をカウント\n  ## p値が5%を下回るか、データが100になるまでデータを増やし続ける\n  while (p[i] &gt;= alpha && count &lt; 100) { # 条件を満たすまでループを繰り返す\n    # 有意でなかった場合、変数ごとに1つずつデータを追加\n    Y1_add &lt;- rnorm(1, mu, sigma) # グループ1に新しいデータを1つ追加\n    Y2_add &lt;- rnorm(1, mu + sigma * delta, sigma) # グループ2に新しいデータを1つ追加\n    Y1 &lt;- c(Y1, Y1_add) # グループ1のデータを更新\n    Y2 &lt;- c(Y2, Y2_add) # グループ2のデータを更新\n    p[i] &lt;- t.test(Y1, Y2)$p.value # 新しいデータでt検定を再度実行しp値を更新\n    count &lt;- count + 1 # データを追加した回数をカウント\n  }\n  add.vec[i] &lt;- count\n}\n\n## 結果\nifelse(p &lt; 0.05, 1, 0) |&gt; mean() # p値が5%未満の割合を計算\n\n[1] 0.306\n\nhist(p)\n\n\n\n\n\n\n\nhist(add.vec)\n\n\n\n\n\n\n\n\n結果をみると，0.306とかなり逸脱して，誤った結論に辿り着いていることがわかる。努力の結果得られた有意差は，偶然の賜物でもあり，誤った研究実践による幻想にすぎない。加えたデータのヒストグラムからわかるように，悲しいかな，75%もの割合で上限100まで達してしまう。百害あって一利なしとはこのことである。\n\n\n10.1.4 サンプルサイズを事前に決めないことの問題\nサンプルサイズを事前に決めずに検定する，という状況を別の角度から見てみよう。クルシュケ ([2014] 2017) は「コインフリップを24回して，うち7回表が出た」というシーンを例に挙げて説明している。7/24は半分を下回っているから，やや裏が出やすいコインであるように思える。帰無仮説として，このコインは公平である(表と裏が出る確率が半々である)，というのを検証したいとする。\nこの24回中7回成功，という話の背後に「24施行する」ということを決めていたかどうか(サンプルサイズを事前に決めていたか)ということを考えてみよう。\nまずは正直に，最初から24回コインフリップすることを決めていたとする。コインフリップはベルヌーイ試行2 であり，それを繰り返すので二項分布に従うと考えられる。そこで，二項検定として次のように計算できるだろう。\n\nN &lt;- 24\n# 7回表が出る確率\npbinom(7, N, 0.5) * 2\n\n[1] 0.06391466\n\n\n二項分布のp値を出すにはpbinomを使いった。また帰無仮説として，このコインフリップは公平であると考えているのだから，\\(\\theta=0.5\\)が帰無仮説の状態である。この\\(\\theta=0.5\\)とした時に，\\(N=24,k=7\\)という結果になる確率を計算し，かつ両側検定(公平でない，が対立仮説なので裏が7回でもよい)であることを考えて確率を2倍した。p値は0.0639147であるから，5%水準では有意であると判定できない。これぐらいの確率はあるということだ。\nしかしここで第二の状況を考えてみよう。24回コインフリップすることを決めていたのではなくて，7回成功するまでコインフリップを続けたところ，結果的に24回で終わったということだった，とするのである。このようなシーンの確率分布は負の二項分布と呼ばれ，pnbinomで次のように計算できる。\n\nk &lt;- 7\n# 24回以上必要な確率\npnbinom(k, 24, 0.5) * 2\n\n[1] 0.003326893\n\n\nこの結果から，\\(\\theta=0.5\\)の時に7回表がでるまでに24施行も必要とする確率は，0.0033269だから，5%水準で有意である。つまり，滅多にこんなことが起きないので，\\(\\theta=0.5\\)という帰無仮説が疑わしいことになる。ここではシーンが異なるとp値が違っている，ということに注意してほしい。\nさらに第3のシーンを考えよう。これは「何回やるかは決めてないけど，まあ5分ぐらいかな」と試行にかける時間だけ決めていたという状況である。結果的に24回になったけど，もしかすると23回だったかもしれないし，25回や20回，30回だったかもしれない。これをシミュレーションするために，「24がピークになるような頻度の分布」をポアソン分布を使って生成する3。\n4 ポアソン分布は正の整数を実現値に取る分布で，カウント変数の確率分布として用いられる。パラメータは\\(\\lambda\\)だけであり，期待値と分散が\\(\\lambda\\)に一致する，非常にシンプルな分布である。\n\nset.seed(12345)\niter &lt;- 100000 # 発生させる乱数の数\n## 24回がピークに来るトライアル回数\ntrial &lt;- rpois(iter, 24)\nhist(trial)\n\n\n\n\n\n\n\n\nこの各トライアルにおいて，二項分布で成功した回数を計算し，トライアル回数で割ることによって，表が出る確率のシミュレーションができる。その時の割合は，\\(7/24\\)よりもレアな現象だろうか?\n\nresult &lt;- rep(NA, iter)\nfor (i in 1:iter) {\n  result[i] &lt;- rbinom(1, trial[i], 0.5) / trial[i]\n}\n## 7/24よりも小さい確率で起こった?\nlength(result[result &lt; (7 / 24)]) / iter\n\n[1] 0.02262\n\n\nこれを見ると，両側検定にしても0.04524なので，ギリギリ有意になるかどうか，というところだろうか。\nさて判断にこまった。「24回やる」と決めていたのであれば\\(\\theta=0.5\\)は棄却されないし，「7回成功するまで」と決めていたのであれば\\(\\theta=0.5\\)は棄却される。「5分間」と決めていても棄却されるが，そもそもこうした実験者の意図によって判断が揺らいで良いものだろうか?というのが クルシュケ ([2014] 2017) の指摘する疑問点である。\n問題は，「24回中7回成功」という事実に，二項分布，負の二項分布，あるいは組み合わさった分布のような，確率分布の情報が含まれていないことにある。この確率分布はデータが既知で母数が未知だから尤度関数であり，データ生成メカニズムであるとも言えるだろう。想定するメカニズムが明示されない検定は，ともすれば事後的に「実は負の二項分布を想定していたんですよ，へへ」ということも可能になってしまう。こうした点からも，研究者の自由度をなるべく少なくする研究計画の事前登録制度が必要であることがわかる。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#サンプルサイズ設計",
    "href": "chapter10.html#サンプルサイズ設計",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "10.2 サンプルサイズ設計",
    "text": "10.2 サンプルサイズ設計\nどのようにデータを取り，どのように分析・検定し，どのような基準で判断するかを事前に決めることに加え，事前にサンプルサイズを見積もっておく必要があるだろう。サンプルはとにかく多ければ多いほど良いか，というとそうではなく，過剰にサンプルを集めることは研究コストの増大であり，回答者の負担増でしかない。またサンプルサイズが大きくなると有意差を検出しやすくなるが，必要なのは有意差ではなく実質的に効果を見積もることであり，有意差が見つかれば良いというものではないことに注意が必要である。もちろん上で見てきたように，有意差が検出できるかどうかを指標にしてサンプルサイズを変動させてしまうのは，明らかに誤った研究実践である。\n事前にサンプルサイズを決定するのに必要なのは，これまでのリバースエンジニアリングの演習例からもわかるように，効果量の見積もりである。5 これをどのように定めるかについては，先行研究を考えるとか，研究領域で「これぐらい差がないと意味がないよね」とコンセンサスが取れている程度で決めることになる。6\n\n10.2.1 対応のないt検定\nサンプルサイズの設計には，これまで使ってきた検定統計量に非心度non-centrality parameterというパラメータを加えて考える必要がある。\n具体的に，対応のないt検定を例にサンプルサイズ設計の方法を見てみよう。t検定は言葉の通り，t分布を用いて帰無仮説の元での検定統計量の実現値が問題になるのであった。帰無仮説は\\(\\mu_0 = \\mu_1-\\mu_2 = 0\\)であり，検定統計量は次式で表されるのであった。\n\\[ T = \\frac{d - \\mu_0}{\\sqrt{U^2_p/\\frac{n_1n_2}{n_1+n_2}}}\\]\nこの分子において，\\(d - \\mu_0 = (\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)\\)の第二項を,\\(\\mu_1-\\mu_2 = 0\\)と仮定するから，\\(0\\)が中心の標準化されたt分布が用いられたのである。帰無仮説はこのように理論的に特定できる比較点をもとに置かれているのであって，帰無仮説下ではない現実の世界では，検定統計量は母平均の差\\(\\mu_1 - \\mu_2\\)に応じた分布から生じている。このように中心がずれているt分布のことを非心分布といい，ズレの程度が非心度パラメータである。t検定における非心度\\(\\lambda\\)は，以下の式で表される。\n\\[ \\lambda = \\frac{(\\mu_1-\\mu2) - \\mu_0}{\\sigma\\sqrt{n}} \\]\nこの非心度の分だけ，非心t分布はt分布からズレていることになる。Rではdt関数にncpパラメータがあり，デフォルトではncp=0になっていた。これを変えて描画してみよう。\n\n# データの準備\ndf &lt;- 10 # 自由度を指定\n# ggplotでプロット\nggplot(data.frame(x = c(-5, 5)), aes(x = x)) +\n  stat_function(fun = dt, args = list(df = df, ncp = 0), aes(color = \"ncp=0\")) +\n  stat_function(fun = dt, args = list(df = df, ncp = 3), aes(color = \"ncp=3\")) +\n  labs(\n    title = \"非心t分布\",\n    x = \"x\",\n    y = \"密度\",\n    color = \"ncp\"\n  )\n\n\n\n\n\n\n\n\nncp=0の時は，中心が0にある帰無仮説の世界であり，これを使ってタイプ1エラー，つまり\\(\\alpha\\)が算出されたのであった。ncpを効果量で表現すれば，母平均の差がゼロでない時の分布が描けるのだから，タイプ2エラー，つまり\\(\\beta\\)が計算できる。\n自由度df = 10，非心度ncp = 3の例で考えてみよう。 タイプ1エラーになるのは，自由度10のt分布で上2.5%の臨界値以上の実現値が得られた時である。\n\nqt(0.975, df = 10, ncp = 0)\n\n[1] 2.228139\n\n\nこのとき，実際はncp = 3ほどずれていたのだから，タイプ2エラーが生じる確率は次のとおりである。\n\nqt(0.975, df = 10, ncp = 0) %&gt;% pt(df = 10, ncp = 3)\n\n[1] 0.2285998\n\n\n当然，ncp = 0から離れるほどタイプ2エラーは生じにくくなる。非心度は母効果量\\(\\delta = \\frac{(\\mu_1 - \\mu_2)-\\mu_0}{\\delta}\\)を使って，\\(\\lambda = \\delta \\sqrt{n}\\)で表すことができる。\nこれを使って，t検定のサンプルサイズを設計してみよう。話を簡単にするために，サンプルサイズは2群で等しいものとする。\n検定統計量の式を思い出して，\\(\\sqrt{n}\\)にあたるところは2群のサンプルサイズから計算される，プールされた標本サイズから得られることに注意しよう7。\n\nalpha &lt;- 0.05\nbeta &lt;- 0.2\ndelta &lt;- 0.5\n\nfor (n in 10:1000) {\n  df &lt;- n + n - 2\n  lambda &lt;- delta * (sqrt((n * n) / (n + n)))\n  cv &lt;- qt(p = 1 - alpha / 2, df = df) # Type1errorの臨界値\n  er &lt;- pt(cv, df = df, ncp = lambda) # Type2errorの確率\n  if (er &lt;= beta) {\n    break\n  }\n}\nprint(n)\n\n[1] 64\n\n\nここでは，サンプルサイズを10から徐々に増やしていき，1000までの間で目標とする\\(\\beta\\)まで抑えられたところで，カウントしていくforループをbreakで脱出する，というかたちで組んでいる。結果的に，各群64名，合計128名のサンプルがあれば，目標が達成できることがわかる。サンプルサイズが2群で異なる場合など，詳細は@kosugi2023 に詳しい。\n\n\n10.2.2 シミュレーションによるサンプルサイズ設計\n非心F分布を使えば分散分析でもサンプルサイズができるし，そのほかの検定についても同様に非心分布を活用すると良い。しかし，非心分布の理解や非心度の計算など，ケースバイケースで学ぶべきことは多い。\nそこで，電子計算機の演算力をたのみに，データ生成のシミュレーションを通じて設計していくことを考えてみよう。サンプルサイズや効果量を定めれば，仮想データを作ることができるし，それ対して検定をかけることもできる。仮想データの生成と検定を反復し，タイプ2エラーがどの程度生じるかを相対度数で近似することもできるだろう。であれば，その近似をサンプルサイズを徐々に変えることで繰り返してサンプルサイズを定めることもできる。\n以下は，母相関が\\(\\rho = 0.5\\)とした時に，検出力が80%になるために必要なサンプルサイズを求めるシミュレーションコードである。\n\npacman::p_load(MASS)\nset.seed(12345)\nalpha &lt;- 0.05\nbeta &lt;- 0.2\nrho &lt;- 0.5\nsd &lt;- 1\nSigma &lt;- matrix(NA, ncol = 2, nrow = 2)\nSigma[1, 1] &lt;- Sigma[2, 2] &lt;- sd^2\nSigma[1, 2] &lt;- Sigma[2, 1] &lt;- sd * sd * rho\n\niter &lt;- 1000\n\nfor (n in seq(from = 10, to = 1000, by = 1)) {\n  FLG &lt;- rep(0, iter)\n  for (i in 1:iter) {\n    X &lt;- mvrnorm(n, c(0, 0), Sigma)\n    cor_test &lt;- cor.test(X[, 1], X[, 2])\n    FLG[i] &lt;- ifelse(cor_test$p.value &gt; alpha, 1, 0)\n  }\n  t2error &lt;- mean(FLG)\n  print(paste(\"n=\", n, \"のとき，betaは\", t2error, \"です。\"))\n  if (t2error &lt;= beta) {\n    break\n  }\n}\n\n[1] \"n= 10 のとき，betaは 0.681 です。\"\n[1] \"n= 11 のとき，betaは 0.639 です。\"\n[1] \"n= 12 のとき，betaは 0.612 です。\"\n[1] \"n= 13 のとき，betaは 0.566 です。\"\n[1] \"n= 14 のとき，betaは 0.563 です。\"\n[1] \"n= 15 のとき，betaは 0.471 です。\"\n[1] \"n= 16 のとき，betaは 0.462 です。\"\n[1] \"n= 17 のとき，betaは 0.419 です。\"\n[1] \"n= 18 のとき，betaは 0.402 です。\"\n[1] \"n= 19 のとき，betaは 0.385 です。\"\n[1] \"n= 20 のとき，betaは 0.353 です。\"\n[1] \"n= 21 のとき，betaは 0.344 です。\"\n[1] \"n= 22 のとき，betaは 0.312 です。\"\n[1] \"n= 23 のとき，betaは 0.285 です。\"\n[1] \"n= 24 のとき，betaは 0.256 です。\"\n[1] \"n= 25 のとき，betaは 0.265 です。\"\n[1] \"n= 26 のとき，betaは 0.21 です。\"\n[1] \"n= 27 のとき，betaは 0.227 です。\"\n[1] \"n= 28 のとき，betaは 0.176 です。\"\n\nprint(n)\n\n[1] 28\n\n\nここではシミュレーション回数1000，上限1000，刻み幅を1にしているが，状況に応じて変更すると良い。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#課題",
    "href": "chapter10.html#課題",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "10.3 課題",
    "text": "10.3 課題\n\n一要因3水準のBetweenデザインの分散分析において、1.分散分析で有意差が見られる場合と、2.任意の2水準の組み合わせのどこかで有意差が見られる場合を考えたとき、タイプ1エラーはどのように異なるかをシミュレーションで確かめてみましょう。設定として、n1=n2=n3=10、標準偏差も各群等しくsigma = 1としてみましょう。\nN増し問題は相関係数の検定の時も生じるでしょうか。母相関が\\(\\rho = 0.0\\)のとき、サンプルサイズを10から始めて、有意になるまでデータを追加する仮想研究を1000回行ってみましょう。データ追加の上限は100、有意水準は\\(\\alpha = 0.05\\)として、最終的に有意になる割合を計算してみましょう。\n\\(\\alpha = 0.05, \\beta = 0.2\\)とし、効果量\\(\\delta = 1\\)とした時の対応のないt検定のサンプルサイズ設計をしたいです。1.非心t分布を使った解析的な方法と、2.シミュレーションによる近似的な方法の両方で、同等の結果が出ることを確認しましょう。\n\n\n\n\n\nクルシュケJ. K. (2014) 2017. ベイズ統計モデリング: R, JAGS, Stanによるチュートリアル 原著第2版. Translated by 前田和寛 and 小杉考司. 共立出版.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#footnotes",
    "href": "chapter10.html#footnotes",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "ANOVA君は結果をコンソールに直接出力し，戻り値を持たない。ここでは結果の\\(p\\)値が必要だったので，このようにした。↩︎\n表(1)が出るか，裏(0)が出るか，という2値の結果変数だけを持つ施行のことで，この確率変数がベルヌーイ分布にし違う。ベルヌーイ分布は，表が出る確率\\(\\theta\\)をパラメータに持つ。\\(P(X=k) = \\theta^k(1-\\theta)^{1-k},\\text{ただし}k=\\{0,1\\}\\)という確率変数である。1/0というのが生死，男女，成功失敗などさまざまなメタファに適用できるので応用範囲が広い。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nこの「最低限検出したい効果」のことをSmallest Effect Size of Interest, SESOIと呼ぶ。小杉, 紀ノ定, and 清水 (2023) も参照。↩︎\nt統計量の実現値の式にある分母，\\(\\sqrt{U_p^2/\\frac{n_1n_2}{n_1+n_2}}\\)に見られる，プールされた不偏分散を割るための標本サイズであり，2群の母分散が等しいと仮定して計算するなら，\\(\\sigma^2(\\frac{1}{n_1} + \\frac{1}{n_2}) = \\sigma^2(\\frac{n_1+n_2}{n_1n_2}) = \\sigma^2 / \\frac{n_1n_2}{n_1 + n_2}\\)から得られる。↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter11.html",
    "href": "chapter11.html",
    "title": "11  重回帰分析の基礎",
    "section": "",
    "text": "11.1 回帰分析の基礎\nここでは回帰分析を扱う。説明変数\\(x\\)と被説明変数\\(y\\)の関数関係\\(y=f(x)\\)に，次の一次式を当てはめるのが単回帰分析である。\n\\[ y_i  = \\beta_0 + \\beta_1 x_i + e_i = \\hat{y}_i + e_i\\]\n一次式を\\(\\hat{y}\\)とまとめたものを予測値といい，予測値と実測値\\(y\\)の差分\\(e_i\\)を残差residualsという。\n空間上の一次直線の切片，傾きを求めるというのが基本的な問題であり，二点であれば一意に定めることができるが，データ分析の場面では3点以上の多くのデータセットの中に直線を当てはめることになるので，なんらかの外的な基準が必要になる。この時，「残差の分散が最も小さくなるように」と考えるのが最小二乗法の考え方であり，「残差が正規分布に従っていると考え，その尤度が最も大きくなるように」と考えるのが最尤法の考え方である。前者は記述統計的な，後者は確率モデルとしての感が過多になっていることに注意してほしい。また確率モデルの推定方法としては，事前分布を用いたベイズ推定が用いられることもある。\n最小二乗法による推定値は，次の式で表される。証明は他書(小杉 2018; 西内 2017)に譲るが，ロジックとして残差の二乗和\\(\\sum e_i^2 = \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2\\)を最小にすることを考え，この式を展開するか偏微分を用いて極小値を求めることで算出できるとだけ伝えておこう。いずれにせよ，平均値\\(\\bar{x},\\bar{y}\\)や分散・共分散\\(s_x,s_y,r_{xy}\\)など標本統計量から推定できるのはありがたいことである。\n\\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x},\\quad \\beta_1 = r_{xy} \\frac{s_y}{s_x}\\]\nまた，ここでは\\(x,y\\)ともに連続変数を想定しているが，説明変数\\(x\\)が二値，あるいはカテゴリカルなものであれば\\(y\\)の平均値を通る直線を探すことになる。直線の傾きが0であれば「平均値が同じ」という線型モデルであり，これは平均値差の検定における帰無仮説と同等である。このように，t検定やANOVAは回帰分析の特殊ケースとも考えられ，まとめて一般線型モデルと呼ばれる。一般線型モデルは，被説明変数が連続的で，線型モデルによる平均値に正規分布に従う残差が加わったものとして考えるという意味で統一的に表現される。\nANOVAの場合は，二つ以上の要因による効果を考えることもあった。交互作用項を考慮しなければ，2要因のモデルは次のように表現することができる。\n\\[ y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + e_i \\]\nこのように説明変数が複数ある回帰分析を特に重回帰分析Multiple Regression Analysisと呼ぶ。一次式なので，ある変数に限れば線型性が担保されているから，これも線型モデルの仲間である。重回帰分析を用いる場合は，説明変数同士を比較して「どちらの説明変数の方が影響力が大きいか」ということが論じられることが多いが，係数は当然\\(x_n, y\\)の単位に依存するため，素点の回帰係数は使い勝手が悪い。そこですべての変数を標準化した標準化係数が用いられることが多い。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#回帰分析の特徴",
    "href": "chapter11.html#回帰分析の特徴",
    "title": "11  重回帰分析の基礎",
    "section": "11.2 回帰分析の特徴",
    "text": "11.2 回帰分析の特徴\n以下，具体的なデータを用いて回帰分析の特徴を見てみよう\n\n11.2.1 パラメータリカバリ\n回帰分析のモデル式にそってデータを生成し，分析によってパラメータリカバリを行ってみよう。\n説明変数については制約がないので一様乱数から生成し，平均0，標準偏差\\(\\sigma\\)の誤差とともに被説明変数を作る。\n\npacman::p_load(tidyverse)\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データの生成\nx &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x + e\n\ndat &lt;- data.frame(x, y)\n# データの確認\nhead(dat)\n\n          x          y\n1 -4.248450 -11.120952\n2  5.766103  18.736432\n3 -1.820462  -3.805302\n4  7.660348  25.071541\n5  8.809346  30.026546\n6 -9.088870 -25.355175\n\ndat %&gt;% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y~x\") # 線型モデルの描画\n\n\n\n\n\n\n\n\nこのデータに基づいて回帰分析を実行した結果が以下のとおりである。\n\nresult.lm &lt;- lm(y ~ x, data = dat)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82796 -0.61831  0.03553  0.69367  2.68062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.021928   0.045010   44.92   &lt;2e-16 ***\nx           3.002194   0.007919  379.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.006 on 498 degrees of freedom\nMultiple R-squared:  0.9965,    Adjusted R-squared:  0.9965 \nF-statistic: 1.437e+05 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\nここでは\\(\\beta_0 =2, \\beta_1=3, \\sigma = 1\\)と設定しており，ほぼ理論通りの係数がリカバリーできていることを出力から確認しておこう。 もちろんリカバリの精度は，データの線型性の強さに依存するから，残差の分散が大きかったりサンプルサイズが小さくなると，必ずしもうまくリカバリできないことがあることは想像に難くないだろう。\n\n\n11.2.2 残差の正規性と相関関係\nlm関数が返した結果オブジェクトには，表示されていない多くの情報が含まれている。例えば予測値や残差も含まれているので，これを使って回帰分析の特徴を見てみよう。\n\ndat &lt;- bind_cols(dat, yhat = result.lm$fitted.values, residuals = result.lm$residuals)\nsummary(dat)\n\n       x                  y                yhat            residuals       \n Min.   :-9.99069   Min.   :-28.216   Min.   :-27.9721   Min.   :-2.82796  \n 1st Qu.:-5.08007   1st Qu.:-13.074   1st Qu.:-13.2294   1st Qu.:-0.61831  \n Median :-0.46887   Median :  0.301   Median :  0.6143   Median : 0.03553  \n Mean   :-0.09433   Mean   :  1.739   Mean   :  1.7387   Mean   : 0.00000  \n 3rd Qu.: 4.65795   3rd Qu.: 15.963   3rd Qu.: 16.0060   3rd Qu.: 0.69367  \n Max.   : 9.98809   Max.   : 32.638   Max.   : 32.0081   Max.   : 2.68062  \n\n\n予測値\\(\\hat{y}\\)はfitted.valuesとして保存されている。これの平均値が被説明変数\\(y\\)の平均値に一致していることが確認できる。回帰分析は説明変数\\(x\\)を伸ばしたり(\\(\\beta_1\\)倍する)ズラしたり(\\(\\beta_0\\)を加える)しながら，被説明変数\\(y\\)に当てはめるのであり，位置合わせがなされた予測値の中心が被説明変数の中心と一致することは理解しやすいだろう1。\n次に，残差の平均が0になっていることも確認しておこう。これが0でない\\(c\\)であれば，回帰係数が常に\\(c\\)だけズレていることになるので，そのような系統的ズレは最適な線型の当てはめにおいて除外されているべきだからである2。\nまた，回帰分析において残差は正規分布に従うことが仮定されていた。これを検証するにはQ-Qプロットを見ると良い。\n\ndat %&gt;%\n  ggplot(aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\n\nQ-Qプロットとは2つの確率分布を比較するためのグラフであり，横軸には理論的分布の分位点が，縦軸に実データが並ぶもので，右上がりの直線上にデータが載っていれば分布に従っている，と判断するものである。直線から逸脱している点は理論的分布からの逸脱と考えられる。今回の結果はほとんどが正規分布の直線上にあることから，大きな逸脱がないことが認められる。\nデータ生成メカニズムによっては，被説明変数が二値的であったり，順序的であったり，カウント変数であったり，と正規分布がそぐわないものもあるだろう。そのようなデータに無理やり回帰分析を当てはめることは適切ではない。いかなる時もデータは可視化して，モデルを当てはめることの適切さをチェックすることを忘れたはならない。\nちなみに出力結果を直接plot関数に入れてもよい。ここから残差と予測値の相関関係や，Q-Qプロット，標準化残差のスケールロケーションプロット，レバレッジと標準化残差3 などがプロットされる。\n\nplot(result.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差と予測値のプロットからも想像できるが，両者の相関はゼロである。図で確認しておこう。\n\npacman::p_load(GGally) # 必要ならインストールしよう\nggpairs(dat)\n\n\n\n\n\n\n\n\nこの関係から明らかなように，残差は説明変数や予測値と相関しない4。説明変数と残差に相関関係があるとすると，説明変数でまだ説明できていない分散が残っていることになるし，予測値と残差に相関がないことは予測値が高いか低いかにかかわらず，残差が一様に分布していることを意味する。このことを踏まえて，重回帰分析の特徴を理解していこう。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#重回帰分析の特徴",
    "href": "chapter11.html#重回帰分析の特徴",
    "title": "11  重回帰分析の基礎",
    "section": "11.3 重回帰分析の特徴",
    "text": "11.3 重回帰分析の特徴\n重回帰分析においては，回帰係数は偏回帰係数partial regression coefficientsと呼ばれる。この「偏」の一文字が意味することを考えていこう。\n\n11.3.1 回帰係数と偏回帰係数\n単回帰分析の回帰係数は，説明変数\\(x\\)が一単位上昇した時の被説明変数の変化量，と解釈すればよい。これに対して重回帰分析の偏回帰係数を，「説明変数\\(x_1\\)が一単位上昇した時の被説明変数の変化量」とすることはできない。というのも，説明変数が複数(\\(x_2,x_3,\\ldots\\))あり，他の説明変数の次元についての変化を考慮していない変化量になっているからである。\n重回帰分析において，説明変数が完全に無相関で直交しているのであれば，\\(x_1\\)の変化と\\(x_2\\)の変化を独立して説明できるが，往々にしてそのようなことはない。偏回帰係数は当該変数以外の変動を統制した回帰係数である。\n上で単回帰係数において，説明変数と残差が相関しないことを確認した。言い換えれば，説明変数で説明でき分散は全て説明し尽くされており，残差は説明変数で説明できない被説明変数の分散，つまり説明変数の影響を除外した被説明変数の分散と考えることができる(被説明変数の分散=説明変数が説明する分散+残差の分散)。\nここで第二の変数\\(x_2\\)があったとする。第一の変数\\(x_1\\)で\\(y\\)を説明した残差\\(e_y\\)と，第一の変数で第二の変数を説明した残差\\(e_{x2}\\)との相関を偏相関partial correlationという。これは第一の変数\\(x_1\\)からの影響を両者から取り除いているので，\\(x_1\\)で統制した相関係数ということができる。偏相関は単純な相関が「見せかけの関係」でないことを検証するための重要な指標である。\n偏相関係数を計算してみよう。\n\npacman::p_load(MASS)\npacman::p_load(psych)\nSigma &lt;- matrix(c(1, 0.3, 0.5, 0.3, 1, 0.8, 0.5, 0.8, 1), ncol = 3)\nX &lt;- mvrnorm(1000, c(0, 0, 0), Sigma, empirical = TRUE) %&gt;% as.data.frame()\n## 相関行列\ncor(X)\n\n    V1  V2  V3\nV1 1.0 0.3 0.5\nV2 0.3 1.0 0.8\nV3 0.5 0.8 1.0\n\n## 回帰分析をして残差を求める\nresult.lm1 &lt;- lm(V2 ~ V1, data = X)\nresult.lm2 &lt;- lm(V3 ~ V1, data = X)\ncor(result.lm1$residuals, result.lm2$residuals)\n\n[1] 0.7867958\n\n## 偏相関を求めるR関数で確認\npsych::partial.r(X)[2, 3]\n\n[1] 0.7867958\n\n\n最後はpsychパッケージの偏相関行列を求める関数で検証した。確かに残差同士の相関係数が偏相関係数になっていることが確認できたと思う。\nそして，ここでは残差同士の相関係数として算出しているが，残差をつかった回帰分析の係数が偏回帰係数になるのである。このデータセットの第一変数を従属変数にした重回帰分析の結果から，これを確認してみよう。\n\nresult.mra &lt;- lm(V1 ~ V2 + V3, data = X)\n# 回帰係数を取り出す\nresult.mra$coefficients\n\n  (Intercept)            V2            V3 \n-5.218050e-17 -2.777778e-01  7.222222e-01 \n\n# 残差をつかって偏回帰係数を確認する\nresult.lm3 &lt;- lm(V1 ~ V3, data = X)\nresult.lm4 &lt;- lm(V2 ~ V3, data = X)\nresult.lm5 &lt;- lm(result.lm3$residuals ~ result.lm4$residuals)\n#\nresult.lm5$coefficients\n\n         (Intercept) result.lm4$residuals \n       -7.925711e-18        -2.777778e-01 \n\n\n重回帰分析の結果result.mraのV2からV1への回帰係数は-0.2778である。また，V3でV1,V2を統制した残差同士をつかい，回帰係数を求めた結果は-0.2778と，同じ値になっていることが確認できただろう。\nV3からV1への偏回帰係数も同様で，V2で両者を統制した残差同士による回帰係数になっている。このように，重回帰分析の回帰係数は，他の説明変数で統制した値になっており，日本語で説明するなら「他の変数の値が同じであると想定した条件つきの，当該変数の影響力」とでもいうべき値になっている。\nなぜこのような持って回った説明をするかというと，つい「条件付きの」という話を忘れて報告，解釈してしまうことが多いからで，吉田 and 村井 (2021) の論文での指摘は議論を呼んだのは記憶に新しい5。 たとえば今回の例でも，回帰係数が-0.2778であったのに対し，V1とV2の単相関が0.3 であったことを思い出そう。符号が反転しているため，解釈は真逆になってしまう。実際の単相関は正の関係であるから，条件付きであることを忘れて「V2は負の影響，V3は正の影響」と表現してしまうと，ミスリーディングなことになるからである。\nまた，豊田 (2017) は重回帰分析のこうした誤用を避けるために，独立変数を事前に直交化したデザインで行うコンジョイント分析の積極的な利用を提案している。我々が重回帰分析をうまく使いこなせないのであれば，そうした手法も有用であるだろう。\n\n\n11.3.2 多重共線性\n偏回帰係数の解釈が難しい理由の一つは，説明変数同士に相関関係がみられることにある。 特に，説明変数間の相関関係が高くなることは多重共線性Multicollinearityの問題という。この問題は，回帰係数の標準誤差がインフレを起こすことを指す。\n例えば先ほどの例で，説明変数V2とV3は相関係数\\(0.8\\)を持っていた。この時の回帰係数の標準誤差を確認しておこう。\n\nsummary(result.mra)\n\n\nCall:\nlm(formula = V1 ~ V2 + V3, data = X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.59118 -0.54717 -0.03692  0.55044  2.90735 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5.218e-17  2.690e-02   0.000        1    \nV2          -2.778e-01  4.486e-02  -6.192 8.65e-10 ***\nV3           7.222e-01  4.486e-02  16.100  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8507 on 997 degrees of freedom\nMultiple R-squared:  0.2778,    Adjusted R-squared:  0.2763 \nF-statistic: 191.7 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n標準誤差は0.0449であり，それほど大きくない標準誤差で問題がないようである。しかし両者の相関係数がより高くなり，一方が他方に線型的に従属してしまうと係数の推定値が不安定になるため，注意が必要である。\nこのインフレを確認するための指標がVariance Infration Factor: VIFである。Rではcarパッケージにあるvif関数に重回帰モデルを入れることでこの指標が算出される。一般にVIFが3，あるいは10を超えると多重共線性が生じており，解釈に注意が必要と言われている6。\n\npacman::p_load(car) # なければ入れておこう\nvif(result.mra)\n\n      V2       V3 \n2.777778 2.777778 \n\n\n幸い，今回の値はこれらの基準を下回っていたので許容範囲内である。\n\n\n11.3.3 変数の投入順序\n重回帰分析の場合は複数の説明変数があるが，これを投入するときに全ての変数を同時に投入するか，順番をつけて投入するかといった手法の違いがある。前者を強制投入法と呼ぶこともある。 順番をつけて投入する方法は，逐次投入と呼ばれる。この場合は，適合度指標などを参考に変数を追加あるいは削除して，適合度が統計的に有意に向上するかどうかを考えながら進めていく。\n重回帰係数の予測値\\(\\hat{y}\\)と，被説明変数\\(y\\)の相関係数\\(R_{y\\hat{y}}\\)は重相関係数と呼ばれ，予測がうまくいっているかどうかを表す適合度の一つである。相関係数なので\\(-1\\)から\\(+1\\)までの値を取りうるが，\\(-1\\)は逆に完全に合致していることになるので，この相関係数の符号は大して情報を持たない。そこでこれを二乗した\\(R_{y\\hat{y}}^2\\)を考える。これは決定係数とも呼ばれ，説明変数の分散のうち予測値の分散が占める割合を表している。7\n説明変数の逐次投入は，説明変数を持たないヌルモデルから一つずつ追加していくForward Selection，全ての変数を投入してから一つずつ減らしていくBackward Selectionがある。Forwardのほうは追加することによって\\(R^2\\)が有意に増加するか，Backwardのほうは削除しても有意に\\(R^2\\)が減らないか，を確認しながら進めることになる。この方法は手元のデータに最も適した説明変数のペアを選出できる方法ではあるが，検定を繰り返していることの問題と，手元のデータ以外に一般化する時の根拠の乏しさから，用いられないこともある。\n逐次投入法には別の観点からの手法もある。それが階層的回帰分析である。この手法は，重回帰分析における交互作用項の投入を検討する文脈で発展した。重回帰分析では，説明変数同士の相関がない，もしくは小さい方が望ましい。しかし，交互作用とは分散分析における組み合わせの効果を表すものであり，実験デザインによっては交互作用効果が重要な変動であることも少なくない。回帰分析と分散分析は，一般線型モデルという形で統一的に理解されるが，回帰分析でも連続的に変化する組み合わせの効果を考えることができる。交互作用があるということは説明変数間に相関があることを意味するため，回帰分析の大前提に抵触する可能性があり，その投入には慎重を期する必要がある。\nこうした文脈から，まずは要因の効果を投入し，次に交互作用項を投入してモデル適合度の有意な改善がみられるかどうかを検証する手順が推奨されている。この逐次投入法を特に階層的回帰分析と呼ぶ。ここでの「階層」とは，手順が重要度順に進められていることを意味し，データの特徴に関するものではないことに注意が必要である8。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#係数の標準誤差と検定",
    "href": "chapter11.html#係数の標準誤差と検定",
    "title": "11  重回帰分析の基礎",
    "section": "11.4 係数の標準誤差と検定",
    "text": "11.4 係数の標準誤差と検定\n\n11.4.1 係数の検定\nサンプルが母集団から得られた確率変数であるのだから，(偏)回帰係数もまた確率変数である。すなわち，サンプルが変わるごとに変化し，その揺らぎがある確率分布に従うと考えられる。これを確認するためには，データ生成過程をモデリングし，反復することで近似させて理解するのがいいだろう。\n\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データ生成関数\ndataMake &lt;- function(n, beta0, beta1, sigma) {\n  x &lt;- runif(n, -10, 10)\n  e &lt;- rnorm(n, 0, sigma)\n  y &lt;- beta0 + beta1 * x + e\n  dat &lt;- data.frame(x, y)\n  return(dat)\n}\n\n# 結果オブジェクトの準備\niter &lt;- 2000\nbeta0.est &lt;- rep(NA, iter)\nbeta1.est &lt;- rep(NA, iter)\n# simulation\nfor (i in 1:iter) {\n  sample &lt;- dataMake(n, beta0, beta1, sigma)\n  result.lm &lt;- lm(y ~ x, data = sample)\n  beta0.est[i] &lt;- result.lm$coefficients[1]\n  beta1.est[i] &lt;- result.lm$coefficients[2]\n}\n\ndata.frame(x = beta0.est) %&gt;% ggplot(aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\ndata.frame(x = beta1.est) %&gt;% ggplot(aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n図から明らかなように，回帰係数も確率的に分布する。ただしその平均は理論値に近似している。\n\nmean(beta0.est)\n\n[1] 1.999257\n\nmean(beta1.est)\n\n[1] 2.999798\n\n\nこの分布の幅が回帰係数の標準誤差である。\n\nsd(beta0.est)\n\n[1] 0.04580387\n\nsd(beta1.est)\n\n[1] 0.007659277\n\n\n回帰係数はt分布に従い，その自由度はサンプルサイズからモデルで用いる係数の数を引いたものになる。先ほどのヒストグラムを基準化し，理論分布を重ねて描画してみることで確認しておこう。\n\ndata.frame(x = beta1.est) %&gt;%\n  scale() %&gt;%\n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 100) +\n  stat_function(fun = function(x) dt(x, df = n - 2), color = \"red\", linewidth = 2)\n\n\n\n\n\n\n\n\nこのt分布を用いて，係数が0の母集団から得られたサンプルなのかどうかの検定が行われる。\n\n\n11.4.2 モデル適合度の検定\n一方で，出力の最後にはF統計量による検定も行われていたことを確認しておこう。次に示すのは重回帰分析の例である。\n\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 0\nbeta2 &lt;- 0\nsigma &lt;- 1\nx1 &lt;- runif(n, -10, 10)\nx2 &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + e\nsample &lt;- data.frame(y, x1, x2)\nresult.lm &lt;- lm(y ~ x1 + x2, data = sample)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = sample)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85235 -0.68275 -0.01436  0.67809  2.70488 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.996999   0.045263  44.120   &lt;2e-16 ***\nx1          -0.006453   0.007970  -0.810    0.418    \nx2          -0.003928   0.007795  -0.504    0.615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.012 on 497 degrees of freedom\nMultiple R-squared:  0.001893,  Adjusted R-squared:  -0.002124 \nF-statistic: 0.4713 on 2 and 497 DF,  p-value: 0.6245\n\n\n上の例では，統計量\\(F\\)が，自由度\\(F(\\) 2,497 \\()\\)のもとで，0.4713であり，統計的に有意ではないと判断される(p=0.6245,n.s.)。\nこれは重相関係数に対する検定であり，母集団においてモデル全体としての説明力が0である，という帰無仮説を検証しているものである。この有意性検定には，説明変数の数\\(p\\)，サンプルサイズ\\(n\\)，重相関係数\\(R^2\\)を用いて，以下の式で用いられる検定統計量Fを利用する(南風原 2014)。\n\\[ F= \\frac{R^2}{1-R^2}\\cdot\\frac{n-p-1}{p} \\]\nここで右辺第一項目はCohenの効果量(\\(f^2=\\frac{R^2}{1-R^2}\\)) といわれ，サンプルサイズ設計においてはこの指標が利用される。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#サンプルサイズ設計",
    "href": "chapter11.html#サンプルサイズ設計",
    "title": "11  重回帰分析の基礎",
    "section": "11.5 サンプルサイズ設計",
    "text": "11.5 サンプルサイズ設計\n重回帰分析のサンプルサイズ設計は，変数の効果の大きさ(回帰係数)が事前にわかっているのであれば，nを徐々に増やしていくシミュレーションによって行える。しかしそのようなケースは稀であり，実際には\\(R^2\\)の検定を用いて，ある効果量と検出力の下で，正しく検出できるサイズを算出することになる。\nサンプルサイズの算出には，非心F分布を用いる。この時の非心度は，効果量\\(f^2\\)に\\(n\\)をかけたものになる。これを使ってサンプルサイズ設計をする例は以下のとおりである。\n\nf2 &lt;- 0.15 # 効果量\nalpha &lt;- 0.05 # タイプ1エラー率\nbeta &lt;- 0.2 # タイプ2エラー率\np &lt;- 5 # 説明変数の数\n\nfor (n in 10:500) {\n  lambda &lt;- f2 * n\n  df1 &lt;- p\n  df2 &lt;- n - p - 1\n  cv &lt;- qf(p = 1 - alpha, df1, df2)\n  t2error &lt;- pf(q = cv, df1, df2, ncp = lambda)\n  if (t2error &lt; beta) {\n    break\n  }\n}\n\nprint(n)\n\n[1] 92\n\n\nこの設定では，n = 92以上であればモデルとして影響力がないとは言えない，ということがわかる。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#まとめ",
    "href": "chapter11.html#まとめ",
    "title": "11  重回帰分析の基礎",
    "section": "11.6 まとめ",
    "text": "11.6 まとめ\n重回帰分析は，人文社会科学で多用される技術ではあるが，技術が先行して理解が伴わないまま利用されているケースも少なくない。繰り返しになる点もあるが，以下に注意点をまとめておく。\n\n偏回帰係数の意味；重回帰分析における偏回帰係数は，ほかの変数を統制した上での値であり，あたかも各係数が独立直交しているかのように解釈するのは適切ではない。\n誤差の正規性；誤差は正規分布に従っているという仮定があり，二値データや整数しかとらないカウントデータなどに盲目的にモデルを適用してはならない。誤差の正規性が満たされているかどうかは，分析後にQ-Qプロットを用いて確認する。\n誤差の均質性；誤差はモデル全体にわたって同じ正規分布に従っているというのもモデルの仮定である。すなわち，独立変数に応じて誤差分散が変わるといった均質でないデータの場合は，正しく推定されない。誤差の均質性については，分析後のQ-Qプロットを用いて確認する。\n誤差間の独立性；誤差はモデル全体にわたって同じ正規分布から独立に生成されている(i.i.d)というのがモデルの仮定である。時系列データのように，誤差間に対応(自己回帰)がみられるデータの場合は回帰分析は適切な手法とならない。状態空間モデルなど，誤差間関係を適切にモデリングしたものを当てはめる必要がある。\nモデルの適切な定式化；モデルには被説明変数に影響を与えるすべての変数が正しく含まれている必要がある。例えば，影響を与えることがわかっている変数\\(X_o\\)を意図的に除外して分析をしたとする。そのモデルに含まれる変数\\(X_a\\)が被説明変数\\(y\\)に影響を与えていたとしても，\\(X_a\\)と\\(X_o\\)に相関があれば，\\(X_o\\)の影響力が\\(X_a\\)を通じて\\(y\\)に伝播するkから，\\(X_a\\)の影響力が課題に評価されることになる。自らの仮説のために，意図的に変数を選択するのはQRPsに該当する。\n説明変数間の相関関係；説明変数のうちにあまりにも相関関係が高い変数ペアがあれば，多重共線性の疑いが生じる。多重共線性は推定値の不安定さとなって現れる。このような場合は，説明変数を主成分分析で合成変数にまとめるといった対応が考えられる。また，高い相関ではないが交互作用効果が見たいといった場合は，逐次投入など慎重に個々の影響を考えながら投入するようにする(階層的重回帰分析)。なお交互作用項は，各変数の平均からの偏差をかけ合わせたものにすることが一般的である。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#課題",
    "href": "chapter11.html#課題",
    "title": "11  重回帰分析の基礎",
    "section": "11.7 課題",
    "text": "11.7 課題\n\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。画面には一部しか表示しておらず，全体(\\(n=100\\))はこちらex_regression1.csvからダウンロード可能です。このデータセットを用いて重回帰分析を行い，結果を出力してください。\n\n\n\n           y        x1         x2\n1  1.8685595 -4.248450  1.9997792\n2 -0.5728781  5.766103 -3.3435292\n3  1.0321850 -1.820462 -0.2277393\n4 10.0468488  7.660348  9.0894765\n5 -1.1968078  8.809346 -0.3419521\n6  9.6719213 -9.088870  7.8070044\n\n\n\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。画面には一部しか表示しておらず，全体(\\(n=300\\))はこちらex_regression2.csvからダウンロード可能です。このデータセットを用いて重回帰分析を行ってください。結果のプロットから，上に挙げた重回帰分析の仮定に反しているところを指摘してください。\n\n\n\n          y         x1           x2\n1  3.586304 -0.4248450  0.132767341\n2  8.599252  0.5766103  0.922713561\n3  2.397115 -0.1820462  0.053684622\n4  3.505236  0.7660348  0.007801881\n5  6.517720  0.8809346  0.633076091\n6 -1.394231 -0.9088870 -0.895346802\n\n\n\n\\(R^2=0.3\\)を目標として，説明変数の数\\(p=10\\)の重回帰分析を行う際に，必要なサンプルサイズはいくつになるか，計算してみましょう。ここで，\\(\\alpha = 0.05,\\beta=0.2\\)とします。\n\n\n\n\n\n南風原朝和. 2014. 心理統計学の基礎: 続・統合的理解のために. 有斐閣.\n\n\n吉田寿夫, and 村井潤一郎. 2021. “心理学的研究における重回帰分析の適用に関わる諸問題.” 心理学研究 92 (3): 178–87. https://doi.org/10.4992/jjpsy.92.19226.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房. http://ci.nii.ac.jp/ncid/BB27527420.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.\n\n\n西内啓. 2017. 統計学が最強の学問である[数学編]: データ分析と機械学習のための新しい教科書. ダイヤモンド社.\n\n\n豊田秀樹. 2017. もうひとつの重回帰分析. 東京図書.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#footnotes",
    "href": "chapter11.html#footnotes",
    "title": "11  重回帰分析の基礎",
    "section": "",
    "text": "もちろん証明できる。\\(\\beta_0 = \\bar{y} - \\beta_1\\bar{x},\\beta_1 = r_{xy} \\frac{s_y}{s_x}\\)より，\\(\\bar{\\bar{y}} = \\frac{1}{n}\\sum(\\bar{y} - \\beta_1\\bar{x} + \\beta_1x_i) = \\bar{y} - \\beta_1\\bar{x} + \\beta_1\\frac{1}{n}\\sum x_i = \\bar{y}\\)である。↩︎\nもちろん証明できる。\\(\\bar{e} = \\frac{1}{n}\\sum e_i = \\frac{1}{n} \\sum (y_i - \\hat{y}_i) = \\bar{y} = \\bar{\\hat{y}} = 0\\)である。↩︎\n縦軸の標準化された残差の大きな値は解釈に注意が必要な外れ値である可能性が高い。レバレッジも同様に回帰係数に大きな影響を与える値の指標であり，この図の端に位置する変数は注意が必要，と考える。↩︎\nもちろん証明できる。詳細は 小杉 (2018) を参照すること。↩︎\n論文が早期公開された後，心理学会が主催するオンラインシンポジウムでは著者とこの論文で取り上げられた論文の著者が登場して，議論が交わされた(日本心理学会YouTubeライブ・話題の論文について著者と語るシリーズ,2021年7月2日20時-21時40分)。平日の夜という設定，早期公開版における議論であったにも関わらず，1700名近い視聴者が参加した。↩︎\n2変数重回帰分析モデルで，VIFが3であれば説明変数間の相関は\\(r=0.81\\)程度である。VIFが10であれば\\(r=0.97\\)にもなる。詳しくは 小杉, 紀ノ定, and 清水 (2023) を参照。↩︎\nもちろん証明できる。小杉 (2018) を参照。↩︎\nこれに対して，データの階層性(ex.学級\\(\\subset\\)市区町村\\(\\subset\\)都道府県)を考慮する線型モデルのことを，階層線型モデルHierarchical Linear Model:HLM という。↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter12.html",
    "href": "chapter12.html",
    "title": "12  ベイズ統計の活用",
    "section": "",
    "text": "12.1 ベイズ統計の位置付け\nここまで心理統計の基本的な話題をRで実践しながら見てきたが，ここでベイズ統計の話題を導入しよう。 わざわざこのように断る理由は，ベイズ統計学が推測統計学の枠組みの一種であるにもかかわらず，その歴史や解釈によって様々な誤解が生じているからである。一説にはベイズ統計学には100を超える派閥・解釈のスタイルがあるとも言われており，迂闊に議論をすることが憚られるような言説が交わされることも少なくない。\n筆者はファッションベイジアンを自認しており，主義主張を戦わせることはあまり好ましいことではないと考えている。そのような調子の良い立場の人間が語ることである，という事前情報を十分理解した上で，以下の解説を読んでほしい。\nベイズ統計学の生まれは古く，ベイズの定理にまで遡れば実に300年近く前になる。ベイズの定理を見出したとされるトーマス・ベイズは，18世紀の牧師であり，1763年に死んでいる。その古い人物の名を冠した定理が，現在は「ベイズ統計学」と学問全体を覆うほどになっているのは，それがこれまで紹介してきた統計モデルと考え方や解釈の仕方を大幅に変えているからである。\nまた，ベイズ統計学はその長い歴史に対して，応用的な価値が見出されたのが比較的最近である。筆者の感覚で言えば，2010年以降になって，ベイズ統計学の応用が急速に広まったように思う。古くから知られていた理論が長らくその名をひそめていたのは，ベイズ統計学を応用できるシーンが限定的であったこと，その研究が軍事的な機密を含むことで公開されることが少なかったこと，そして冗談のように聞こえるかもしれないが，アメリカ統計学会の重鎮がベイズ統計学を嫌っていたこと，がその理由と考えられる。このあたりの事情については(シャロン・バーチュ・マグレイン [2011] 2018)を参考にしてほしい。\nそして近年になって改めてベイズ統計学が注目されるようになったのには，2つの理由がある。第一は社会心理学における再現性の危機に対する対応として，従来の統計学ではない新しいアプローチとして期待されたこと。第二は計算機科学の発展により，非常にパワフルな推定方法が開発され，統計モデルを非常に柔軟に扱うことができるようになったこと，である。\n再現性の危機に対する対応としてのベイズ統計学は，従来の統計学を「頻度主義的」なものとして，ベイズ統計学を「ベイズ主義的」なものとして区別することで，その違いを強調するきらいがある。しかし，頻度主義的な方法に問題があるからという単純な理由でベイズ主義的な方法に乗り換えると，そもそもの問題であった統計法の悪用や誤用の種類が変わるだけに過ぎない。\n筆者は教育的観点から，ベイズ主義的な統計学の方が初学者には優しく，より理解しやすいのではないかと考えている。しかし，心理学のこの100年の歴史の中で積み重ねられてきた「頻度主義的」な研究のお作法は，誤用悪用を招く恐れもあるとは言え，非常にリッチな教育コンテンツ，分析ツールを提供してきた。またこれまでの心理学的文献が「頻度主義的」なものであったことを考えると，これらを捨て去って心機一転，一気にベイズに乗り換えようというのは現実的ではない。ベイズ統計学の弱点としては，教育コンテンツ，分析ツール，そしてベイズ統計学の教育者がそもそも少ないこと，が挙げられる。もちろんこれらの問題点が，近い将来のうちに解決されることを望むものである。\n第二の理由はベイズ統計学のポジティブな未来を予感させる。統計モデルが複雑になるにつれ，最尤推定法は実質的な限界を迎えるときでも，ベイズ推定の新しいアプローチは対応できる。これは，ベイズ統計学のパワーを具現化する計算機手法，すなわちMCMC法の発展によるところが大きい。統計モデルを非常に柔軟に，自らの研究データにカスタマイズした分析モデルを構築でき，その他のモデルともベイズファクターという一元的な指標で評価できることは，ベイズ統計学の大きな魅力である。しかし反面，細かく統計モデルをカスタマイズできるということは，研究者にプログラマとしての技量を要求することになる。心理学者はあくまでも統計をツールとして使いたいユーザなのだから，ソフトウェア的なエンジニアリングにあまりエフォートをかけていられないということもあるだろう。しかし，平均値の差の検定や要因計画にとらわれない，自由なモデリングができる魅力はおおきく，心理学の中でも統計モデリングによるアプローチをとる人も年々増加傾向にある。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter12.html#mcmc法",
    "href": "chapter12.html#mcmc法",
    "title": "12  ベイズ統計の活用",
    "section": "12.2 MCMC法",
    "text": "12.2 MCMC法\nベイズ統計を広く応用できるようにした，革新的な技術であるMCMC法についてみていこう。その前に，ベイズの定理を使った考え方について基本的な説明をしておく。\n\n12.2.1 ベイズの定理\nベイズの定理は，条件付き確率の定理である。ある事象\\(A\\)が起きたときに，事象\\(B\\)が起きる確率を\\(P(B|A)\\)と表すと，ベイズの定理は以下のように表される。\n\\[\nP(B|A) = \\frac{P(A|B)P(B)}{P(A)}\n\\]\nここで，\\(P(A)\\)は事象\\(A\\)が起きる確率，\\(P(B)\\)は事象\\(B\\)が起きる確率，\\(P(A|B)\\)は事象\\(B\\)が起きたときに事象\\(A\\)が起きる確率，\\(P(B|A)\\)は事象\\(A\\)が起きたときに事象\\(B\\)が起きる確率を表す。ここでの重要な点は，この式の右辺と左辺とで，条件付き確率の位置が変わっていることである。\nこの式は，事象\\(A\\)が起きたときに事象\\(B\\)が起きる確率\\(P(B|A)\\)を，事象\\(B\\)が起きたときに事象\\(A\\)が起きる確率\\(P(A|B)\\)と，事象\\(B\\)が起きる確率\\(P(B)\\)と，事象\\(A\\)が起きる確率\\(P(A)\\)を用いて表している。ここで\\(A\\)をデータ，\\(B\\)をモデルのパラメータと考えると，ベイズの定理は以下のように表される。\n\\[\nP(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}\n\\]\nこの式の左辺にあるのは，データ\\(D\\)が与えられた時のパラメータ\\(\\theta\\)が得られる確率であり，データに基づいてパラメータを推論するという推測統計学の基本的な考え方である。これを構成する右辺は，\\(P(D|\\theta)\\)はパラメータ\\(\\theta\\)が与えられた時のデータ\\(D\\)が得られる確率，\\(P(\\theta)\\)はパラメータ\\(\\theta\\)が得られる確率と言える。パラメータ\\(\\theta\\)がある値になったときに，データ\\(D\\)が得られる確率\\(P(D|\\theta)\\)は尤度と考えることができる。それにかけられる\\(P(\\theta)\\)は，パラメータ\\(\\theta\\)の事前確率と呼ばれるが，これがあることによってベイズ統計学の特徴がより鮮明になる。\nデータを取る前にパラメータがどのようにあるのか，この不確実さをベイズ統計学では事前確率，あるいは一般に事前分布として表現する。それと尤度を掛け合わせたものが(周辺尤度\\(P(D)\\)で除されるとは言え)統計的な推測の結果である事後確率，事後分布\\(P(\\theta|D)\\)となっている。\n古典的には，データを取る前にパラメータのあり方を想定するのは，科学的な態度として不適切であるとして，ベイズ統計学は批判されてきた。この点を認めたとして，改めて式を見てみると，確率分布とデータの関係を表す尤度に，事前分布と周辺尤度の比をかけた結果，事後分布が得られていることになる。尤度を計算してその最も高い値を持つパラメータを推定値としよう，というのが最尤推定法だったわけだが，ベイズ推定はその拡張で，尤度から得られる結果を分布として考えるために事前分布と周辺尤度の比を掛け合わせた，とも言える。尤度関数は確率関数ではないので，そのピークになるところを推定値として用いることしかできなかったが，ベイズの定理を経由すると結果的に得られるのが確率分布になるのだから，パラメータがどのあたりにありそうか，その分布として定量的に評価することも可能である。\n\n\n12.2.2 ベイズの定理の実践史\nベイズ統計学では，未知なるものを確率で表現する。心理統計では平均値パラメータや，複数の平均値パラメータ間の差をすることが目的であり，データを取っただけでは母数が未知なものだったわけだから，これを確率分布として表現することがベイズ統計の最初のステップである。\nベイズ統計は，「未知なるパラメータを確率で表現する（事前分布）」「データによって事前分布を事後分布にアップデートする」というたった2つのステップを繰り返すだけである。事前分布も，データの得られた状況に応じて適切に選択することができる。例えば男性と女性の平均身長の差を知りたい，という状況を考えてみよう。男性の身長の平均値，女性の身長の平均値が未知なるパラメータであるが，長さであること，人間であることを考えると，どれほど低く見積もっても100cm，どれほど高く見積もっても300cmという幅で考えていれば十分であろう。ならば，この区間のどこかにピークが来るような広い正規分布(例えば平均100，標準偏差10)を事前分布として用いれば十分である。\n尤度，すなわちデータが得られるメカニズムとして，確率分布としての正規分布を考えたとしよう。このとき，ベイズの定理の分子に乗るのはいずれも正規分布であるから，正規分布と正規分布の組み合わせから得られる事後分布の形は正規分布になる。\nこのように，事後分布の形が明確であれば，ベイズ統計学は従来の統計学と同じように，パラメータの推定値を求めることができる。しかし問題は，このように事後分布の形が明確でないことが少なくない，ということである。複雑なモデルになればなるほど，複数の確率分布がいろいろ組み合わさった形になり，結果的に事後分布がどのような確率分布になるのか，その形がわからないということになる。\nこれまでのベイズ統計学は，事後分布の形がわかるような，あるいは計算しやすい形になるような(分子の)確率分布の組み合わせを見出す，というのが中心的な問題であった。非常に限定的なものに感じられるかもしれない。まさにその理由で，ベイズ統計学は絵に描いた餅だったのである。\n計算機科学が発達するにつれて，複雑怪奇な事後分布の形であっても，そのピークを探索する方法が考えられた。これは確率分布のパラメータを少しずつ変えていくことで，事後分布の確率密度がより高い位置に動くように変化させていく，つまりあらゆる組み合わせを考えていくグリッドサーチの方法である。グリッドサーチは計算量が膨大になるが，頑張ればなんとかなる。その意味で，ベイズ統計学が少しは実用的になってきた。\nしかしもちろん，まだまだ普段使いできるほどのレベルではない。そのレベルにまで達したのがMCMCという技術である。\n\n\n12.2.3 Markov Chain Monte Carlo法\nMCMC法は，マルコフ連鎖モンテカルロ法の略で，マルコフ連鎖とモンテカルロ法という2つの技術を合わせたものである。前者は確率過程のモデル，後者はシミュレーションによる確率分布のサンプリングを行う方法である。一言で言ってしまえば，マルコフ連鎖によってどんな形の確率分布でも機械の中に作り出すことができるようになり，モンテカルロ法によってその確率分布から乱数を取り出すことができるようになったのである。\n確率分布と乱数の関係はこれまでに見てきた通りで，乱数ひとつひとつは確率分布の実現値であり，ありえる状態の一つでしかないが，これが大量になってくると全体的な傾向，すなわち確率分布の形状を目に見える形にしてくれるのである。数式では表現できない，複雑な事後分布の形であっても，そこから乱数を生成することはできる。そしてその乱数が大量に集まり、ヒストグラムをかいて稜線を眺めてみれば，それが事後分布の形である，と考えることができる。\nこの方法の第1の利点は，確率の計算を集計の問題に置き換えるところである。計算機能力の発達によって，数千，数万程度のデータの集計は一瞬でできるようになった昨今，パラメータについて数千の代表値を得て，その平均値を計算するのは容易いことである。確率分布による平均値，すなわち期待値の近似値として，この平均値を用いることができる。その近似値の精度はMCMCのサンプルサイズに依存するが，サンプルを一桁上げるとその精度も一桁上がるわけだから，乱数を大量に作ることでその精度は一気に向上させられる。\nこの方法の第2の利点は，積分計算が容易になることである。複数のパラメータを持つ確率分布は，多次元空間における確率分布である。そのなかで特定のパラメータに注目する場合，それパラメータ以外のパラメータは不要なので積分によって周辺化する必要がある。数式でこれを行うと，多重積分になって非常に面倒が生じるが，乱数生成アプローチの場合は当該パラメータについてのみ代表値を計上すれば良いので，非常に簡単である。\nあくまでも近似に過ぎないと言われればそうだが，この方法はかなり強力で，事前分布や尤度など考えるべき確率分布とその組み合わせ方を適切に設定すれば，事後分布の形を計算できなくても事後分布からの乱数は得られるのである。この組み合わせの設定は，確率型プログラミング言語によって実装される。確率型プログラミング言語では，事前分布と尤度(データがどの確率分布から出てきているか)を記述するだけで，そのまま事後分布からの乱数を生成することができる。ユーザは自分の好きな確率分布を好きな形で組み合わせることができるから，モデルの表現力が飛躍的に上がったのである。\n確率型プログラミング言語として，代表的なものはJAGSとStanである。現在はStanが主流である。StanはRだけでなく，Pythonなど他の言語からも利用可能である。RでStanを利用するには，パッケージとしてcmdstanrを用いるのが一般的である。このパッケージを導入するにあたっては，cmdstanrのホームページを参照してほしい。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter12.html#ツールの導入stanとbrms",
    "href": "chapter12.html#ツールの導入stanとbrms",
    "title": "12  ベイズ統計の活用",
    "section": "12.3 ツールの導入；Stanとbrms",
    "text": "12.3 ツールの導入；Stanとbrms\nStanの導入には，環境によって若干の違いがあるので，公式のホームページを参照してほしい。2025年3月現在公式ページからGet Startedへと進むと，OSとインターフェイス，インストーラをどこにするかを選択すると，導入に必要なもの(Requirements)と導入方法が表示される。\n\n画面ではOSとしてMacOSを選んでいるが，ここは各自の環境に合わせてもらいたい。インターフェイスはCmdStanRを選択して欲しい。Stanを実行するにはCコンパイラが必要であり，またCmdStanRはコマンドラインからStanを実行してRに繋げるという代物で，cmdstanr::install_cmdstan関数を実行した後，インストール先のパスを設定する必要がある。\nCmdStanRはstan言語で書いた確率モデルを実行し，計算機内部で事後分布を作ってその代表値(MCMCサンプル)を出力させることができる。自ら確率モデルを書くことができるので自由度が高いが，線型モデルに限定して実行するのであれば，Stanを開発しているのと同じチームが提供するbrmsパッケージが便利である。\nbrmsパッケージを用いれば，Rのformulaの指定の仕方で一般化混合線型モデル，階層線型モデルなどが表現できる。これらの非ベイズ推定版であるlmerパッケージとその書式が同じなので，非常に使いやすい。このパッケージの導入は，一般的なCRANからのインストールでも可能である。詳しくはbrmsのサイトを参照してほしい。\n\ninstall.packages(\"brms\")\n\nこれらの環境の準備ができたものとして，使い方を見ていこう。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter12.html#ベイズ法による推定の例",
    "href": "chapter12.html#ベイズ法による推定の例",
    "title": "12  ベイズ統計の活用",
    "section": "12.4 ベイズ法による推定の例",
    "text": "12.4 ベイズ法による推定の例\n\n12.4.1 パラメータリカバリによる確認と結果の読み取り\n前回に倣って，回帰分析のモデル式にそってデータを生成し，分析によってパラメータリカバリを行ってみよう。\n説明変数については制約がないので一様乱数から生成し，平均0，標準偏差\\(\\sigma\\)の誤差とともに被説明変数を作り，従来のやり方で推定してみよう。\n\npacman::p_load(tidyverse)\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データの生成\nx &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x + e\n\ndat &lt;- data.frame(x, y)\nresult.lm &lt;- lm(y ~ x, data = dat)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82796 -0.61831  0.03553  0.69367  2.68062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.021928   0.045010   44.92   &lt;2e-16 ***\nx           3.002194   0.007919  379.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.006 on 498 degrees of freedom\nMultiple R-squared:  0.9965,    Adjusted R-squared:  0.9965 \nF-statistic: 1.437e+05 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\n結果は切片2.0219277，傾き3.0021943であるから，設定した\\(\\beta_0 = 2,\\beta_1 = 3\\)が正しく復元できた，という話であった。\nこれは最尤法による推定であったが，brmsパッケージを使ってベイズ推定に変えてみよう。 方法は次のとおりである。\n\npacman::p_load(brms)\nresult.bayes &lt;- brm(y ~ x, data = dat)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 1:                0.007 seconds (Sampling)\nChain 1:                0.016 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 2:                0.008 seconds (Sampling)\nChain 2:                0.019 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 3:                0.007 seconds (Sampling)\nChain 3:                0.017 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 4:                0.007 seconds (Sampling)\nChain 4:                0.016 seconds (Total)\nChain 4: \n\n\n実行に際して，Compiling Stan program...との文字が表示されるが，これはbrmsパッケージが内部でstan言語を書き，それをC言語に書き換えてコンパイルしていることを意味する。他にもいろいろ出力されているが解説は後述する。簡単なモデルなので，すぐにプロンプトが待機状態に戻るはずである。\nさて，summary関数で結果の要約を見てみよう。\n\nsummary(result.bayes)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: dat (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.02      0.05     1.93     2.11 1.00     3576     2877\nx             3.00      0.01     2.99     3.02 1.00     4181     2652\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.01      0.03     0.95     1.07 1.00     3602     2759\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nまずRegression Coefficientsのところを見てみよう。とりあえずリカバリーが上手くいってるか見てみたいからだ。推定値Estimateのところに，2.0214066と3.0022318とあるから，なるほど正しく推定できているようである。その後に標準誤差(SE)があるのはいいとして，その次にあるのがl-95%CIとu-95%CI，すなわちlower-upperで表される区間である。\nベイズ統計学ではわからないものを確率分布として表現する。今回わからなかったものは回帰係数だから，切片\\(\\beta_0\\)と傾き\\(\\beta_1\\)はそれぞれ確率として表現され，結果も確率分布(事後分布)として得られている。つまり，ここに表されているのは事後分布の95%区間であり，\\(\\beta_0\\)は1.9328496, 2.1114449の間に分布しているもの，という結果になっている。\n最尤推定が点推定であったのに対し，ベイズ推定はこのように分布で表現されるから，表示されている推定値もその代表値である。代表値の取り方はご存知の通り，平均値，最頻値，中央値などが考えられ，そのいずれもが推定値として用いられる。平均値による代表値はExpectation A Posteriori，EAP推定値と呼ばれる。中央値による代表値はMEDian A Posteriori，そして最頻値というより分布のピークを取る推定値のことをMaximum A Posteriori，MAP推定値という。ここで得られた事後分布は分布の関数形によるものではなく，事後分布の代表値の集積による稜線で形を見ているに過ぎないから，MAP推定値を得る方法は1.ヒストグラムを書いてその最頻値である級数の平均値をとる，2.ヒストグラムにフィットする関数を近似し，そのピークを算出する，といった方法が考えられる。計算そのものはパッケージに含まれる関数を利用すれば良い。ここで大事なことは，分布の形状に応じてその代表値を選ぶことである。すなわち，正規分布のような左右対称の分布であれば，平均値，中央値，最頻値は同じ値になるが，ここが異なるようであれば歪んだ分布をしていることが考えられるから，事後分布のヒストグラムを描いて確認した上で，中央値やMAP推定値などを用いるといいだろう。\n\n\n12.4.2 MCMCを評価する\n結果はその他にもいろいろな情報を提供してくれているので，見ていこう。summary関数によって表示された出力の5行目，DrawsのところにMCMCサンプリングの情報が表示されている。これによると，4つのチェインがあり，そのそれぞれが2000回反復(iter)されたこと，そのうちの最初の1000回はwarmupと呼ばれるステップであったこと，最後にthinというのがあるが，これはMCMCサンプルを間引きするためのもので，ここでは1回毎にサンプルを取っている，すなわち毎回のサンプルを用いたことがわかる。\nMCMCは事後分布を作り，そこから乱数を取り出すステップであったこをと思い出そう。乱数を取り出すステップは，まず適当な初期値から始め，次に高次元同時確率空間の中である方向に移動する。その場所からまた次の場所を選ぶ，と次々とステップを進めていく形で事後分布の代表値を拾い集めてゆく。今回の例で言うと，\\(\\beta_0,\\beta_1,\\sigma\\)という3つのパラメータを推定しているので3次元空間を探索する。この空間のある座標は，この3つのパラメータが取りうる可能性のある値の組み合わせである。この空間で，初期値\\(t_0\\)の座標に立ち，近傍の別の座標\\(t_1\\)に移動する。この\\(t_1\\)の座標も，この3つのパラメータが取りうる別の可能性の組み合わせである。同様に\\(t_1\\)の近傍\\(t_2\\),\\(t_2\\)の近傍\\(t_3\\)，とステップを踏むことで，それぞれのステップがMCMCサンプルの1つとして記録されていく。この記録の集約が事後分布の近似になる。\nこのような形でサンプリングが進むことを考えると，まず懸念されるのが「初期値によって結果が変わるのではないか」ということである。実際，初期値によっては，サンプリングがうまくいかないことはある。うまく推定できるときは，どんな初期値から発生しても，事後分布が作る空間の密度の濃いところからサンプリングが進むので，同じような値を集めてくることができるだろう。\nMCMCがうまくいってるかどうかを評価するために，MCMCでは一般的に複数の初期値から始め，別々のステップを踏んでログを取る。このステップのログをチェインと呼ぶ。つまり，それぞれのチェインは異なる初期値から始まる一連の代表値の連なりなのである。今回の結果では，4つのチェイン，つまり4つの初期値から始まったことがわかる。\nさて，もう一度サンプリングの進み方を思い返してみよう。初期値から始めてステップしている最初のうちは，最終的に目的としている事後分布の代表値からは大きく外れているかもしれない。ステップを繰り返すことで，より事後分布の密度の濃いところに近づいていくのだから，最初のうちはうまくいってなくても当然である。この最初のうちのステップは代表値として信用できないので，「バーンインburn in期間だった」ということで切り捨てることが一般的である。brms が採用しているMCMCアルゴリズムのstanは，この初期の探索時に，効率よくサンプリングできるようにステップサイズ(どれぐらい遠くの近傍まで考えるか)等アルゴリズムを自動で調整する期間を設定している。これを特にウォームアップ期間という。\n結果に戻ってみてみると，4つのチェインで2000ステップ(iter)を踏んでいるが，ウォームアップ期間が1000あるので，実際にサンプリングが行われたのは1000ステップ以降である。なのでtotal post-warmup draws =4000となっている。\nthinは，サンプリングの間引き間隔を指定するものであった。ステップ毎に代表値のログを取っていくとしたが，原理的にはそれぞれのステップ・代表値は事後分布から独立にサンプリングされたものであるはずだ。MCMCの結果を見て，ステップ毎の自己相関を確認し，もし\\(t-1\\)時点の代表値が\\(t\\)時点の代表値に影響を与えているようであればよろしくない。そこで間引きをすることで，そのような影響を与えるようなステップを省略することで，より独立性の高いサンプルを得ることができる，と考えるのである。間引きをすると事後分布からのサンプルの数が減ることになる。また，自己相関が高いようなMCMCサンプルしか得られないのは，モデルやパラメタライゼーションが不適切である場合が多く，thinオプションを使うのは結果オブジェクトのサイズを減らす目的である，と考えた方がいいだろう。\n推定値の出力結果に，RhatとBulk-ESSという指標がある。Rhatは，チェイン間の自己相関を表す指標で，1に近いほどよいとされる。基準として，全てのパラメータにおけるRhatが1.1未満であれば，複数のチェインが絡み合った，良いサンプリングであったと評価される。逆に，Rhatが1.1以上であれば，初期値によって異なるパラメータ空間を探索していたことになり，事後分布からの適切な代表値ではない可能性があるので，モデルの見直しなどが必要になる。Bulk-ESSは，有効サンプルサイズEffective Sample Sizeの略であり，実際に独立したサンプルが何個分の情報を持っているかを推定する指標である。これの量的目安は大まかにいって，3桁以上の数字があれば良いと言われる。逆に1，2桁の数字しか示されないのであれば，有効なサンプリングができていないと考えて，モデルの見直しなどが必要になる。\n\n\n12.4.3 可視化してモデルの評価をみる\n結果を可視化してみるとわかりやすい。次のコードは，推定されたパラメータの事後分布のヒストグラムと，トレースプロットと呼ばれるものを描画する。\n\nplot(result.bayes)\n\n\n\n\n\n\n\n\nトレースプロットとは，チェイン毎のステップログを描画したものである。チェインがうまく混合しているかどうかを確認するためのものであり，今回は4つのチェインが混じり合った状態であるから，事後分布からのサンプリングとしてはうまくいっていると考えられる。\n\n\n12.4.4 MCMCサンプルを確認する\nこれら今回の出力はすでに要約されたものになっているが，具体的にどのようなMCMCサンプルが得られているか確認してみよう。結果オブジェクトから，brmsパッケージに含まれるas_draws_df関数を用いて，MCMCサンプルをデータフレームとして取り出すことができる。\n\nmcmc_samples &lt;- brms::as_draws_df(result.bayes)\nmcmc_samples %&gt;%\n  as.data.frame() %&gt;%\n  head()\n\n  b_Intercept      b_x     sigma Intercept    lprior      lp__ .chain\n1    2.005643 3.007028 0.9670094  1.722002 -7.430387 -720.0947      1\n2    2.034874 2.998960 0.9822221  1.751993 -7.430557 -719.4520      1\n3    2.139143 2.994792 0.9970785  1.856656 -7.431063 -723.0209      1\n4    1.997631 3.008462 1.0296125  1.713854 -7.430537 -719.7706      1\n5    2.075360 3.003420 1.0175833  1.792059 -7.430834 -719.8164      1\n6    2.039586 2.998572 0.9869394  1.756742 -7.430591 -719.4091      1\n  .iteration .draw\n1          1     1\n2          2     2\n3          3     3\n4          4     4\n5          5     5\n6          6     6\n\n\nいま取り出したmcmc_samplesは，as_draws_df関数の出力によって，data.frame型の特殊系になっているので，改めてas.data.frame関数を用いてデータフレームに変換し，最初の数行を表示させている。 これの後ろの3列に，.chain,.iteration,.drawという列があるが，これはそれぞれMCMCサンプルのchain番号，サンプルの通し番号，サンプルの通し番号である。\nすでに述べたように，デフォルトでは4つのチェインからサンプリングを行う。画面に表示されているのは，第1チェインの1，2，…,6番目のステップ＝サンプルである。つまり，各行が3次元同時確立空間の代表値であることを指している。\nb_Interceptは切片\\(\\beta_0\\)のサンプル，b_xは傾き\\(\\beta_1\\)のサンプルである。また今回は単回帰分析であるから，\\(Y \\sim N(\\beta_0 + \\beta_1 x, \\sigma)\\)というモデルを推定していたわけで，sigmaはこの\\(\\sigma\\)のサンプルである。Interceptは\\(\\beta_0 + \\beta_1 x\\)，すなわち正規分布の位置を表している。\nlpriorはLog Priorの略で，パラメータの事前分布の対数を表している。lp__はLog Posteriorの略で，パラメータの事後分布の対数を表している。いずれも，モデルの推定に関係する情報として提供されているが，今ここは気にしなくてもいいだろう。\n次のコードは，MCMCサンプルの要約統計量を使って事後分布を記述したものである。MAP推定値については，Rのdensity関数を使って，観測データからカーネル密度推定（Kernel Density Estimation, KDE）を計算し，そのピークの度数を取ることで，MAP推定値を算出している。\n\n# MAP推定用の関数を定義\nfind_map &lt;- function(x) {\n  density_obj &lt;- density(x)\n  return(density_obj$x[which.max(density_obj$y)])\n}\n\nmcmc_samples %&gt;%\n  as.data.frame() %&gt;%\n  select(b_Intercept, b_x, sigma) %&gt;%\n  rowid_to_column(\"iter\") %&gt;%\n  pivot_longer(-iter) %&gt;%\n  group_by(name) %&gt;%\n  summarise(\n    EAP = mean(value),\n    MAD = median(value),\n    MAP = find_map(value),\n    SD = sd(value),\n    L95 = quantile(value, probs = 0.025),\n    U95 = quantile(value, probs = 0.975),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 7\n  name          EAP   MAD   MAP      SD   L95   U95\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 b_Intercept  2.02  2.02  2.02 0.0458  1.93   2.11\n2 b_x          3.00  3.00  3.00 0.00786 2.99   3.02\n3 sigma        1.01  1.01  1.00 0.0318  0.948  1.07\n\n\n今回は，ベイズ統計の位置付けとMCMC法によるベイズ推定の実際を，パッケージを用いて説明した。 brmsパッケージは線型モデルやその応用において非常に強力であり，ベイズ統計の実践においては，これを用いることが多いだろう。ただし，線型モデルでないモデルについては，自分でstanのコードを書いて推定することも多い。線型モデルの限界に囚われず，自由な統計モデリングの世界があることも視野に入れておいてほしい。\n\n\n12.4.5 brmsパッケージのオプション\nベイズ推定には事前分布が必要である。しかしbrm関数を実行した時に，事前分布は特段指定しなかった。これはパッケージがデフォルトで用意した事前分布を用いたからである。 どのような事前分布が用いられたかを確認するには，以下のようにする。\n\nbrms::get_prior(result.bayes)\n\n                   prior     class coef group resp dpar nlpar lb ub\n                  (flat)         b                                 \n                  (flat)         b    x                            \n student_t(3, 0.3, 21.3) Intercept                                 \n   student_t(3, 0, 21.3)     sigma                             0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nこれをみると，回帰係数にはflatな事前分布が用いられていることがわかる。これはデータから考えられた取りうる範囲が全て等確率な，一様分布を次全部ぷとしたことを表している。これは無情報時全部分布と呼ばれる。\n残差の分散\\(\\sigma\\)については，student_t(3, 0, 21.3)，すなわちt分布が使われていることがわかる。具体的にこの分布がどのような形状か，描いてみてみよう。\n\ndf &lt;- 3\nmu &lt;- 0\nsigma &lt;- 21.3\n\ncurve(2 * dt((x - mu) / sigma, df) / sigma,\n  from = 0, to = 100,\n  col = \"blue\", lwd = 2, main = \"Half-Student-t Distribution\",\n  xlab = \"sigma\", ylab = \"Density\"\n)\n\n\n\n\n\n\n\n\n分散は正の値しか取らないので，brms は student_t(3, 0, 21.3) を 半分に折りたたみ、半t分布（half-Student-t） に変換して利用している。t分布は正規分布より裾の重い分布であり，大きな値が出る可能性も考慮されている。分散の事前分布には他にも，cauchy分布やexponential分布などが用いられることもある。\n\n\n12.4.6 MCMCサンプリングの設定\nbrmsパッケージでは，MCMCサンプリングの設定もできる。例えばチェインの数を増やすとか，warmup期間を変えるとか，間引きを変えるとか，いろいろな設定が可能である。また，再現性を確保するために乱数のシードを固定することもできる。これらの設定をしたコードの例を示す。必要に応じて，いろいろな設定を試してみるといいだろう。特に事前分布については，いろいろ変えても結果が大きく変わらない方が，推定値の信頼性も高まると考えられる。事前分布の変更によって，事後分布がどの程度影響されるかを分析することを感度分析というが，このためにも事前分布をデフォルトに任せず設定したり，デフォルトを利用したとしてもどの設定にしているのかを確認できるようになっておこう。\n\n# 事前分布の設定\npriors &lt;- c(\n  set_prior(\"uniform(0, 100)\", class = \"Intercept\"), # 切片: 一様分布(0, 100)\n  set_prior(\"normal(0, 10)\", class = \"b\"), # 回帰係数: N(0, 10)\n  set_prior(\"cauchy(0, 5)\", class = \"sigma\") # 標準偏差: Cauchy(0, 5)\n)\n\nfit &lt;- brm(y ~ x,\n  data = dat,\n  prior = priors,\n  iter = 3000,\n  warmup = 2000,\n  chains = 3,\n  seed = 12345,\n)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter12.html#課題",
    "href": "chapter12.html#課題",
    "title": "12  ベイズ統計の活用",
    "section": "12.5 課題",
    "text": "12.5 課題\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。データセット全体(\\(n=100\\))はex_regression3.csvからダウンロード可能です。このデータセットを用いて，brmsパッケージによる重回帰分析を実行してください。\n分析結果について，以下の項目を順に報告してください：\n\n回帰係数（切片，\\(x1\\)，\\(x2\\)）の推定値と95%信用区間\nMCMCの収束診断（RhatとBulk-ESSの値に基づいて判断）\n各パラメータの事後分布のヒストグラムとトレースプロット\nデフォルトで使用された事前分布の確認と説明\nMCMCサンプルからのMAP推定値の算出（本章で示したfind_map関数を利用）\nMCMCサンプルに基づく90%信用区間と75%信用区間の算出\n\nなお，レポートには使用したRコードと，各項目の結果に対する簡潔な解釈を含めてください。\n\n\n\n\nシャロン・バーチュ・マグレイン. (2011) 2018. 異端の統計学ベイズ. Translated by 冨永星. 草思社.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter13.html",
    "href": "chapter13.html",
    "title": "13  線型モデルの展開",
    "section": "",
    "text": "13.1 一般線型モデル(General Linear Model)\nこの章では，線型モデルの展開を説明する。線型モデルは，説明変数と目的変数の関係を線型すなわち一次式で表現するモデルである。 線型モデルは，一般化線型モデル(GLM)，一般化線型混合モデル(GLMM)，階層線型モデル(HLM)と拡張していくが，まずは一般線型モデル(LM)についてみておこう。\n線型モデルの基本は回帰分析モデルである。単回帰モデルは次の式で表される。\n\\[\ny_i = \\beta_0 + \\beta_1 x + e_i\n\\]\nここで，\\(y_i\\)は\\(i\\)番目の観測値，\\(\\beta_0\\)は切片，\\(\\beta_1\\)は回帰係数，\\(e_i\\)は誤差項である。 この式を拡張したのが重回帰分析で，次の式で表される。\n\\[\ny_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + e_i\n\\]\nここで，\\(x_1, x_2, \\cdots, x_p\\)は説明変数，\\(\\beta_1, \\beta_2, \\cdots, \\beta_p\\)は回帰係数である。\nこの式をベクトルと行列で表現すると，次のようになる。\n\\[\ny = X\\beta + e\n\\]\nここで，\\(y\\)は\\(n\\)次元のベクトル，\\(X\\)は\\(n \\times p\\)の計画行列(design matrix)，\\(\\beta\\)は\\(p\\)次元のベクトル，\\(e\\)は\\(n\\)次元のベクトルである。\n計画行列は，説明変数のデータをまとめた行列であり，次のようになる。\n\\[\nX = \\begin{pmatrix}\n1 & x_1 & x_2 & \\cdots & x_p\\\\\n1 & x_1 & x_2 & \\cdots & x_p\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_1 & x_2 & \\cdots & x_p\n\\end{pmatrix}\n\\]\n第一列目にあるのは切片にかかる係数であり，第二列目以降にあるのは説明変数にかかる係数である。 ここに係数ベクトル\\(\\beta= (\\beta_0, \\beta_1, \\beta_2, \\cdots, \\beta_p)^T\\)をかけることで，説明変数の線型結合が得られる。\nこの係数ベクトルにおいて，\\(x\\)の値がバイナリ(0/1)の場合，その説明変数はダミー変数と呼ばれる。線型回帰モデルにおいて説明変数\\(X\\)がダミー変数である場合は，横軸が2つの値しかとらないため散布図を描くと奇妙な形になることがわかるだろう。\nこの散布図に対して回帰直線を引けば，2群の平均値を通る直線が引かれる。回帰分析においては，予測式\\(\\hat{y}\\)を中心とした正規分布に従って誤差が生じると仮定されているのであった。ダミーデータの場合も，両群の平均値を中心に正規分布に従って誤差が生じると仮定したことになる。これはいわゆる2群の平均値の差を検定する時の仮定と同じであり，このことから平均値差の検定は説明変数が名義尺度水準の変数である回帰分析と同じ(一般)であることがわかる。回帰分析と平均値差の検定は数学的には同じ式で表現できるから，これを総称して一般線型モデルGeneral Linear model という。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html#一般化線型モデルglm",
    "href": "chapter13.html#一般化線型モデルglm",
    "title": "13  線型モデルの展開",
    "section": "13.2 一般化線型モデル(GLM)",
    "text": "13.2 一般化線型モデル(GLM)\nかつて心理学実践においては，要因計画法と帰無仮説検定の組み合わせによって研究を行うことが主流であった。平均値差の検定に落とし込むように研究をデザインし，帰無仮説検定でYESかNOか決着がつく。帰無仮説検定はデータの特性や生成メカニズムに依存せず，統計パッケージを用いれば誰にでも扱うことができ，\\(p\\)値という手垢のついてない数値だけで判断できると考えられてきた。\nこのことに関する問題については置くとして，ともかく「正規分布の平均値の差」に持ち込めさえすれば良い，ということが当時の研究者の共通認識であった。そのため，例えば比率のデータやカウントデータなど，正規分布を想定することができないデータに対しても，対数変換・角変換などを行って分布を正規分布のそれに近づけ，検定を行うということが行われていた。\n正規分布は理論的に\\(\\pm \\infty\\)の範囲を取るのに対し，比率のデータは0から1の範囲にしかとらない。カウントデータは正の整数しかとらないデータであるから，こうしたデータに対して一般線型モデルをあてがうのは重大な仮定違反である1。\nこのような背景から，正規分布以外の確率分布を用いた統計モデルが考えられた。これが一般化線型モデル(Generalized Linear Model, GLM)である2。\n確率分布の多くは，位置パラメータとスケールパラメータを持つ。統計モデルは平均的な挙動について考えるモデルだから，位置パラメータに線型モデルが当てがえるように，数式を変形してやれば良い。以下に例として，ベルヌーイ分布とポアソン分布をGLMで表現したものを考えてみよう。\n\n13.2.1 ベルヌーイ分布に対する線型モデル；ロジスティック回帰分析\nベルヌーイ分布はコイントスの表/裏のような2値をとるデータに対して用いられる。ベルヌーイ分布の確率関数は次のように表される。\n\\[\nP(Y = y) = p^y (1-p)^{1-y}\n\\]\nここで，\\(p\\)は成功確率であり，0から1の範囲の値を取る。 こうしたデータは現実にも少なくない。生死，病気の有無，テストの正答/誤答のようなデータが代表的である。こうしたデータを目的変数にして，直線回帰を行うと当然おかしなことが生じる。すなわち，予測値が0と1の範囲を超えることがありえるからである。\nそこで，線型予測子と確率を適切に結びつけるリンク関数を用いる。ロジスティック回帰ではロジット関数（logit function）をリンク関数として使用する：\n\\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x\n\\]\nこの関係を\\(p\\)について解いてみよう。\\(\\eta = \\beta_0 + \\beta_1 x\\)とおくと：\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\eta\n\\]\n両辺の指数を取ると：\n\\[\n\\frac{p}{1-p} = e^{\\eta}\n\\]\n両辺に\\((1-p)\\)をかけると：\n\\[\np = (1-p) \\cdot e^{\\eta}\n\\]\n展開すると：\n\\[\np = e^{\\eta} - p \\cdot e^{\\eta}\n\\]\n\\(p\\)について整理すると：\n\\[\np + p \\cdot e^{\\eta} = e^{\\eta}\n\\]\n\\[\np(1 + e^{\\eta}) = e^{\\eta}\n\\]\n\\[\np = \\frac{e^{\\eta}}{1 + e^{\\eta}}\n\\]\n分子分母を\\(e^{\\eta}\\)で割ると：\n\\[\np = \\frac{1}{e^{-\\eta} + 1} = \\frac{1}{1 + e^{-\\eta}}\n\\]\n\\(\\eta = \\beta_0 + \\beta_1 x\\)を代入すると，ロジスティック関数（逆リンク関数）が得られる：\n\\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\n\\]\nこのようにして得られた確率を用いて，ベルヌーイ分布の確率関数を次のように表現できる。\n\\[ y \\sim \\text{Bernoulli}(p) \\]\nサンプルデータを作って，線型回帰モデルとロジスティック回帰モデルの予測値を比較してみよう。\n\npacman::p_load(tidyverse, patchwork)\n\n# データ生成\nset.seed(17)\nn &lt;- 200\nx &lt;- runif(n, min = -10, max = 10)\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\np &lt;- beta_0 + x * beta_1\nprob &lt;- 1/(1+exp(-p))\ny &lt;- rbinom(n, size = 1, prob = prob)\n\ndf_logistic &lt;- data.frame(x = x, y = y)\n\n# p1: 線型回帰\np1 &lt;- ggplot(df_logistic, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"線型回帰\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ロジスティック回帰\np2 &lt;- ggplot(df_logistic, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"ロジスティック回帰\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# パッチワークで結合\np1 + p2\n\n\n\n\n\n\n\n\n線型回帰の予測値が不適切な範囲にまで延伸するのに対し，ロジスティック回帰はうまく適合していることがわかるだろう。\nデータからロジスティック回帰モデルを推定するには，glm関数を用いることができるが，ここではすでに学んだbrmsパッケージによるベイズ推定でアプローチしてみよう。brm関数の書き方は，glm関数の書き方とほぼ同じであり，推定法をMLからベイズに変えることできる。\n\npacman::p_load(brms)\nresult.bayes.logistic &lt;- brm(\n    y ~ x,\n    family = bernoulli(),\n    data = df_logistic,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nsummary(result.bayes.logistic)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: y ~ x \n   Data: df_logistic (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.88      0.41     0.10     1.70 1.00     2314     2331\nx             1.35      0.26     0.91     1.92 1.00     2164     2080\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.logistic)\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\nresult.ml &lt;- glm(y ~ x, family = binomial(), data = df_logistic)\nsummary(result.ml)\n\n\nCall:\nglm(formula = y ~ x, family = binomial(), data = df_logistic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.8673     0.4058   2.137   0.0326 *  \nx             1.2661     0.2413   5.248 1.54e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.869  on 199  degrees of freedom\nResidual deviance:  45.112  on 198  degrees of freedom\nAIC: 49.112\n\nNumber of Fisher Scoring iterations: 8\n\n\n最尤推定の結果とベイズ推定の結果に多少のズレが見られるが，これはサンプルサイズの小ささによるものである。アウトプット変数が2値しか持たないため，分散がどうしても小さくなりがちであり，正確な推定値を得るためにはより多くのデータが必要である。\nまた，回帰係数の解釈には注意が必要である。普通の回帰分析であれば，説明変数が1単位変わると目的変数がどれだけ変わるかを表すが，ロジスティック回帰ではこのような直接的な解釈はできない。ロジスティック関数によって変換されたものが意味を持つからである。\n線型モデルが表すのは次の関係なのであった。 \\[ \\beta_0 + \\beta_1 x = \\log \\frac{p}{1-p} \\]\nこの\\(\\log \\frac{p}{1-p}\\)はロジット(logit)と呼ばれる。ロジスティック回帰では，確率\\(p\\)をロジットに変換することで線型関係を表現している。逆に言えば，線型モデルで表現されているのはログを取った確率の比であり，この比が説明変数の線型関係によって決まるということである。であるから，結果を解釈するには係数を指数関数\\(e\\)を取り，確率の比として理解する必要がある。\n今回のデータでは説明変数の係数が1.36と推定されたから，\\(e^{1.36} = 3.89\\)である。これは，説明変数が1単位増加すると，成功確率が3.89倍になる，ということを意味する。\n\n\n13.2.2 ポアソン分布に対する線型モデル；ポアソン回帰\n今度はカウント変数に対するモデルを考えてみよう。カウント変数は正の整数をとるデータであり，ポアソン分布を用いることができる。ポアソン分布の確率関数は次のように表される。\n\\[\nP(Y = y) = \\frac{\\lambda^y e^{-\\lambda}}{y!}\n\\]\nここで，\\(\\lambda\\)は平均である。ポアソン分布の形状も確認しておこう。\n\n\n\n\n\n\n\n\n\nこのような正の整数しかとらないデータに対してはポアソン分布で回帰した方がよい。ポアソン回帰では対数関数をリンク関数として使用する：\n\\[\n\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_i\n\\]\nこの関係を\\(\\lambda_i\\)について解くと，指数関数（逆リンク関数）が得られる：\n\\[ \\lambda_i = \\exp(\\beta_0 + \\beta_1 x_i) \\]\nとして\n\\[ y_i \\sim \\text{Pois}(\\lambda_i) \\]\nを考えるのである。\n以下にサンプルデータを作ったポアソン回帰の例を示す。\n\n# データ生成\nset.seed(17)\nn &lt;- 200\nx &lt;- runif(n, min = 0, max = 10)\nbeta_0 &lt;- 0.5\nbeta_1 &lt;- 0.3\nlambda &lt;- exp(beta_0 + beta_1 * x)\ny &lt;- rpois(n, lambda = lambda)\n\ndf_pois &lt;- data.frame(x = x, y = y)\n\n# p1: 線型回帰（不適切な例）\np1 &lt;- ggplot(df_pois, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"カウント変数\", title = \"線型回帰（不適切）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ポアソン回帰（適切な例）\np2 &lt;- ggplot(df_pois, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_smooth(method = \"glm\", method.args = list(family = \"poisson\"), se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"カウント変数\", title = \"ポアソン回帰（適切）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# パッチワークで結合\np1 + p2\n\n\n\n\n\n\n\n\n線型回帰では負の値の予測値が出てしまう可能性があるのに対し，ポアソン回帰は指数関数による変換によって適切にカウントデータの特性を捉えていることがわかる。\nポアソン回帰を実行するRコードの例を次に示す。\n\nresult.bayes.pois &lt;- brm(\n    y ~ x,\n    family = poisson(),\n    data = df_pois,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nsummary(result.bayes.pois)\n\n Family: poisson \n  Links: mu = log \nFormula: y ~ x \n   Data: df_pois (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.49      0.07     0.36     0.63 1.00     1275     1675\nx             0.30      0.01     0.29     0.32 1.00     1471     1943\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.pois)\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\nresult.ml.pois &lt;- glm(y ~ x, family = poisson(), data = df_pois)\nsummary(result.ml.pois)\n\n\nCall:\nglm(formula = y ~ x, family = poisson(), data = df_pois)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.495962   0.071166   6.969 3.19e-12 ***\nx           0.304829   0.009555  31.902  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1457.6  on 199  degrees of freedom\nResidual deviance:  214.3  on 198  degrees of freedom\nAIC: 975.04\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html#一般化線型混合モデルglmm",
    "href": "chapter13.html#一般化線型混合モデルglmm",
    "title": "13  線型モデルの展開",
    "section": "13.3 一般化線型混合モデル(GLMM)",
    "text": "13.3 一般化線型混合モデル(GLMM)\n線型モデルのさらなる拡張として，一般化線型混合モデル(GLMM)Generalized Linear Mixed Modelがある。ここまでの回帰モデルは，説明変数が目的変数に対して一貫した効果を持つと仮定してきた。この効果を特に固定効果fixed effectというが，GLMMでは固定効果に加えて変量効果random effectを考慮することができる。\n変量効果とは，説明変数が個体ごとに異なる値を持つことを意味する。たとえばWithinデザインの要因計画は個人差を考慮することができるが，これは変量効果を考慮したモデルであるといえる。すなわち，研究として見たい効果はWithin要因の水準ごとの固定効果であり，これとは別に個人の平均値が異なることを想定しているから，個人ごとの平均値を変量効果として考慮しているモデルといえる。\nこのように変量効果は個人ごとに異なる影響を考えることであるが，仮定としてこうした個人差が確率分布，特に正規分布に従っていると考えるのである。個人差は確率分布からランダムに生じるものであり，個人同士の平均的な違いは交換可能であると考えられる。またこうした個人差の分散は，個人の平均値の分散として捉えられる。このように個人差を表す確率分布も混ぜ込むので，混合(Mixed)モデルと呼ばれるのである。\n\n13.3.1 個人差の分布を「混ぜる」\n分散分析のBetweenモデルとWithinモデルの違いについて考えてみよう。 一要因の場合，Betweenモデルは次のように表される。 \\[ SS_T = SS_A + SS_e \\]\nここで\\(SS_T\\)とは全体の平方和(Sum of Squares)であり，これを右辺の要因Aで説明する平方和\\(SS_A\\)と誤差\\(SS_E\\)に分解するのであった。\nこのときWithinモデルは，\n\\[SS_T = SS_A + SS_s + SS_e\\]\nと表され，ここで\\(SS_s\\)が個人差の平方和である。 検定に際しては\\(SS_e\\)に対する\\(SS_A\\)の比率を考える3ことになるから，効果の大きさが同じであればWithinデザインの方が有利である。Betweenデザインは誤差から個人差を分離できていないからで，これを分離して誤差を小さくできる方が効果vs誤差の比は大きくなるからである。\nここでは分散での記述であったが，個別の値\\(Y_{ij}\\)を考えると，Betweenデザインは\n\\[ Y_{ij} = \\beta_0 + \\beta_1 x_{ij} + e_{ij} \\] であり，Withinデザインは \\[ Y_{ij} = \\mu + \\beta_{0i} + \\beta_1 x_{ij} + e_{ij} \\] である。ここで\\(\\mu\\)は全平均であり，\\(\\beta_{01}\\)は全平均から個人平均の差，つまり個人差を表している。個人\\(i\\)に対して反復測定がなされているからその平均を計算することができ，相対的な切片の違いを個人の効果として取り出していると言える。\nここで確率変数を考えると， \\[e_{ij} \\sim N(0, \\sigma_e)\\] \\[\\beta_{0i} \\sim N(0, \\sigma_s)\\]\nとなるから，一つの値に対して複数の確率分布が混合(mix)されていることがわかる。\nここでは個人差が平均値，すなわち切片が異なるものとして説明したが，傾きに対して個人差を考えることもできる。変量効果がどこにあるかによって，ランダム切片モデル，ランダム傾きモデル，ランダム切片ランダム傾きモデルなどと呼ばれることがある。\n\n\n13.3.2 ランダム切片モデル\nランダム切片モデルは，切片が個人ごとに異なるモデルである。切片が個人ごとに異なるということは，切片の個人差が正規分布に従うと考えることである。\n\\[\n\\beta_{0i} = \\beta_0 + u_{0i}\n\\]\nここで，\\(\\beta_0\\)は全体の切片，\\(u_{0i}\\)は個人\\(i\\)の切片の個人差である。個人差は正規分布に従うと考えるから，\n\\[\nu_{0i} \\sim N(0, \\sigma_u)\n\\]\nと表現できる。モデル全体としては\n\\[\ny_{ij} = (\\beta_0 + u_{0i}) + \\beta_1 x_{ij} + e_{ij}\n\\]\nとなる。ここで，\\(e_{ij}\\)は誤差項であり，個人\\(i\\)の\\(j\\)番目の観測値に対する誤差を表す。\n具体的なデータを作って見てみよう。\n\n# データ生成\nset.seed(17)\nn_person &lt;- 10  # 個人数\nn_obs &lt;- 20     # 各個人の観測数\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nsigma_u &lt;- 1    # 個人差の標準偏差\nsigma_e &lt;- 0.5  # 誤差の標準偏差\n\n# 個人ごとのランダム切片\nperson_intercepts &lt;- rnorm(n_person, mean = 0, sd = sigma_u)\n\n# データフレーム作成\ndf_random_intercept &lt;- expand_grid(\n  person = 1:n_person,\n  obs = 1:n_obs\n) %&gt;%\n  mutate(\n    x = runif(n(), min = 0, max = 10),\n    u_0 = person_intercepts[person],\n    y = beta_0 + u_0 + beta_1 * x + rnorm(n(), mean = 0, sd = sigma_e),\n    person_factor = factor(person)\n  )\n\n## データの確認\ndf_random_intercept %&gt;% head()\n\n# A tibble: 6 × 6\n  person   obs     x   u_0     y person_factor\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        \n1      1     1 8.81  -1.02 17.0  1            \n2      1     2 6.07  -1.02 11.5  1            \n3      1     3 7.40  -1.02 15.4  1            \n4      1     4 8.03  -1.02 16.9  1            \n5      1     5 9.02  -1.02 18.3  1            \n6      1     6 0.927 -1.02  2.00 1            \n\n# p1: 線型回帰（全体で一つの回帰線）\np1 &lt;- ggplot(df_random_intercept, aes(x = x, y = y)) +\n    geom_point(alpha = 0.5, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"線型回帰（固定効果のみ）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ランダム切片モデル（個人ごとに異なる切片の回帰線）\np2 &lt;- ggplot(df_random_intercept, aes(x = x, y = y, color = person_factor)) +\n    geom_point(alpha = 0.6, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"ランダム切片モデル\") +\n    theme(\n      text = element_text(family = \"IPAexGothic\"),\n      legend.position = \"none\"\n    )\n\np1 + p2\n\n\n\n\n\n\n\n\nランダム効果を含むモデルを推定するコード例を以下に示す。\n\nresult.bayes.random_intercept &lt;- brm(\n    y ~ x + (1 | person),\n    family = gaussian(),\n    data = df_random_intercept,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.6 seconds.\n\nsummary(result.bayes.random_intercept)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x + (1 | person) \n   Data: df_random_intercept (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~person (Number of levels: 10) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.01      0.30     0.60     1.73 1.01      408      786\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.29      0.35     0.61     2.01 1.01      620      461\nx             1.98      0.01     1.96     2.01 1.00     2380     2052\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.53      0.03     0.48     0.59 1.00     1790     2245\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.random_intercept)\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\npacman::p_load(lmerTest)\nresult.ml.random_intercept &lt;- lmer(y ~ x + (1 | person), data = df_random_intercept)\nsummary(result.ml.random_intercept)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x + (1 | person)\n   Data: df_random_intercept\n\nREML criterion at convergence: 356.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3167 -0.5745 -0.1189  0.6343  2.6464 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n person   (Intercept) 0.7462   0.8638  \n Residual             0.2773   0.5266  \nNumber of obs: 200, groups:  person, 10\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   1.26699    0.28414  10.15076   4.459  0.00117 ** \nx             1.98396    0.01361 189.35597 145.774  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.242\n\n## さらに比較のために，普通の回帰分析も行ってみる\nresult.ml.random_intercept.ordinal &lt;- lm(y ~ x, data = df_random_intercept)\nsummary(result.ml.random_intercept.ordinal)\n\n\nCall:\nlm(formula = y ~ x, data = df_random_intercept)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.27275 -0.59838  0.08193  0.50855  2.67596 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.31764    0.14235   9.256   &lt;2e-16 ***\nx            1.97394    0.02464  80.124   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9771 on 198 degrees of freedom\nMultiple R-squared:  0.9701,    Adjusted R-squared:  0.9699 \nF-statistic:  6420 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n変量効果のモデル表記は，固定効果に加えて(1 | person)のように表記する。ここで，1は切片を表し，personは個人を表す。このようにすることで，個人ごとに異なる切片を考慮することができる。\n設定したパラメタ（個人差の標準偏差\\(\\sigma_u\\) = 1，残差の標準偏差\\(\\sigma_e\\) = 0.5）に対して，ベイズ推定では個人差の標準偏差が1.012，残差の標準偏差が0.53と推定されている。ML推定では個人差の標準偏差が0.864，残差の標準偏差が0.527と推定された。両手法ともに設定値に近い値が得られており，適切にパラメタが推定されていることがわかる。\nこのデータに対して，固定効果のみで推定すると切片が1.318，傾きが1.974と推定される。傾きに対しては同じ値になるが切片の変動を考慮するかどうかで推定値が異なっていることがわかる。固定効果のみのモデルの場合，切片に想定された正規分布の平均値のみ推定したことになっている。実践的な意味としては，Within計画をBetween計画と皆して推定しているようなものであり，せっかく綺麗に取り分けられる個人差分散を無視しているようなものである。\n\n13.3.2.1 個人レベルの推定値の取得\nベイズ推定の利点の一つは，個人ごとの推定値とその不確実性を定量化できることである。MCMCサンプルから個人の変量効果を取り出し，事後分布を可視化してみよう。\nbrmsパッケージの関数ranefを使うと面倒なことをしなくても直接取り出してくれるが，MCMCサンプルを取り出して加工して確認することは，MCMCによる推定の理解を深めるのに良い訓練となる。\n\n# 個人ごとの変量効果を取得して表示してみる\nrandom_effects &lt;- ranef(result.bayes.random_intercept)\nprint(random_effects)\n\n$person\n, , Intercept\n\n     Estimate Est.Error       Q2.5      Q97.5\n1  -1.2842574 0.3604082 -2.0328445 -0.6093264\n2  -0.2815002 0.3612608 -1.0005500  0.4143775\n3  -0.4328320 0.3614323 -1.1734005  0.2534434\n4  -1.0881607 0.3642191 -1.8473405 -0.4056347\n5   0.5388313 0.3636371 -0.2012679  1.2376413\n6  -0.2689177 0.3633187 -1.0316235  0.4175829\n7   0.5917789 0.3634518 -0.1563293  1.3063205\n8   1.6501281 0.3597518  0.9219949  2.3568723\n9   0.1666085 0.3624218 -0.5813912  0.8600216\n10  0.2276495 0.3628401 -0.5134040  0.9184448\n\n# 個人の切片の変量効果の事後分布(MCMCサンプル)を取得\nposterior_samples &lt;- as_draws_df(result.bayes.random_intercept) %&gt;% \n  select(starts_with(\"r_person\")) %&gt;% \n  rowid_to_column(\"iter\") %&gt;% \n  pivot_longer(-iter) %&gt;% \n  mutate(person = str_extract(name,pattern=\"\\\\d+\")) %&gt;% \n  mutate(person = factor(person, levels = as.character(1:10))) %&gt;% \n  select(-name)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n# MCMCサンプルを使った要約\nposterior_samples %&gt;% \n  group_by(person) %&gt;% \n  summarise(\n    EAP = mean(value),\n    median = quantile(value, 0.5),\n    q025 = quantile(value, 0.025),\n    q975 = quantile(value, 0.975),\n    sd = sd(value),\n    .groups = \"drop\"\n  )\n\n# A tibble: 10 × 6\n   person    EAP median   q025   q975    sd\n   &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 1      -1.28  -1.28  -2.03  -0.609 0.360\n 2 2      -0.282 -0.273 -1.00   0.414 0.361\n 3 3      -0.433 -0.425 -1.17   0.253 0.361\n 4 4      -1.09  -1.08  -1.85  -0.406 0.364\n 5 5       0.539  0.545 -0.201  1.24  0.364\n 6 6      -0.269 -0.270 -1.03   0.418 0.363\n 7 7       0.592  0.593 -0.156  1.31  0.363\n 8 8       1.65   1.66   0.922  2.36  0.360\n 9 9       0.167  0.171 -0.581  0.860 0.362\n10 10      0.228  0.235 -0.513  0.918 0.363\n\n# 事後分布の描画\np1 &lt;- ggplot(posterior_samples, aes(x = value, fill = person)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ person, scales = \"free_y\") +\n  theme_minimal() +\n  labs(x = \"切片の変量効果\", y = \"事後密度\", title = \"個人ごとの切片変量効果の事後分布\") +\n  theme(text = element_text(family = \"IPAexGothic\"), legend.position = \"none\")\n\nprint(p1)\n\n\n\n\n\n\n\n\nここで抜き出された個人の効果は，全体平均からの偏差であり，実際の切片は固定効果＋変量効果の形で得られている。これについてもcoef関数あるいはMCMCサンプルを加工することで，より具体的にイメージしながら利用できるだろう。\n\n# 各個人の実際の切片（固定効果+変量効果）を取得\nindividual_coefs &lt;- coef(result.bayes.random_intercept)$person\n\n# 実際の切片の事後分布（固定効果+変量効果）\ntotal_intercept_samples &lt;- as_draws_df(result.bayes.random_intercept) %&gt;% \n  select(b_Intercept, starts_with(\"r_person\")) %&gt;% \n  rowid_to_column(\"iter\") %&gt;% \n  pivot_longer(-c(iter, b_Intercept)) %&gt;% \n  mutate(person = str_extract(name, pattern=\"\\\\d+\")) %&gt;% \n  mutate(person = factor(person, levels = as.character(1:10))) %&gt;% \n  mutate(total_intercept = b_Intercept + value) %&gt;% \n  select(iter, person, total_intercept)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n# 実際の切片の事後分布の描画\np2 &lt;- ggplot(total_intercept_samples, aes(x = total_intercept, fill = person)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ person, scales = \"free_y\") +\n  theme_minimal() +\n  labs(x = \"実際の切片（固定効果+変量効果）\", y = \"事後密度\", \n       title = \"個人ごとの実際の切片の事後分布\") +\n  theme(text = element_text(family = \"IPAexGothic\"), legend.position = \"none\")\n\nprint(p2)\n\n\n\n\n\n\n\n# 実際の切片の信頼区間\ntotal_intercept_summary &lt;- total_intercept_samples %&gt;% \n  group_by(person) %&gt;% \n  summarise(\n    EAP = mean(total_intercept),\n    median = quantile(total_intercept, 0.5),\n    q025 = quantile(total_intercept, 0.025),\n    q975 = quantile(total_intercept, 0.975),\n    sd = sd(total_intercept),\n    .groups = \"drop\"\n  )\n\nprint(total_intercept_summary)\n\n# A tibble: 10 × 6\n   person     EAP  median    q025  q975    sd\n   &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1      0.00382 0.00210 -0.284  0.279 0.146\n 2 2      1.01    1.01     0.746  1.27  0.135\n 3 3      0.855   0.857    0.597  1.10  0.128\n 4 4      0.200   0.197   -0.0805 0.484 0.143\n 5 5      1.83    1.83     1.55   2.11  0.142\n 6 6      1.02    1.02     0.753  1.29  0.136\n 7 7      1.88    1.88     1.61   2.14  0.137\n 8 8      2.94    2.94     2.67   3.21  0.137\n 9 9      1.45    1.45     1.18   1.73  0.139\n10 10     1.52    1.52     1.24   1.80  0.144\n\n\nこのように，ベイズ推定では個人ごとの推定値とその不確実性を詳細に分析することができる。\n\n\n\n13.3.3 ランダム傾きモデル\nランダム傾きモデルは，傾きが個人ごとに異なるモデルである。傾きが個人ごとに異なるということは，傾きの個人差が正規分布に従うと考えることである。\n\\[\n\\beta_{1i} = \\beta_1 + u_{1i}\n\\]\nここで，\\(\\beta_1\\)は全体の傾き，\\(u_{1i}\\)は個人\\(i\\)の傾きの個人差である。個人差は正規分布に従うと考えるから，\n\\[\nu_{1i} \\sim N(0, \\sigma_u)\n\\]\nと表現できる。モデル全体としては\n\\[\ny_{ij} = \\beta_0  + (\\beta_1 + u_{1i}) x_{ij} + e_{ij}\n\\]\nとなる。\nこれについても具体的なデータを作って見てみよう。\n\n# データ生成\nset.seed(17)\nn_person &lt;- 10  # 個人数\nn_obs &lt;- 20     # 各個人の観測数\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nsigma_u &lt;- 0.5  # 傾きの個人差の標準偏差\nsigma_e &lt;- 1.5  # 誤差の標準偏差\n\n# 個人ごとのランダム傾き\nperson_slopes &lt;- rnorm(n_person, mean = 0, sd = sigma_u)\n\n# データフレーム作成\ndf_random_slope &lt;- expand_grid(\n  person = 1:n_person,\n  obs = 1:n_obs\n) %&gt;%\n  mutate(\n    x = runif(n(), min = 0, max = 10),\n    u_1 = person_slopes[person],\n    y = beta_0 + (beta_1 + u_1) * x + rnorm(n(), mean = 0, sd = sigma_e),\n    person_factor = factor(person)\n  )\n\n## データの確認\ndf_random_slope %&gt;% head()\n\n# A tibble: 6 × 6\n  person   obs     x    u_1     y person_factor\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        \n1      1     1 8.81  -0.508 12.5  1            \n2      1     2 6.07  -0.508  8.12 1            \n3      1     3 7.40  -0.508 13.9  1            \n4      1     4 8.03  -0.508 15.5  1            \n5      1     5 9.02  -0.508 15.2  1            \n6      1     6 0.927 -0.508  2.88 1            \n\n# p1: 線型回帰（全体で一つの回帰線）\np1 &lt;- ggplot(df_random_slope, aes(x = x, y = y)) +\n    geom_point(alpha = 0.5, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"線型回帰（固定効果のみ）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ランダム傾きモデル（個人ごとに異なる傾きの回帰線）\np2 &lt;- ggplot(df_random_slope, aes(x = x, y = y, color = person_factor)) +\n    geom_point(alpha = 0.6, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"ランダム傾きモデル\") +\n    theme(\n      text = element_text(family = \"IPAexGothic\"),\n      legend.position = \"none\"\n    )\n\np1 + p2\n\n\n\n\n\n\n\n\nランダム傾きモデルでは，個人ごとに回帰直線の傾きが異なることがわかる。これは，説明変数の効果が個人によって異なることを表している。\nランダム傾きモデルを推定するコード例を以下に示す。\n\nresult.bayes.random_slope &lt;- brm(\n    y ~ x + (0 + x | person),\n    family = gaussian(),\n    data = df_random_slope,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.5 seconds.\n\nsummary(result.bayes.random_slope)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x + (0 + x | person) \n   Data: df_random_slope (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~person (Number of levels: 10) \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(x)     0.51      0.15     0.30     0.86 1.00      527     1003\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.24      0.24     0.79     1.71 1.00     5674     3203\nx             2.04      0.16     1.73     2.36 1.00      605      743\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.60      0.08     1.45     1.76 1.00     1642     1684\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.random_slope)\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\nresult.ml.random_slope &lt;- lmer(y ~ x + (0 + x | person), data = df_random_slope)\nsummary(result.ml.random_slope)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x + (0 + x | person)\n   Data: df_random_slope\n\nREML criterion at convergence: 792.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2526 -0.6127 -0.0716  0.6858  2.6584 \n\nRandom effects:\n Groups   Name Variance Std.Dev.\n person   x    0.1894   0.4352  \n Residual      2.5139   1.5855  \nNumber of obs: 200, groups:  person, 10\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)   1.2415     0.2335 189.2290   5.317 2.96e-07 ***\nx             2.0451     0.1436  10.2643  14.240 4.31e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.250\n\n\n変量効果のモデル表記で，(0 + x | person)は傾きのみにランダム効果を考慮することを表す。0によって切片の固定効果を除き，xで傾きにランダム効果を指定している。それぞれの出力が，理論値のどこを反映したものであるか確認しておこう。\nまた出力結果はランダム切片モデルの時と同様に，関数あるいはMCMCサンプルから推定値やその分布情報を得ることもできる。自分なりに加工して出力結果を確認してみてほしい。\n\n\n13.3.4 ランダム切片ランダム傾きモデル\nランダム切片ランダム傾きモデルは，切片と傾きの両方が個人ごとに異なるモデルである。これは最も複雑な変量効果モデルで，個人ごとに異なる切片と傾きを同時に考慮する。\n\\[\n\\beta_{0i} = \\beta_0 + u_{0i}\n\\] \\[\n\\beta_{1i} = \\beta_1 + u_{1i}\n\\]\nここで，\\(u_{0i}\\)は個人\\(i\\)の切片の個人差，\\(u_{1i}\\)は個人\\(i\\)の傾きの個人差である。 このランダム効果はいずれも個人に係るものであるから相関することが仮定されるため，それぞれに正規分布を仮定するのではなく多変量正規分布から得られるものと仮定する。当然のことながら，この相関係数も推定対象である。\n\\[\n\\begin{pmatrix} u_{0i} \\\\ u_{1i} \\end{pmatrix} \\sim MVN\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{u0}^2 & \\sigma_{u01} \\\\ \\sigma_{u01} & \\sigma_{u1}^2 \\end{pmatrix} \\right)\n\\]\nモデル全体としては\n\\[\ny_{ij} = (\\beta_0 + u_{0i}) + (\\beta_1 + u_{1i}) x_{ij} + e_{ij}\n\\]\nとなる。\n\n# データ生成\nset.seed(17)\nn_person &lt;- 10  # 個人数\nn_obs &lt;- 20     # 各個人の観測数\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nsigma_u0 &lt;- 1.0  # 切片の個人差の標準偏差\nsigma_u1 &lt;- 0.5  # 傾きの個人差の標準偏差\nrho &lt;- 0.3       # 切片と傾きの相関\nsigma_e &lt;- 0.5   # 誤差の標準偏差\n\n# 切片と傾きの共分散行列\nSigma &lt;- matrix(c(sigma_u0^2, rho*sigma_u0*sigma_u1, \n                  rho*sigma_u0*sigma_u1, sigma_u1^2), \n                nrow = 2)\n\n# 個人ごとのランダム効果（切片と傾き）\npacman::p_load(MASS)\nrandom_effects &lt;- mvrnorm(n_person, mu = c(0, 0), Sigma = Sigma)\nperson_intercepts &lt;- random_effects[, 1]\nperson_slopes &lt;- random_effects[, 2]\n\n# データフレーム作成\ndf_random_both &lt;- expand_grid(\n  person = 1:n_person,\n  obs = 1:n_obs\n) %&gt;%\n  mutate(\n    x = runif(n(), min = 0, max = 10),\n    u_0 = person_intercepts[person],\n    u_1 = person_slopes[person],\n    y = (beta_0 + u_0) + (beta_1 + u_1) * x + rnorm(n(), mean = 0, sd = sigma_e),\n    person_factor = factor(person, levels = as.character(1:10))\n  )\n\n## データの確認\ndf_random_both %&gt;% head()\n\n# A tibble: 6 × 7\n  person   obs     x   u_0    u_1     y person_factor\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        \n1      1     1  7.52  1.12 -0.351 15.2  1            \n2      1     2  6.34  1.12 -0.351 12.8  1            \n3      1     3  2.48  1.12 -0.351  6.12 1            \n4      1     4  5.51  1.12 -0.351 11.3  1            \n5      1     5  2.35  1.12 -0.351  5.18 1            \n6      1     6  2.59  1.12 -0.351  5.38 1            \n\n# p1: 線型回帰（全体で一つの回帰線）\np1 &lt;- ggplot(df_random_both, aes(x = x, y = y)) +\n    geom_point(alpha = 0.5, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"線型回帰（固定効果のみ）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ランダム切片ランダム傾きモデル（個人ごとに異なる切片と傾きの回帰線）\np2 &lt;- ggplot(df_random_both, aes(x = x, y = y, color = person_factor)) +\n    geom_point(alpha = 0.6, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"ランダム切片ランダム傾きモデル\") +\n    theme(\n      text = element_text(family = \"IPAexGothic\"),\n      legend.position = \"none\"\n    )\n\np1 + p2\n\n\n\n\n\n\n\n\nプロットを見ると，個人ごとに色分けしていない図(左；固定効果のみの線型回帰)だけ見て単純な線型回帰でもいいように思えるが，色分けした図(右；ランダム切片ランダム傾きモデル)をみると，より個体差をうまく捉えていることがわかる。データはまず可視化することが重要であることがわかるだろう。\nさて，これについての推定コードの書き方も見ておこう。切片と傾きの両方が変量効果として()の中に記述される。\n\nresult.bayes.random_both &lt;- brm(\n    y ~ x + (1 + x | person),\n    family = gaussian(),\n    data = df_random_both,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 2.2 seconds.\nChain 3 finished in 2.2 seconds.\nChain 1 finished in 2.4 seconds.\nChain 4 finished in 2.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.3 seconds.\nTotal execution time: 2.4 seconds.\n\nsummary(result.bayes.random_both)\n\nWarning: There were 7 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x + (1 + x | person) \n   Data: df_random_both (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~person (Number of levels: 10) \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)        0.96      0.29     0.55     1.70 1.00     1083     2237\nsd(x)                0.34      0.10     0.20     0.59 1.00     1244     1640\ncor(Intercept,x)     0.33      0.30    -0.34     0.80 1.00     1492     2408\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.83      0.33     0.14     1.47 1.00     1360     1763\nx             1.65      0.11     1.43     1.89 1.00     1064     1454\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.53      0.03     0.47     0.58 1.00     4200     2756\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.random_both)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\nresult.ml.random_both &lt;- lmer(y ~ x + (1 + x | person), data = df_random_both)\nsummary(result.ml.random_both)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x + (1 + x | person)\n   Data: df_random_both\n\nREML criterion at convergence: 385\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4362 -0.6324 -0.0689  0.5880  2.7101 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n person   (Intercept) 0.62433  0.7901       \n          x           0.07568  0.2751   0.43\n Residual             0.27260  0.5221       \nNumber of obs: 200, groups:  person, 10\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)  0.83552    0.26122 8.76380   3.199   0.0112 *  \nx            1.65027    0.08804 8.98000  18.745 1.65e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.369 \n\n\n変量効果のモデル表記で，(1 + x | person)は切片と傾きの両方にランダム効果を考慮することを表す。1は切片，xは傾きのランダム効果を指定している。\nこのモデルでは切片と傾きの相関も推定される。設定した相関（\\(\\rho\\) = 0.3）がどの程度再現されているか確認してみよう。また，個人レベルの推定値も前述の方法で取り出すことができる。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html#階層線型モデルhlm",
    "href": "chapter13.html#階層線型モデルhlm",
    "title": "13  線型モデルの展開",
    "section": "13.4 階層線型モデル(HLM)",
    "text": "13.4 階層線型モデル(HLM)\n階層線型モデルは，さらに確率分布をMixしていくモデルである。 個人差変数を変量効果として考えた時，ある個人\\(i\\)から複数の反応\\(j\\)を得てデータとしているのであった。これを個人に「ネストされた」データ(入子状のデータ。マトリョーシカのようなもの)として考えると，階層線型モデルのイメージに近い。\n階層線型モデルが扱うのは，階層構造を持つデータである。例えばある学級での教育効果を考えるとして，調査研究の範囲がひろがれば学級間比較，学校間比較となっていくだろう。学級は学校にネストされているし，学校は地区にネストされているし，地区は市区町村に，市区町村は都道府県にネストされている，という形で階層を考えることができる。\nまた，学級の中の個人，個人の中の課題種別・・・と内側にネストしていくこともできる。ポイントはネストのレベルに対応した確率分布を仮定するから，そのネストレベルに含まれる各要素は質が同じで相互に交換可能なものであると考える点である。例えば記憶実験などにおいて，同程度の「無意味さ」を持った単語を覚えると言ったとき，「めぬそ」「ぬきは」といっった言葉はどちらを刺激として与えられても三文字の無意味な綴りであるという点では等質と皆しているのである。\nまた階層線型モデルの面白いところは，上位の変数が下位の変数に影響を与えるようなモデルや，その逆のようなレベル間関係をモデリングできるところである。たとえば市区町村レベルの住民の数が，学級レベルの教育水準に影響をしているかもしれない。こういったレベル間関係も(理論的には)表現できるのである。\nどこにレベルを想定するか，そのレベルを想定する意味があるかどうかについても検証する必要がある。過度に複雑なモデルにしても意味がないので，そのレベルで一旦まとめる必要があるかどうかを，事前にチェックする必要がある。レベルでまとめることの意義は，級内相関(Interclass Correlation Coefficient)を見ることで判断する。\n\n13.4.1 2レベル階層線型モデル\n最も基本的な階層線型モデルは2レベルモデルである。ここでは学級（クラスター）にネストされた生徒の学習データを例に，2レベル階層線型モデルについて説明する。\n\n13.4.1.1 レベル1（個人レベル）のモデル\n個人\\(i\\)（\\(i = 1, 2, \\ldots, n_j\\)）が学級\\(j\\)（\\(j = 1, 2, \\ldots, J\\)）に属している場合，レベル1のモデルは以下のように表される：\n\\[\nY_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + r_{ij}\n\\]\nここで：\n\n\\(Y_{ij}\\)：学級\\(j\\)の生徒\\(i\\)の学習成績\n\\(X_{ij}\\)：学級\\(j\\)の生徒\\(i\\)の説明変数（例：学習時間）\n\\(\\beta_{0j}\\)：学級\\(j\\)の切片（学級\\(j\\)の平均的な学習成績）\n\\(\\beta_{1j}\\)：学級\\(j\\)の傾き（学級\\(j\\)における説明変数の効果）\n\\(r_{ij}\\)：個人レベルの誤差項，\\(r_{ij} \\sim N(0, \\sigma^2)\\)\n\n\n\n13.4.1.2 レベル2（学級レベル）のモデル\n学級レベルでは，レベル1の係数が学級レベルの変数の関数として表される：\n\\[\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01}W_j + u_{0j}\n\\]\n\\[\n\\beta_{1j} = \\gamma_{10} + \\gamma_{11}W_j + u_{1j}\n\\]\nここで：\n\n\\(W_j\\)：学級\\(j\\)の説明変数（例：学級規模）\n\\(\\gamma_{00}\\)：全体の切片（全学級の平均的な学習成績）\n\\(\\gamma_{01}\\)：学級レベル変数\\(W_j\\)の切片への効果\n\\(\\gamma_{10}\\)：全体の傾き（全学級の平均的な効果の大きさ）\n\\(\\gamma_{11}\\)：学級レベル変数\\(W_j\\)の傾きへの効果\n\\(u_{0j}, u_{1j}\\)：学級レベルの誤差項（変量効果）\n\n変量効果は多変量正規分布に従うと仮定される：\n\\[\n\\begin{pmatrix} u_{0j} \\\\ u_{1j} \\end{pmatrix} \\sim MVN\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\tau_{00} & \\tau_{01} \\\\ \\tau_{01} & \\tau_{11} \\end{pmatrix} \\right)\n\\]\n\n\n13.4.1.3 統合モデル\nレベル1とレベル2のモデルを統合すると：\n\\[\nY_{ij} = \\gamma_{00} + \\gamma_{01}W_j + \\gamma_{10}X_{ij} + \\gamma_{11}W_j X_{ij} + u_{0j} + u_{1j}X_{ij} + r_{ij}\n\\]\nこの式は以下のように分解できる：\n固定効果（Fixed Effects）：\n\n\\(\\gamma_{00}\\)：全体切片\n\\(\\gamma_{01}\\)：学級レベル変数の主効果\n\\(\\gamma_{10}\\)：個人レベル変数の主効果\n\\(\\gamma_{11}\\)：クロスレベル交互作用効果\n\n変量効果（Random Effects）：\n\n\\(u_{0j}\\)：学級の切片における変量効果\n\\(u_{1j}X_{ij}\\)：学級の傾きにおける変量効果\n\\(r_{ij}\\)：個人レベルの誤差\n\n\n\n13.4.1.4 級内相関係数（ICC）\n階層構造の意義を評価するために，級内相関係数（Intraclass Correlation Coefficient: ICC）を計算する。ICCは同じクラスター内の観測値間の相関を表す：\n\\[\n\\text{ICC} = \\frac{\\tau_{00}}{\\tau_{00} + \\sigma^2}\n\\]\nここで：\n\n\\(\\tau_{00}\\)：学級間分散（between-group variance）\n\\(\\sigma^2\\)：学級内分散（within-group variance）\n\nICCが0に近い場合，階層構造を考慮する必要性は低く，ICCが大きい場合（一般的に0.05以上），階層モデルの適用が推奨される。\n\n\n\n13.4.2 モデルの図解\n数式で表現することでわかりにくくなったかもしれないので，図解しておく。この図は下から順にみていくとよい。 データ\\(Y_{ij}\\)が図の底にあり，これをある正規分布が生成していると考える。この正規分布の幅\\(\\sigma_e^2\\)に応じて個人レベルの誤差\\(r_{ij}\\)が生成される。この正規分布の平均は会期モデルで表現されるが，その時の回帰係数が多次元正規分布によって生成される。多次元正規分布はそれぞれの位置(平均)と幅をもち，さらに両者の相関関係\\(\\rho\\)も含まれる。切片\\(\\beta_{0j}\\)および傾き\\(\\beta_{1j}\\)に生じる誤差\\(u_{0j},u_{1j}\\)が幅\\(\\tau_{00},\\tau_{11}\\)に応じて生成され，またその平均は学級レベルの変数で説明される会期式になっている。ここでの切片と傾きについては事前情報がないので，無情報分布(一様分布)で表している。誤差の事前分布は負の数を取らないように，切断された分布をもちいる。cauchy分布やt分布など，裾の重い分布が用いられることが多い。\n\n\n\nHLMのモデル図\n\n\n\n\n13.4.3 具体例\n具体的な応用例を見ておこう。 ここでは，これまで用いてきた野球のデータを用いる。このデータには選手の個人成績（身長，体重，安打数など）と，選手が所属するチーム，守備位置といった階層構造が含まれている。\n野球データの特徴として，選手は特定のチームに所属し，そのチーム内で特定の守備位置を担当するという階層構造がある。つまり，守備位置はチームにネストされた構造となっている。チームレベルでは戦術や指導方法が共通し，同じチーム内の守備位置レベルでは求められる技能や体格が類似する傾向がある。このような階層構造を無視して分析すると，個人差を過大評価したり，統計的推論に誤りが生じる可能性がある。\n本例では2020年度のデータに限定し，投手を除く野手のデータのみを使用する。投手は他の野手とは明らかに異なる特性を持つため，分析から除外することで，より一貫性のあるデータセットとする。\nまずは階層構造の意義を確認するため，チームおよび守備位置のICC（級内相関係数）を計算し，階層モデルの適用が適切かどうかを検証する。ICCの計算にはmultilevelパッケージを用いた。\n\npacman::p_load(tidyverse, brms, bayesplot, multilevel)\ndat &lt;- read_csv(\"Baseball.csv\") %&gt;%\n    filter(Year == \"2020年度\") %&gt;%\n    mutate(\n        position = as.factor(position)\n    ) %&gt;%\n    filter(position != \"投手\")\n\nRows: 7944 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): Year, Name, team, bloodType, UniformNum, position\ndbl (11): salary, height, weight, Games, AtBats, Hit, HR, Win, Lose, Save, Hold\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate ICC using multilevel package\n# チームレベルのICC\nicc_team &lt;- multilevel::ICC1(aov(Hit ~ team, data = dat))\n\n# ネスト構造を考慮したICC計算\n# チーム内ポジションのICC\ndat$team_position &lt;- paste(dat$team, dat$position, sep = \"_\")\nicc_team_position &lt;- multilevel::ICC1(aov(Hit ~ team_position, data = dat))\n\n# 参考：ポジション単独のICC\nicc_position &lt;- multilevel::ICC1(aov(Hit ~ position, data = dat))\n\nprint(paste(\"Team ICC1:\", round(icc_team, 3)))\n\n[1] \"Team ICC1: -0.031\"\n\nprint(paste(\"Team:Position ICC1:\", round(icc_team_position, 3)))\n\n[1] \"Team:Position ICC1: -0.029\"\n\nprint(paste(\"Position ICC1:\", round(icc_position, 3)))\n\n[1] \"Position ICC1: 0.046\"\n\n\nICCの値が負になっているので，群内のばらつきの方が群間のばらつきよりも大きいことがわかる。つまり群に分けた意味がないことを意味している。本来はある程度の正の大きさのICCを持つことが望ましい4。ただし，チーム\\(\\times\\)ポジションの組み合わせで群をつくると，群内の類似性が現れるため，これを群わけ変数として用いることにしよう。\n従属変数を安打数(Hit)，説明変数をゲーム出場回数(Games)ならびに選手の体躯(heightとweight)とする。 これらがチームの中でのポジションでネストされていて切片が異なるとしたモデルを構成する。なお安打数は正の整数しか取らないから，ポアソン分布を用いた。\n\n# ネスト構造を考慮したポアソンモデル\n# (1 | team) + (1 | team:position) でポジションをチーム内にネスト\nmodel_hit &lt;- brm(\n    Hit ~ Games + height + weight + (1 | team:position),\n    data = dat,\n    family = poisson(),\n    chains = 4,\n    iter = 10000,\n    cores = 4\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n# Model summary for Hit model\nsummary(model_hit)\n\nMCMCのチェックのプロットを見ておく。Rhatなどの数値も良好である。\n\nplot(model_hit)\n\n\n\n\n\n\n\n\nモデルがデータにあっているかどうかを確認する一つの方法が，事後予測分布による可視化である。モデルから生成されるデータの分布を実際のデータに重ねてみて，同じような分布が得られているかを見ておく。 bayesplotパッケージのpp_check関数は，事後予測分布の一部とデータとの重なり具合を可視化してくれる。これを見ると，安打数がごく少ないあたりの分布はうまく適合していない。モデルとして改良の余地があるところだろう。\n\n# Posterior predictive check for Hit model\npp_check(model_hit, ndraws = 100)\n\n\n\n\n\n\n\n\n最後に，グループごとのプロットを見てみよう。チーム差，ポジションの違いといった細かい調整により，よりうまくデータに適合している様子が見てとれるだろう。\n\npredicted_hit &lt;- fitted(model_hit,\n    newdata = dat,\n    allow_new_levels = TRUE\n) %&gt;% as.data.frame()\n\n\nplot_data &lt;- data.frame(\n    observed = dat$Hit,\n    predicted = predicted_hit$Estimate,\n    team = dat$team,\n    position = dat$position\n)\n\nggplot(plot_data, aes(x = observed, y = predicted, color = team)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n    labs(\n        x = \"Observed Hit\", y = \"Predicted Hit\",\n        title = \"Predicted vs Observed Hit Count (2020年度)\",\n        color = \"Team\"\n    ) +\n    facet_wrap(team ~ position, scales = \"free\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html#課題",
    "href": "chapter13.html#課題",
    "title": "13  線型モデルの展開",
    "section": "13.5 課題",
    "text": "13.5 課題\n\n13.5.1 基本問題：パラメータリカバリ\nbrmsパッケージを使用してベイズ統計モデリングにおけるパラメータリカバリを実践してください。パラメータリカバリとは、既知のパラメータで生成したデータから、そのパラメータを正しく推定できるかを確認する手法のことです。\n\n13.5.1.1 課題1-1: ロジスティック回帰のパラメータリカバリ\n\n# ロジスティック回帰のパラメータリカバリ\n# Step 1: 既知のパラメータでデータを生成\nset.seed(123)\nn &lt;- 200\ntrue_intercept &lt;- 0.5\ntrue_slope &lt;- 1.2\nx &lt;- rnorm(n, mean = 0, sd = 1)\np &lt;- plogis(true_intercept + true_slope * x)  # logistic function\ny &lt;- rbinom(n, size = 1, prob = p)\n\n# データフレーム作成\ndf_logistic &lt;- data.frame(x = x, y = y)\n\n# Step 2: brmsでベイズ推定\nmodel_logistic &lt;- brm(\n    y ~ x,\n    family = bernoulli(),\n    data = df_logistic,\n    prior = c(\n        prior(normal(0, 2.5), class = Intercept),\n        prior(normal(0, 2.5), class = b)\n    ),\n    chains = 4,\n    iter = 2000,\n    cores = 4,\n    refresh = 0\n)\n\n# Step 3: 結果の確認\nsummary(model_logistic)\nplot(model_logistic)\n\n# Step 4: パラメータリカバリの評価\n# 真の値との比較\nposterior_summary(model_logistic)\n\n課題: 上記のコードを実行し、真の切片(0.5)と傾き(1.2)が適切に推定されているかを確認してください。また、事後分布の95%信頼区間に真の値が含まれているかを確認してください。\n\n\n13.5.1.2 課題1-2: ポアソン回帰のパラメータリカバリ\n\n# 課題1-2: ポアソン回帰のパラメータリカバリ\nset.seed(456)\nn &lt;- 300\ntrue_intercept &lt;- 1.0\ntrue_slope &lt;- 0.8\nx &lt;- runif(n, min = 0, max = 3)\nlambda &lt;- exp(true_intercept + true_slope * x)\ny &lt;- rpois(n, lambda = lambda)\n\ndf_poisson &lt;- data.frame(x = x, y = y)\n\n# ベイズ推定\nmodel_poisson &lt;- brm(\n    y ~ x,\n    family = poisson(),\n    data = df_poisson,\n    prior = c(\n        prior(normal(0, 2.5), class = Intercept),\n        prior(normal(0, 2.5), class = b)\n    ),\n    chains = 4,\n    iter = 2000,\n    cores = 4,\n    refresh = 0\n)\n\nsummary(model_poisson)\nplot(model_poisson)\n\n課題: 真の切片(1.0)と傾き(0.8)が適切に推定されているかを確認してください。\n\n\n\n13.5.2 応用問題：野球データの階層モデリング\n野球データBaseball.csvを用いて、Year、Teamでネストされた階層構造を持つモデルを構築します。2018年度から2020年度の3年間のデータを使用し、投手のデータに限定します。変数には年俸(salary)、身長(height)、体重(weight)のほか、投手の成績として勝利数(Win)、負け数(Lose)、ホールドポイント(Hold)、セーブポイント(Save)があります。これらの変数を用いて、年度・チームの階層効果を考慮したモデリングを行います。\n\ndat &lt;- read_csv(\"Baseball.csv\") %&gt;%\n  filter(Year %in% c(\"2018年度\", \"2019年度\", \"2020年度\")) %&gt;%\n  filter(position == \"投手\") %&gt;%\n  filter(!is.na(Win), !is.na(Lose), !is.na(Games), !is.na(salary), !is.na(height), !is.na(weight)) %&gt;%\n  mutate(\n    Year = as.factor(Year),\n    team = as.factor(team)\n  )\n\n\n13.5.2.1 モデルA:ポアソン回帰モデル\n次のモデルを実行してください。\n\nmodel1 &lt;- brm(\n  Win ~ height + weight + Games + (1 | team) + (1 | Year),\n  family = poisson(),\n  data = dat,\n  iter = 2000,\n  cores = 4\n)\n\n課題1: このモデルの構造を説明してください。以下の点について述べてください：\n\n従属変数と確率分布\n固定効果(説明変数)の意味\n変量効果(階層構造)の意味\nなぜこの確率分布を選んだのか\n\n課題2: モデルの結果を解釈してください。以下の観点が含まれるといいでしょう。：\n\n各固定効果の係数の意味と統計的有意性\n変量効果の分散の大きさとその解釈\n\n課題3: 以下の可視化を行ってください：\n\nplot(model1) でMCMCの収束診断\npp_check(model1) で事後予測チェック\nチーム別・年度別の予測値と実測値の比較プロット\n\n\n\n13.5.2.2 モデルB:対数正規モデル\n次のモデルを実行してください。\n\nmodel2 &lt;- brm(\n  log(salary) ~ Win + Lose + (1 + Games | team) + (1 | Year),\n  family = gaussian(),\n  data = dat,\n  iter = 2000,\n  cores = 4\n)\n\n課題1: このモデルの構造を説明してください。以下の点について述べてください：\n\n従属変数の対数変換の理由\n固定効果(Win, Lose)の経済的意味\n変量効果の構造(ランダム切片・ランダム傾き)の意味\n\n課題2: モデルの結果を解釈してください。以下の観点が含まれるといいでしょう。：\n\n勝利数・敗戦数が年俸に与える影響（%変化として）\nチーム間の年俸格差とGames効果の違い\n年度効果の大きさと経済的解釈\n\n課題3: 以下の可視化を行ってください：\n\nplot(model2) でMCMCの収束診断\nチーム別のGames効果の違いを可視化\n実際の年俸と予測年俸の散布図\n\n\n\n\n\nHox, J. 2002. Multilevel Analysis: Techniques and Applications. Lawrence Erlbaum Associates.\n\n\nRaudenbush, S.W., and Liu.X. 2000. “Statistical Power and Optimal Design for Multisite Randomnized Trials.” Psychological Methods 5: 199–213.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html#footnotes",
    "href": "chapter13.html#footnotes",
    "title": "13  線型モデルの展開",
    "section": "",
    "text": "エクスキューズが許されるなら，当時の統計学，計算機科学は現代に比べると貧弱で，理論的に正しくないことがわかっていても，ユーザにはそれ以上のことができなかったということがある。また心理学で測定されるデータは，あくまでも目に見えない心を表現したラフな近似値で，統計学上の仮定に違反したことで生じる問題よりも，そもそも本質的な問題があるのだから，統計の運用もあくまでもラフな近似値で良い，という風潮があったのかもしれない。↩︎\n一般化線型モデルと一般線型モデルは，一文字「化」が入るかどうかの微妙な違いである。英語ではgeneralとgeneralizedの違いなので，こちらで理解した方が間違いが少なくて良いかもしれない。↩︎\nもちろん正確には自由度で割った平均平方(Mean Squares)の比が検定統計量の\\(F\\)値である。↩︎\n0.05を小，0.10を中，0.15. を大とする基準が最も一般的である(Hox (2002),Raudenbush and Liu.X (2000))。↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線型モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter14.html",
    "href": "chapter14.html",
    "title": "14  多変量解析(その1)",
    "section": "",
    "text": "14.1 多変量解析の包括的な説明\nここでは心理系で最もよく使われる分析法のひとつ，因子分析を中心に多変量解析の全体的な解説を行う。\n多変量解析はその名のとおり，変数が多く含まれるデータの解析であり，その目的は情報の要約にある。多くの変数が含まれる時にひとつ一つの変数を解釈していくのは大変な労力であるから，要領よくまとめることができればそれに越したことはないからである。 この目的から派生して，多変量解析にはさまざまな意味解釈が付随する。以下にモデルの解釈と対応する多変量解析技術を列記してみた。\nこれらのモデルに共通する本質的な特徴は「変数間関係をデータから見出すこと」である。変数間関係をどのようなもので表現するかによって，多少モデルの扱い方は変化する。一般的には分散共分散(相関係数)を変数間関係とするが，この場合は間隔尺度水準以上のデータが得られていることが必要になる。もし順序尺度水準でしか得られていないのであれば，ピアソンの相関係数の代わりにポリコリック相関係数やポリシリアル相関係数と呼ばれる相関係数を用いることになる。0/1のバイナリデータの場合はさらに特殊で，ピアソンの相関係数の代わりにテトラコリック相関係数を用いることになる。\n変数間関係は必ずしも相関係数だけではない。カテゴリカル変数の場合は，あるカテゴリと他のカテゴリが同時に選択される・発生する頻度(共頻関係)を，その変数間関係の指標として捉えることができる。共頻関係を分析する手法としては双対尺度法(西里 2010) (対応分析や数量化III類と原理的に同じ)などが知られている。このようなカテゴリデータの分析は，自由記述などの自然言語を形態素解析し，多変量解析で分析するテキストマイニングなどで応用されている。\nまた，変数間関係を変数同士の類似度，すなわち距離であると考えることもできる。距離の公理を満たすデータがあれば，それを元に多次元尺度構成法(高根 1980; 岡太 and 今泉 1994)で可視化したり，クラスター分析で分類(新納 2007; 足立 2006)したりすることができる。\nさらに，相関係数は変数間の直線的な関係の強さを意味するが，隠れた変数による擬似相関の可能性も含まれるため，偏相関係数を使って周囲の変数からの影響を統制した変数間関係を考えることもある。この手法はグラフィカルモデリング(宮川 1997)やネットワーク分析(アデラ＝マリア et al. [2022] 2024)で用いられるものである。\nいずれにせよ，こうした変数間関係をもとに，外的な基準があればそこに対するフィッティングを目的として未知数を推定するし，外的な基準がなければモデル的仮定に基づいてデータから構造化していくことになる。またその目的の基本は情報の要約であるが，可視化に重点をおいたモデルや潜在得点の推定に重点を置いたモデルなど，各種モデルによって得意とする場所や理論的に強調されるところは異なる。\nやや本筋から外れるが，こうした多変量を同時に扱うための数学的基盤として，線型代数の知識が必要となることが少なくない。線型代数はベクトルや行列の演算体系であり，その利点は「計算」と「可視化」を統合する観点を得られるところにある。より深く理解したいものにとっては，これらの学習も合わせて行うことを期待する。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#多変量解析の包括的な説明",
    "href": "chapter14.html#多変量解析の包括的な説明",
    "title": "14  多変量解析(その1)",
    "section": "",
    "text": "要約する＝すべての情報を使わず，いくつかの情報を捨象することでもある。\n1つの合成変数にまとめ上げる：総合評価として一次元化することでもある(主成分分析)。\n2つ3つなど少数の次元に写像する：多次元空間に可視化することでもある(多次元尺度法)。\n少数のグループに変数を分類する：グループごとの特徴を記述したり分析することで解釈を深める(クラスター分析)。\n観測変数の背後にある少数の潜在変数にまとめ上げる：構成概念を測定して数値を割り振る(因子分析)。\n観測変数が0/1のバイナリデータに対して1つの潜在変数からの影響を考える：正答誤答を意味するテストと考える(項目反応理論)\n観測された回答パターンから潜在変数を仮定したグループ分類を行う：隠れた購買層の発見などに使われる(潜在クラス分析)\n潜在変数間の構造的な関係もモデル化する：回帰分析や因子分析などの線型関数関係をデータ全体に当てはめて考察する(構造方程式モデリング)\n潜在変数を仮定せず，変数同士の結びつきの強さを可視化する：ノードnodeとタイtieで位相的な関係を描画したり，数学的構造を含めたモデリングを行ったりする(ネットワーク分析)\n\n\n\n\n\n\n\n\n14.1.1 データキューブと断面\nCattel.R.Bは共変図covariation chart，あるいはdata cubeと呼ばれる図を描いてデータの分類をしている。彼は，どんな観察あるいは測定事象でも，\n\n観察が行われる時timeあるいは場所occasion\n観測される変数variable\nその変数が属せられる場所reference pointあるいは個人person\n\nの3つの属性で規定できるとしている。\n\n\n\nCattelのデータキューブ\n\n\n心理学におけるデータというと，行に人，列に変数のある2次元行列(あるいは表計算ソフトのスプレッドシートと言えばいいだろうか)を想定するが，それはデータの時間軸でカットした一時点のものである。しかしこのキューブは他の軸でも断面を切り出すことができるし，行・列の入れ替えを考えると6種類の共変動を計算できる。\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\nQ\nO\nP\nS\nT\n\n\n\n\n技法\n変数\\(\\times\\)人\n人\\(\\times\\)人\n時・所\\(\\times\\)時・所\n人\\(\\times\\)時・所\n時・所\\(\\times\\)変数\n変数\\(\\times\\)時・所\n\n\n実験計画\n変数\\(\\times\\)人\n人\\(\\times\\)時・所\n時・所\\(\\times\\)変数\n変数\\(\\times\\)時・所\n人\\(\\times\\)変数\n時・所\\(\\times\\)人\n\n\n共変動(取り出す因子)\n変数\n人\n時・所\n人\n時・所\n変数\n\n\n\nCattelは各段面における因子分析を「technique」として分類した。一般的な因子分析は，変数と人の組み合わせから，変数同士の相関行列を計算して変数の背後にある因子を抽出する。これはR技法である。これに対し，人と人の相関行列を計算して，人因子を抽出することもできる。これはQ技法として知られている。あるいは，人と時間・場所の断面で切り出して，時間の因子を出すこともできるし(O技法)，この面から見た人の因子を取り出しても良い(P技法)。同様に，S技法，T技法を考えることもできる。S技法を用いた実践例は確認されていないが，それ以外の方法についてはそれぞれ対応する研究もわずかながら存在する。\n変数間関係は距離行列(類似度行列)でもいいし，共頻関係でも良い。例えば距離行列を使って変数をクラスター分析で分類しても良いし，人を分類しても良い。どの共変動，どの側面で分析するかは損外自由なのである。\n昨今では3次元という制約を外してデータキューブを一般化し，含まれるデータの種類(相mode) とその組み合わせ回数(元way)で分類する。例えば，ある集団においてメンバーが相互に評定し合うデータを，複数の集団に対して取るとすると，含まれるデータはメンバーと集団の2種類で，メンバー\\(\\times\\)メンバーの関係が並ぶので，2相3元データ(2-mode 3-way data)などと呼ぶ。家族システム論ではこうしたデータを扱うことになる。\n多変量解析はこのように，どの組み合わせにおいて，どの側面からアプローチするか，またどの要素を要約するかといったということを考える一般的な技法なのである。繰り返すが，「この側面で切り取ってはいけない」といった制約はなく，分析者はデータの全体像をイメージしながら，実践的に意味のある側面で自由に切り取ると良い。それができるような数学的・統計的モデル上の制約はなく，Rをはじめとするパッケージを駆使すればいかなる分析も可能である。\n多変量解析の全体像を俯瞰したところで，心理学で最も愛されてよく使われている因子分析について詳しく見ていくことにしよう。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#因子分析",
    "href": "chapter14.html#因子分析",
    "title": "14  多変量解析(その1)",
    "section": "14.2 因子分析",
    "text": "14.2 因子分析\nここでは心理学で最もよく用いられる手法の一つである，因子分析法について概説する。 因子分析法は測定についての統計モデルである。類似の手法として主成分分析があげられるが，主成分分析は測定のモデルというより要約のモデルというべきである。\n因子分析のモデルは次の式で表される。 \\[ z_{ij} = a_{j1}f_{i1}+a_{j2}f_{i2}+\\cdots+a_{jm}f_{im}+d_jU_{ij} \\]\nここで\\(z_{ij}\\)は個人\\(i\\)の項目\\(j\\)に対するスコアを標準化したものである。\\(a_{j.}\\)は項目\\(j\\)の因子負荷量(factor loadings)，\\(f_{i.}\\)は個人\\(i\\)の因子得点(factor score)，\\(d_j\\)は項目\\(j\\)の独自因子負荷量，\\(U_{ij}\\)は項目\\(j\\)に伴う個人\\(i\\)の独自因子得点である。\\(a_{j.}\\)で表される\\(m\\)個の因子を共通因子と呼ぶ。一般に\\(m\\)は項目数\\(M\\)よりもかなり小さい。例えば性格検査のBIG-fiveは\\(M=25\\)で\\(m=5\\)である。YG正確検査は\\(M=120\\)で\\(m=12\\)である。この意味で多変量解析の目的の一つ，情報圧縮のモデルであるということもできる。\nこれに対して主成分分析は次のように表される。 \\[ P_{i} = w_1X_{i1} + w_2X_{i2} + \\cdots + w_MX_{iM} \\]\nここで\\(X_{i.}\\)は個人\\(i\\)の項目\\(j\\)に対する反応を表し，この重み付き線型結合で主成分\\(P\\)を形成する。ここでの未知数は\\(w_j\\)であるから，一つの合成変数に作るための最適な重みを見つけることが目的となる。その基準の一つが，生成される合成変数\\(P\\)が個人\\(i\\)の特徴を最大限際立たせるように，すなわち\\(P_i\\)の分散を最大にすることと考える。もちろん一つの合成変数で\\(M\\)個の変数が持つ情報をすべて反映させることは難しいので，第二，第三の合成得点(主成分)を作ることも可能である。\nそうすると，情報圧縮という観点から見た\\(m\\)個の共通因子，\\(m\\)個の主成分の違いは何だろうか。これはすでに述べたように，因子分析は測定のモデルなので，得られたデータ\\(z_{ij}=\\frac{X_{ij}-\\bar{X_j}}{\\sigma_j}\\)には誤差\\(d_jU_{ij}\\)が含まれていると考えているのに対し，主成分分析では\\(X_{ij}\\)にそれを仮定せず，得られた値をそのまま用いているところが異なる。\n実践的な面では，心理尺度のような反応に誤差が仮定されるものには因子分析を用い，公的な記録など値に誤差が想定されないものには主成分分析を用いることが相応しい。因子分析が心理学やテスト理論の領域で広まり，主成分分析が経済学，商学，社会学の領域で広まったのはそうした背景による。\n計算論的には，いずれも変数間関係を元に最大限説明できる要素を抽出するというところで，行列の固有値分解を用いるという点が同じであるから，統計パッケージによっては同じメニューで異なる出力になっているものも少なくない。しかし上で述べたように，モデルの設計上の違いがあることは知っておいて損はないだろう。また，因子分析は変数間関係として相関行列を，主成分分析は分散共分散行列を用いることが多い。これは因子分析を用いる心理学的な領域では，測定値に絶対的な意味がなく相対的な意味(ex.より外向的，より内向的)しかないことに対し，他の社会科学領域では絶対的な意味がある(ex.国家間の貿易黒字・赤字の額など)場合が多いからである。また，主成分分析は多くの変数を情報圧縮する目的で第一主成分のみに注目することが多いのに対し，因子分析は測定しているものの考え方から複数の因子を考えることが多い。因子分析において単因子で考えるか多因子で考えるかについては，知能検査において知能を一般的な単一の因子で考えるのか，各領域に対応する複数の因子があるのかといった，理論的な相違がその黎明期にみられたことを反映している。\n類似の手法ではあるが，こうした背景を知っておくことで適切な手法を用いることができるようになるだろう。\n\n14.2.1 探索的因子分析\n特に断りなく単に因子分析というとき，探索的因子分析(Exploratory Factor Analysis)を指すことが多い。探索的というのは，因子負荷量(因子から項目へのパス係数，因子と項目の関係の強さを反映したもの)はもちろん共通因子の数についても事前に定めず，データから因子構造を探ることを目的とするものだからである。\n探索的因子分析は次のステップで進められる。\n\n因子数の決定\n因子負荷量の推定(因子軸の回転)\n因子得点の推定\n\nもちろん分析に入る前に，分析対象となるデータの記述統計や可視化を通じて基本的な項目属性を把握していることが前提である。\n\n14.2.1.1 因子数の決定\n因子分析を数学的に語れば，\\(M\\)個の項目相互の相関係数を表した相関行列\\(\\mathbb{R}\\)を固有値分解することに尽きる。相関係数はピアソンの積率相関係数を用いることが一般的であるが，項目が順序尺度水準であるとかバイナリ変数であるとかいった場合は，それに応じた相応しい相関係数を用いる。\n固有値分解とは相関行列の次元性を見ることでもある。\\(M\\)個の項目もつ情報は\\(M\\)次元あると考える。例えば2変数\\(X,Y\\)があれば，変数\\(X\\)をx軸，変数\\(Y\\)をy軸に取った2次元空間に核反応をプロットすることでデータ全体の関係を表現できるだろう。しかしこれらの二変数が相関しているなら，変数\\(X,Y\\)を直交させた空間で表現する必要は必ずしもなく，より分散の大きくとれる二次元基底を見つけることができるに違いない。これが因子分析，主成分分析に共通する考え方であり，最大の分散を持つ次元に注目するのが主成分分析，多次元のなかで有用な次元を共通因子，それ以外を誤差因子と区別して多因子(多次元)で考えるのが因子分析である。\nここで共通因子の数を決めるのは分析者であり，「有用な次元」の決定は主観的な側面を含むことに注意しよう。もちろんデータの構造から適した次元数を考える手法はいろいろ提案されており，昨今はより客観的基準で因子数を決定するのが一般的であるが，数学的な特徴から実践的な意味合いをもつ共通次元とみなすのは，あくまでも分析者の責任において行われるものである。\n因子数を決定する手法として，スクリープロットをつかった平行分析がある。 次のコードを見ながら具体的に見ていこう。分析にはpsychパッケージを用い，データはpsychパッケージの持つサンプルデータ，bfiを用いる。 これは性格テストのビッグファイブ因子それぞれについて5項目で測定したデータである。\n\npacman::p_load(tidyverse, psych)\ndat &lt;- psych::bfi |&gt; select(-gender, -education, -age)\n# 並行分析\nfa.parallel(dat)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  6  and the number of components =  6 \n\n\nデータ行列から得られる固有値の大きい順に折れ線グラフを描いたものを，スクリープロットという。 デフォルトではPCすなわち主成分分析Principle Component Analysisのスクリープロットと，FAすなわち因子分析Factor Analysisのスクリープロットが表示されている。この違いは上で述べたように，データに誤差を仮定するかどうかの違いにある。因子分析はこれを仮定するため1つの項目のもつ情報量が1単位以下になる(相関係数\\(r_{jj}\\)が\\(1.0\\)より小さくなる。正確には，\\(r_{jj} = 1-h_j^2 = u^2 &lt; 0\\)であり，ここで\\(h_j^2\\)は共通性と呼ばれる共通因子負荷量の二乗和，\\(u_j^2\\)は独自性因子負荷量の二乗和)ため，主成分分析のそれより必ず低くなる。\nプロットされているのはActual Data, Simulated Data, Resmapled Dataとなっているのがわかるだろう。実際のデータは何らかの意味構造を有しているだろうから，その相関関係にも偏りが生じ，よく説明できる次元とそうでない次元とが生まれるため，徐々に減衰するカーブで表示される。これに対してSimulated Dataは同じサイズの乱数データから，Resampled Dataは実際のデータをごちゃ混ぜにした行列を作って得られた固有値構造を表している。乱数や撹拌したデータは実際の意味構造を持たず，どの次元も均等に無意味になるため，フラットな線で表示されるだろう。このフラットな線と実データの線を比べ，フラットなラインよりも大きな意味がある次元は無意味ではない，と考えて因子数を決めるのが平行分析の考え方である。この考え方に基づくと，因子分析解も主成分分析解も6因子(6成分)がてきせつであるということになる。\nなお図中には固有値が1.0のところにもラインが引かれている。これはかつて使われていたガットマン基準というもので，項目1つ分の分散も持たないような因子は共通因子たり得ない，という考え方である。この考え方によると3因子が妥当ということになる。ただし判断の基準として，共通因子で分散全体の何%を説明したか，というのもあり，たとえば3因子までで50%も説明しないようであれば半分以上の情報を捨てることになるから，4，5因子まで採用するという考え方もあり得る。\n\n\n14.2.1.2 因子負荷量の推定\n因子の数が決まると，その過程のもとで因子負荷量の推定に入る。例えば次のようにして結果を得る。\n\nresult.fa &lt;- fa(dat, nfactors = 6, fm = \"ML\", rotate = \"geominQ\")\n\nLoading required namespace: GPArotation\n\n\npsychパッケージのfa関数は実に多くのオプションを持っているが，ここでは因子数(nfactors)，推定法(fm)，回転法(rotate)の3つを指定した。 因子数はすでに述べたので，推定法と回転法について解説する。\n推定法は，ここでは最尤法(ML)を指定した。サンプルサイズが200を超えるような大きなデータであれば，多変量正規分布のもとからデータが得られたと仮定して因子負荷量を推定するのが最も適切だろう。サンプルサイズが小さい場合は，最小二乗法系列(ULS,OLS,WLS,GLSなど)の推定法を指定し，データとモデルのずれを最も小さくするような手法にするのが良い。特段の指定がなければ最小残差法(minres)が選ばれる。これは最小二乗法と同じだが，アルゴリズムが改善されていて収束しやすいという特徴がある。推定法として主成分解(pa)を選べば，残差を推定しないモデルとなる。アルゴリズムの違い，仮定の違いなどでいろいろ変えうるが，基本的にこれで大きく変化が出るようなものではない。\n回転法は因子負荷量を推定した後で，さらに解釈をしやすくするためのものである。因子分析や主成分分析は，データの持ってる空間的特徴の軸を見つけ直すという説明はすでにした通りだが，この軸は原点こそ決まっているが，線型代数的変換によって軸を任意の方向に回転させることができる。であれば最も解釈がやりやすい方向に回転させるのが実践上便利である。この解釈がやりやすい方向というのを数学的に言い換えるならば，一つは項目と因子の関係が単純構造にあることだろう。単純構造とは，ある項目が特定の因子に寄与しているのなら，そのほかの因子には寄与していないということである。例えば，外向性を測定する項目が第1因子に重く負荷しているのであれば，第2,3,4,5因子には負荷していないほうが解釈しやすい。因子はデータの空間的特徴を表す軸(次元)なのだから，事後的にその軸がの意味であったかを考察する必要があるので，「この因子はこの項目にもあの項目にも影響している」という状況は悩みの種だからである。\nこの基本方針のもと，いくつかの計算法が考えられている。もっとも古典的なバリマックス回転は，因子負荷量の二乗和の分散が最大になるように回転角を定める。ほかにも，オブリミン回転やジオミン回転などさまざまな回転方法が考えられており，これらについて詳しくは 小杉 (2018) などを参照されたい。 また，回転方法は大きく分けて斜交回転と直交回転とに分けられる。直交回転は回転後の軸が直交する，すなわち因子間相関を仮定しない方法であり，斜交回転は因子間相関を仮定する回転方法である。後者の方が数学的な仮定が緩いため，分析の手順としてはまず斜交回転を行い，因子間相関が十分にひくく直交をかていできるなら直交回転をやり直す，という方法をとるべきである。ちなみにここではgeominQというジオミン回転の斜交版を適用して結果を出力している。\nBernaards and Jennrich (2005) パッケージには多くの回転法が含まれており，回転法をrotateオプションで選択することができるので，ヘルプなどを見て理解を深めて欲しい。\n因子軸の回転についても，推定法と同じように絶対的な基準はなく，それぞれの考え方や仮定に基づくアルゴリズムの違いがあるだけである。推定法と違って，因子負荷量は異なる回転法を施すと大きく変わることがある。因子軸の回転は解釈を容易にするためのものであるから，分析者にとって都合の良い回転方法を指定していいが，その回転方法が何で，どういう仮定があるのかについては，自身の言葉で説明できるようになっていた方がいい。\n\n\n14.2.1.3 出力結果を確認する\n推定法，回転法についての概略を踏まえた上で，結果を見てみよう。\n\nprint(result.fa, sort = T, cut = 0.3)\n\nFactor Analysis using method =  ml\nCall: fa(r = dat, nfactors = 6, rotate = \"geominQ\", fm = \"ML\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n   item   ML1   ML2   ML5   ML3   ML4   ML6   h2   u2 com\nE2   12  0.70                               0.55 0.45 1.0\nE1   11  0.58                               0.39 0.61 1.4\nN4   19  0.51  0.35                         0.48 0.52 2.0\nE4   14 -0.50                          0.33 0.56 0.44 2.2\nE5   15 -0.41                               0.40 0.60 2.8\nN2   17        0.84                         0.69 0.31 1.1\nN1   16        0.83                         0.71 0.29 1.1\nN3   18        0.61                         0.52 0.48 1.3\nN5   20  0.33  0.37                         0.34 0.66 2.8\nA2    2              0.70                   0.50 0.50 1.2\nA3    3              0.65                   0.51 0.49 1.1\nA1    1             -0.57              0.37 0.33 0.67 1.8\nA5    5              0.50                   0.48 0.52 1.7\nA4    4              0.42                   0.28 0.72 1.7\nC2    7                    0.67             0.50 0.50 1.2\nC4    9                   -0.60        0.35 0.55 0.45 1.9\nC3    8                    0.54             0.31 0.69 1.1\nC1    6                    0.53             0.35 0.65 1.4\nC5   10                   -0.51             0.43 0.57 1.8\nO3   23                          0.67       0.48 0.52 1.0\nO1   21                          0.58       0.34 0.66 1.1\nO5   25                         -0.49  0.41 0.37 0.63 2.0\nO2   22                         -0.40  0.34 0.29 0.71 2.3\nO4   24  0.40                    0.40       0.25 0.75 2.4\nE3   13                          0.38       0.48 0.52 3.0\n\n                       ML1  ML2  ML5  ML3  ML4  ML6\nSS loadings           2.34 2.25 2.00 1.89 1.77 0.82\nProportion Var        0.09 0.09 0.08 0.08 0.07 0.03\nCumulative Var        0.09 0.18 0.26 0.34 0.41 0.44\nProportion Explained  0.21 0.20 0.18 0.17 0.16 0.07\nCumulative Proportion 0.21 0.41 0.59 0.77 0.93 1.00\n\n With factor correlations of \n      ML1   ML2   ML5   ML3   ML4   ML6\nML1  1.00  0.24 -0.36 -0.20 -0.28 -0.08\nML2  0.24  1.00 -0.01 -0.12  0.05  0.25\nML5 -0.36 -0.01  1.00  0.19  0.28  0.26\nML3 -0.20 -0.12  0.19  1.00  0.14  0.04\nML4 -0.28  0.05  0.28  0.14  1.00  0.11\nML6 -0.08  0.25  0.26  0.04  0.11  1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 6 factors are sufficient.\n\ndf null model =  300  with the objective function =  7.23 with Chi Square =  20163.79\ndf of  the model are 165  and the objective function was  0.36 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  2762 with the empirical chi square  661.28  with prob &lt;  1.4e-60 \nThe total n.obs was  2800  with Likelihood Chi Square =  1013.79  with prob &lt;  4.6e-122 \n\nTucker Lewis Index of factoring reliability =  0.922\nRMSEA index =  0.043  and the 90 % confidence intervals are  0.04 0.045\nBIC =  -295.88\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   ML1  ML2  ML5  ML3  ML4  ML6\nCorrelation of (regression) scores with factors   0.90 0.93 0.89 0.87 0.86 0.78\nMultiple R square of scores with factors          0.81 0.86 0.78 0.76 0.75 0.61\nMinimum correlation of possible factor scores     0.61 0.72 0.57 0.53 0.49 0.22\n\n\nこの出力ではsortオプションとcutオプションを指定した。sortオプションは因子負荷量の大きい順に並べ替えてくれるものであり，cutオプションは因子負荷量の表示を抑制するものである。あくまで表示上のオプションであり，実際は各因子から各項目へのパス(\\(5 \\times 25\\)本)が計算されている。\nまず表示されているのが因子負荷行列であり，項目の因子ごとの負荷量に加え，共通性\\(h_j^2\\)と独自性\\(u_j^2=1-h_j^2\\)，複雑度complexityが示されている1。なお，ここで表示されている因子負荷量などは回転後のパターン行列であり，斜交回転の場合は，因子軸の負荷量をどう考えるかによって因子パターンと因子構造とに分かれる。因子パターンは変数を斜交座標系に直交に投影した影のようなものであり，変数から因子への直接的な効果を表すと考えられる。因子構造は因子構造は変数を各因子軸に平行に投影した影のようなものであり，変数と因子の間の単純相関を表している。\nその下には負荷量の平方和SS loagingsがあり，これが説明する分散の大きさである。それを比率にしたもの(Proportion Var)，累積比率にしたもの(Cumulative Var)がある。今回は累積して44%の説明しかしていないことになるから，56%もの情報をカットしているので，情報圧縮の観点から言えば少し捨てすぎている危険性もある。\n続いて，回転行列に斜交回転を指定しているから，因子間相関が出力されている。これを見ると絶対値最大で-0.36がみられる。全ての因子間相関が\\(\\pm 0.3\\)に収まるようであれば，直交回転を考えても良い。\nその後に出力されているのは適合度に関する指標である。各指標に関する解説は割愛する。\n\n\n14.2.1.4 因子得点の推定\nここまでで因子と項目の関係を探索的に求める方法について見てきたが，心理学的な研究としては因子と回答者の関係についても興味関心をもつだろう。すなわち，「外向性が高い人は誰か」「情緒不安定性が低い人はどういう特徴を持つか」といった，人に対する理解を深めることである。\n数学的には，行列の固有値分解のときに固有値と同時にえらえる固有ベクトルが因子負荷量になる。この相関行列などを構成する時点で，すでに個人の相がもつ情報は要約されて欠落している。なので，因子の構造が明らかになってから，逆算的に個人の得点を考えることになる。因子分析のモデル式で見たように，因子得点は\\(f_{i.}\\)であるが，右辺の因子負荷量がすでに定まっているのなら，左辺も実測値から与えられているので，方程式を解くように答えを求めることができるのである。\npsych::fa関数はデフォルトで因子得点を返すようになっており，以下のコードで確認できる。\n\nhead(result.fa$scores, 10)\n\n              ML1         ML2         ML5         ML3        ML4         ML6\n61617 -0.04010511 -0.13419381 -0.69145376 -1.29299334 -1.6430660 -0.12400705\n61618 -0.35267122  0.09080773 -0.04506789 -0.60267233 -0.0566277  0.39616337\n61620 -0.05290788  0.69698270 -0.68175428 -0.03704608  0.2334657  0.01280638\n61621  0.22194371 -0.07729865 -0.15482554 -0.90538802 -0.9125738  0.95278898\n61622 -0.39695952 -0.28825362 -0.61037363 -0.12382926 -0.5814368  0.21976607\n61623 -1.05223173  0.41585860  0.29450029  1.35906961  0.8457775  0.45985066\n61624 -0.43875292 -1.21974967  0.06875832  0.03201599  0.6001998 -0.69274350\n61629  1.42563085  0.35294078 -2.23058559 -0.99435391 -1.2166723 -1.00627947\n61630          NA          NA          NA          NA         NA          NA\n61633 -0.62399161  1.11010731  0.53805411  1.05139495  0.4870489  0.04325653\n\n\nここで一部NAが返されているところがある(例えばID 61630)，これは回答の中に欠測値が含まれていた場合におこる。\n\nhead(dat, 10)\n\n      A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4\n61617  2  4  3  4  4  2  3  3  4  4  3  3  3  4  4  3  4  2  2  3  3  6  3  4\n61618  2  4  5  2  5  5  4  4  3  4  1  1  6  4  3  3  3  3  5  5  4  2  4  3\n61620  5  4  5  4  4  4  5  4  2  5  2  4  4  4  5  4  5  4  2  3  4  2  5  5\n61621  4  4  6  5  5  4  4  3  5  5  5  3  4  4  4  2  5  2  4  1  3  3  4  3\n61622  2  3  3  4  5  4  4  5  3  2  2  2  5  4  5  2  3  4  4  3  3  3  4  3\n61623  6  6  5  6  5  6  6  6  1  3  2  1  6  5  6  3  5  2  2  3  4  3  5  6\n61624  2  5  5  3  5  5  4  4  2  3  4  3  4  5  5  1  2  2  1  1  5  2  5  6\n61629  4  3  1  5  1  3  2  4  2  4  3  6  4  2  1  6  3  2  6  4  3  2  4  5\n61630  4  3  6  3  3  6  6  3  4  5  5  3 NA  4  3  5  5  2  3  3  6  6  6  6\n61633  2  5  6  6  5  6  5  6  2  1  2  2  4  5  5  5  5  5  2  4  5  1  5  5\n      O5\n61617  3\n61618  3\n61620  2\n61621  5\n61622  3\n61623  1\n61624  1\n61629  3\n61630  1\n61633  2\n\n\n因子得点はモデル式から逆算的に推定するので，一箇所でも値が見つからなければ答えが出ないのである。 また，この推定法による因子得点は標準化されたスコアなので単位がなく，相対的に比較することしかできない。 加えて，推定された相対的なスコアであるから，以下の研究プロセスにおいて差の検定などをすることは不適切である，という考え方もある。\n実践的には簡便的因子得点と呼ばれる因子得点の計算方法がある。これは因子分析の結果，当該因子に関係する項目に着目し，その評定値を平均することで算出するものである。 先の具体的にみていこう。第一因子はE2,E1,N4,E4,E5から構成されていると考えたとする。ここでE4,E5は因子負荷量が負であるから，評定値を逆転して考える必要がある。これを踏まえて，たとえば以下のように計算する。\n\nFscore1.raw &lt;- dat |&gt;\n  # 第一因子に該当しそうな項目だけ抜き出す\n  select(E2, E1, N4, E4, E5) |&gt;\n  # 逆転項目の評定値を反転する\n  mutate(\n    E4 = 7 - E4,\n    E5 = 7 - E5\n  )\n\n# 行ごとに欠損値を除いた平均値を計算する\nFscore1 &lt;- apply(Fscore1.raw, 1, function(x) mean(x, na.rm = TRUE))\nsummary(Fscore1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   2.800   2.892   3.600   6.000 \n\n\nここでは6件法の評定値を逆転させるために，7から引くという操作をして，欠測を除いて平均するようにしている。 こうすることで，尺度値の持つ意味(中点以上が賛成，未満が反対といったような)を踏まえて考えることができるし，全てが欠測でない限りスコアの算出ができるという利点がある。\nただしこの方法は，因子負荷量による項目ごとの重みづけを考えないこと，因子分析法によって除外したはずの誤差分散の情報を含んだスコアにしていること，といった短所をもつ。また，そもそも評定値が尺度構成法で正しくスコアリングされたものであるべきだが，実践的にそのような工夫をしている例はほとんど見られないため，非常に精度の低い，荒い推定値になっていると言わざるを得ない。\nとはいえ，推定法でもとめたものと簡便法で求めたものは，非常に高く相関するので，心理学のデータがそれほどの精度を持つものでないと割り切れるのであれば，勘弁法でも十分だろう。\n\ncor(result.fa$scores[, 1], Fscore1, use = \"pairwise\")\n\n[1] 0.9595003\n\nplot(result.fa$scores[, 1], Fscore1)\n\n\n\n\n\n\n\n\n\n\n\n14.2.2 確認的因子分析\nここまで探索的因子分析について詳しく解説してきた。 そこでは因子数や因子負荷量は事前の情報がなく，データによって語らせる方法で後付け的に解釈を行なっていくのであった。数値例にもあるように，第一因子は主にE因子の項目(外向性,Extraversion)から構成されているが，中にはN因子(情緒不安定性,Neuroticism)の項目も一部含まれており(N4)，解釈に頭を悩ませることも少なくない。そもそもBig5の名前にあるように理論的には5因子なのだが，データは6因子を示す，ということもある。\nこのように探索的因子分析はデータに沿った解釈をするしかないのだが，性格検査のような理論的背景や仮定があるのなら，そちらを重視したいということもあるだろう。 このような場合は，因子の構造や仮定を盛り込んだモデルをデータに当てはめるという，確認的因子分析(Confirmatory Factor Analysis, CFA)を用いる。これは構造方程式モデリング(Structural Equation Modeling)の枠組みで因子分析をとらえたものであり，項目と潜在変数の関係を方程式で表して推定する。\n構造方程式モデリングは，分散共分散構造/相関構造の要素に潜在変数を含んだ方程式を当てはめた時の係数を推定するモデルである。方程式はパス図と呼ばれる表現方法で図示されることが多い。パス図では相関的関係を双方向の矢印で，回帰的関係を単方向の矢印で表現し，観測変数を矩形で，潜在変数を楕円で表す。パス図の表現を使うと，因子分析と主成分分析の違いは一目瞭然である。\n\n\n\n主成分分析(左)と因子分析(右)のパス図表現\n\n\nまた，探索的因子分析と確認的因子分析の違いも明白である。\n\n\n\nEFA(左)とCFA(右)のパス図表現。簡略化するために誤差因子は描画しない。\n\n\n確認的因子分析では，どの因子がどの項目に影響しているかを個別に指定している。言い換えるなら，影響がないと仮定するパスの係数を\\(0\\)に固定しているともいえる。この図では確認的因子分析モデルの因子間相関が\\(0\\)であると仮定しているため，パスを引いていない(引くことももちろん可能である)。\nこの方法ではモデルの方が先にあり，このモデルから考えられる分散共分散行列の式を実際の分さ共分散行列に当てはめることになる。当てはめる，すなわち係数を推定する方法は，大きく分けて最小二乗法，最尤法，ベイズ法であるが，実際は各種推定法のアルゴリズム名まで把握しておくといいだろう。さらに推定後，モデルと実際の分散共分散行列がどれほど一致しているか，即ち適合度Model Fit Indicesをみてその評価を行うことになる。適合度指標も複数あるため，それらを見ながら総合的に評価することになる。\nRでの具体例を見ておこう。パッケージlavvaanを用いて2以下のようにモデルを与える3。\n\npacman::p_load(lavaan)\n# モデル指定\nmodel &lt;- \"\nNeuroticism =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\nExtraversion =~ E1 + E2 + E3 + E4 + E5\nOpenness =~ O1 + O2 + O3 + O4 + O5\nConscientiousness =~ C1 + C2 + C3 + C4 + C5\n\"\n\nここでmodelオブジェクトは文字列として入力されていることに注意しよう。シングル，あるいはダブルクォーテーションでモデル記述を囲むのである。また，潜在変数名を左辺に置き，それを構成する観測変数を右辺に置く測定方程式は，=~という演算子で繋ぐ。変数同士の相関的関係は~~という演算子を，回帰的関係は~という演算子を用いる。特に潜在変数同士の関係を記述する方程式は構造方程式と呼ばれる。\nここでは因子間相関に関する記述はないが，デフォルトで\\(0\\)と指定しないところには相関のパスが仮定される。係数をゼロに指定したい場合は，Neuroticism ~~ 0 * Opennessのように記述するといいだろう。\nさて，モデルの指定が終われば，データと推定法を指定して推定させよう。 ここでは推定法(estimator)オプションを最尤法(ML)とした。また，要約を出力するときに，適合度指標(fit.meassures)と標準化係数(standardized)も表示するよう指定してある。\n\n# モデル推定\nmodel.fit &lt;- sem(model, estimator = \"ML\", data = dat)\nsummary(model.fit, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 55 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        60\n\n                                                  Used       Total\n  Number of observations                          2436        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                              4165.467\n  Degrees of freedom                               265\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             18222.116\n  Degrees of freedom                               300\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.782\n  Tucker-Lewis Index (TLI)                       0.754\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -99840.238\n  Loglikelihood unrestricted model (H1)     -97757.504\n                                                      \n  Akaike (AIC)                              199800.476\n  Bayesian (BIC)                            200148.363\n  Sample-size adjusted Bayesian (SABIC)     199957.729\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.078\n  90 Percent confidence interval - lower         0.076\n  90 Percent confidence interval - upper         0.080\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.037\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.075\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                       Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                            \n    N1                    1.000                               1.300    0.825\n    N2                    0.947    0.024   39.899    0.000    1.230    0.803\n    N3                    0.884    0.025   35.919    0.000    1.149    0.721\n    N4                    0.692    0.025   27.753    0.000    0.899    0.573\n    N5                    0.628    0.026   24.027    0.000    0.816    0.503\n  Agreeableness =~                                                          \n    A1                    1.000                               0.484    0.344\n    A2                   -1.579    0.108  -14.650    0.000   -0.764   -0.648\n    A3                   -2.030    0.134  -15.093    0.000   -0.983   -0.749\n    A4                   -1.564    0.115  -13.616    0.000   -0.757   -0.510\n    A5                   -1.804    0.121  -14.852    0.000   -0.873   -0.687\n  Extraversion =~                                                           \n    E1                    1.000                               0.920    0.564\n    E2                    1.226    0.051   23.899    0.000    1.128    0.699\n    E3                   -0.921    0.041  -22.431    0.000   -0.847   -0.627\n    E4                   -1.121    0.047  -23.977    0.000   -1.031   -0.703\n    E5                   -0.808    0.039  -20.648    0.000   -0.743   -0.553\n  Openness =~                                                               \n    O1                    1.000                               0.635    0.564\n    O2                   -1.020    0.068  -14.962    0.000   -0.648   -0.418\n    O3                    1.373    0.072   18.942    0.000    0.872    0.724\n    O4                    0.437    0.048    9.160    0.000    0.277    0.233\n    O5                   -0.960    0.060  -16.056    0.000   -0.610   -0.461\n  Conscientiousness =~                                                      \n    C1                    1.000                               0.680    0.551\n    C2                    1.148    0.057   20.152    0.000    0.781    0.592\n    C3                    1.036    0.054   19.172    0.000    0.705    0.546\n    C4                   -1.421    0.065  -21.924    0.000   -0.967   -0.702\n    C5                   -1.489    0.072  -20.694    0.000   -1.012   -0.620\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism ~~                                                        \n    Agreeableness     0.141    0.018    7.712    0.000    0.223    0.223\n    Extraversion      0.292    0.032    9.131    0.000    0.244    0.244\n    Openness         -0.093    0.022   -4.138    0.000   -0.112   -0.112\n    Conscientisnss   -0.250    0.025  -10.117    0.000   -0.283   -0.283\n  Agreeableness ~~                                                      \n    Extraversion      0.304    0.025   12.293    0.000    0.683    0.683\n    Openness         -0.093    0.011   -8.446    0.000   -0.303   -0.303\n    Conscientisnss   -0.110    0.012   -9.254    0.000   -0.334   -0.334\n  Extraversion ~~                                                       \n    Openness         -0.265    0.021  -12.347    0.000   -0.453   -0.453\n    Conscientisnss   -0.224    0.020  -11.121    0.000   -0.357   -0.357\n  Openness ~~                                                           \n    Conscientisnss    0.130    0.014    9.190    0.000    0.301    0.301\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.793    0.037   21.575    0.000    0.793    0.320\n   .N2                0.836    0.036   23.458    0.000    0.836    0.356\n   .N3                1.222    0.043   28.271    0.000    1.222    0.481\n   .N4                1.654    0.052   31.977    0.000    1.654    0.672\n   .N5                1.969    0.060   32.889    0.000    1.969    0.747\n   .A1                1.745    0.052   33.725    0.000    1.745    0.882\n   .A2                0.807    0.028   28.396    0.000    0.807    0.580\n   .A3                0.754    0.032   23.339    0.000    0.754    0.438\n   .A4                1.632    0.051   31.796    0.000    1.632    0.740\n   .A5                0.852    0.032   26.800    0.000    0.852    0.528\n   .E1                1.814    0.058   31.047    0.000    1.814    0.682\n   .E2                1.332    0.049   26.928    0.000    1.332    0.512\n   .E3                1.108    0.038   29.522    0.000    1.108    0.607\n   .E4                1.088    0.041   26.732    0.000    1.088    0.506\n   .E5                1.251    0.040   31.258    0.000    1.251    0.694\n   .O1                0.865    0.032   27.216    0.000    0.865    0.682\n   .O2                1.990    0.063   31.618    0.000    1.990    0.826\n   .O3                0.691    0.039   17.717    0.000    0.691    0.476\n   .O4                1.346    0.040   34.036    0.000    1.346    0.946\n   .O5                1.380    0.045   30.662    0.000    1.380    0.788\n   .C1                1.063    0.035   30.073    0.000    1.063    0.697\n   .C2                1.130    0.039   28.890    0.000    1.130    0.650\n   .C3                1.170    0.039   30.194    0.000    1.170    0.702\n   .C4                0.960    0.040   24.016    0.000    0.960    0.507\n   .C5                1.640    0.059   27.907    0.000    1.640    0.615\n    Neuroticism       1.689    0.073   23.034    0.000    1.000    1.000\n    Agreeableness     0.234    0.030    7.839    0.000    1.000    1.000\n    Extraversion      0.846    0.062   13.693    0.000    1.000    1.000\n    Openness          0.404    0.033   12.156    0.000    1.000    1.000\n    Conscientisnss    0.463    0.036   12.810    0.000    1.000    1.000\n\n\n出力として，まずモデルの要約(推定法など)が示され，続いて適合度指標(CFI, TFI, AIC, BIC, RMSEA, SRMR)などが表示される。これらの適合度指標は大きく3つのカテゴリーに分類できる：\nまずは，飽和モデルとヌルモデルの間の比較指標である。CFIやTFIがこれに該当する。ヌルモデルを0，飽和モデルを1としたとき，今回のモデルがどこに位置づくかを示す。ここで飽和モデルとはデータに完全に適合するモデルであり，すべての観測変数間にすべての可能なパスが引かれたものを指す。逆にヌルモデル(独立モデル)は観測変数間に関連性がまったくないと仮定する最も制約の強いモデルである。\n次に，尤度に基づく相対指標である。AIC，BIC，SABICがこれに該当する。尤度はデータが確率モデルにどれほど近いかを表したものであるから，データやモデルが異なれば比較はできない。当該データに対する相対比較として用いる。AIC(赤池情報量規準，Akaike Information Criterion)は対数尤度とパラメータの数で計算されており，対数尤度が大きくパラメータ数が少ない方が良いモデルだと考える規準である。-2LL + 2p(LLは対数尤度，pはパラメータ数)で算出され，小さければ小さいほど当てはまりが良いと判断する。BIC(ベイジアン情報量規準)は，AICよりもサンプルサイズに対して強いペナルティを与えている。SABICはサンプルサイズを調整したBICの亜形である。\n最後に，実データの分散説明量の残量に関する指標がある。RMSEAとSRMRがこれに該当する。RMSEA(Root Mean Square Error of Approximation)はモデルの近似誤差を評価しており，0.05未満が良好とされる。SRMR(Standardized Root Mean Square Residual)は実データと予測データの残差の標準化された平方根で， 0.08未満が良好とされる。\nモデル評価の際は、これらの指標を総合的に検討することが推奨される。単一の指標だけではなく、複数の指標を参照し、それらが一貫して良好な適合を示すかを確認することが重要である。\n続く出力で，推定値Estimator や検定統計量が表示される。心理学の場合は，全ての変数を標準化した推定値Std.allを参照することが多い4。ここでパス係数が小さかったり，統計的に有意にならないものを削除することでモデルの適合度は上げることができる。 ただし，適合度を上げることが研究の目的になってはならない。仮定を当てはめるのだから，適当とされる目安を達成できない場合は仮定を省みて改良することはあるだろうが，適合度を上げるために仮定に合わないパスを引くようなことは，「頑張って有意にする」のと同じQRPsである。\nところで，パッケージを使えば，パス図も自動で描いてくれる。他にも，lavaanExtra，tidySEM，lavaanPlot,lavaanPlot2など開発中のものも含めて様々なものがあるが，ここでは古典的なsemPlotパッケージによる出力例を示す。\n\npacman::p_load(semPlot)\nsemPaths(model.fit, what = \"stand\", style = \"lisrel\")\n\n\n\n\n\n\n\n\n以上が因子分析法の概略である。\n因子分析法は測定に関するモデルであり，心理尺度を作成する場合は非常によく用いられるものである。しかしあくまでもデータや項目間の相関関係から共通次元を見出すものであるから，構成概念を直接測定したとか，構成概念の存在が証明されたかのような利用・解釈は適切ではない。例えば，何らかの話題についての文言，極端な話「ラーメンに関する記述」を用意して，そこに量的な評価を加えれば，ラーメン因子だろうが豚骨因子だろうが，何らかの解釈ができる因子を抽出することはできる。そのことと，人が心理的に豚骨因子を内在化しているということにはならない。\nまた構造方程式モデリングによって潜在変数間に回帰や相関のパスを仮定することはできるが，そのことが実際どのような形で顕現化するかについて考えておく必要がある。モデルが非常に適合していて，潜在変数間に強い影響関係があったとしても，測定方程式のパス係数が低かったりすると，結局一方の因子得点が1単位増えたことで，従属する潜在変数がどのように変化し，それがどのように行動・測定値に反映されるかを意識しよう。そのような実態的な影響がない，つまり妥当性のない統計モデルは机上の空論に過ぎないからである。\n測定とその実際的影響については，因子分析モデルの一種とも言える項目反応理論と問題意識を軌を一にする。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#項目反応理論",
    "href": "chapter14.html#項目反応理論",
    "title": "14  多変量解析(その1)",
    "section": "14.3 項目反応理論",
    "text": "14.3 項目反応理論\nつづいて項目反応理論(Item Response Theory, IRT)を取り上げる。項目反応理論は古典的テスト理論(Classical Test Theory, CTT)との対比で，現代テスト理論と呼ばれることもある。テスト理論を背景に持つものであるから，従属変数としてバイナリ変数を前提としている(0が誤答，1が正答を表す)。これは言い換えれば因子分析において従属変数がバイナリであるものといってもよく，実際にカテゴリカル因子分析との数学的等価性が明らかになっている。\nもう一つ因子分析と異なる側面としては，因子分析が性格心理学を背景に因子構造を探索することに重点が置かれていることに対して，項目反応理論は因子得点をより精緻にすることに重点が置かれている。また性格心理学の場合は何因子構造であるかということがすでに学術的な問いであるが，項目反応理論を一とするテスト理論においては「学力」の一因子構造であることが望ましいとされる。このことから，因子分析を使った心理尺度の構成は「単純構造」が良いものであると考え項目を洗練(取捨選択)することが多いのに対し，項目反応理論は第一因子の負荷量が十分大きければ(一般に30%程度の分散説明率があれば良い)一因子構造であると考えるし，いかなる項目であっても何らかの情報をもたらすものと考えて項目をプールする(捨てない)という方針で進められることが多い点である。\n項目反応理論の各種モデルを用いた，コンピュータ適応型テスト(Computer Adaptive Test, CAT)が現在のテスト理論の主流である。これは受験者の回答パターンに応じて項目プールから動的に次々問題を提供し，効率よく受験者の能力を推定していくものである。CATに必要なのはIRTを背景にしたモデルはもちろんのこと，各能力水準を測定するのに適した項目プールであり，また項目プールというデータベースとの連携システムである5。\n\n14.3.1 ロジスティックモデル\nIRTはバイナリデータに対する単因子モデルである。バイナリデータであるから，連続値を前提とするピアソンの積率相関係数ではなく，テトラコリック相関係数を用いてその次元性を解析する。また因子に該当する被験者母数\\(\\theta\\)によって項目反応が回帰されるモデルでもあるから，ロジスティック回帰分析のようなモデル化をすることになる。\n被験者母数は標準正規分布が仮定されるが，これを累積正規分布の形で表現するとロジスティックカーブがよくあてまるし，項目の特徴を表現するための項目母数を線型モデルの中に組み込むためには，ロジスティックモデルで表現する方がわかりやすいという側面もある6。以下に標準正規分布と累積正規分布，並びにロジスティック関数による正規累積分布の近似を示す。なお，ロジスティックモデルで扱われる関数は，以下のように係数(1.702)を用いると，よりよく近似することが知られている。\n\\[ f(x) = \\frac{1}{1 + exp(-1.702x)} \\]\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# データの準備\nx &lt;- seq(-4, 4, length.out = 1000)\nnormal_df &lt;- data.frame(\n  x = x,\n  density = dnorm(x),\n  cdf = pnorm(x),\n  logistic = 1 / (1 + exp(-1.702 * x))\n)\n\n# 1. 標準正規分布\np1 &lt;- ggplot(normal_df, aes(x = x, y = density)) +\n  geom_line() +\n  labs(title = \"標準正規分布\",\n       x = \"x\",\n       y = \"確率密度\") +\n  theme_minimal()\n\n# 2. 累積正規分布\np2 &lt;- ggplot(normal_df, aes(x = x, y = cdf)) +\n  geom_line() +\n  labs(title = \"累積正規分布\",\n       x = \"x\",\n       y = \"累積確率\") +\n  theme_minimal()\n\n# 3. ロジスティック曲線\np3 &lt;- ggplot(normal_df, aes(x = x, y = logistic)) +\n  geom_line() +\n  labs(title = \"ロジスティック曲線による近似\",\n       x = \"x\",\n       y = \"確率\") +\n  theme_minimal()\n\n# 3つのプロットを横に並べて表示\np1 + p2 + p3\n\n\n\n\n\n\n\n\nこのロジスティック関数を用いて，項目母数を使って項目の特徴を描画することを考えよう。IRTのロジスティックモデルには，パラメータが1つのもの，2つのもの…とさまざまなものが考えられているが，パラメータ数が多いモデルはパラメータ数が少ないモデルに含まれる(特殊形)である。\n\n14.3.1.1 1PLモデル\nまずは1パラメータロジスティックモデル(1PLモデル)を考えよう。このモデルは，項目母数\\(b\\)を用いて，以下のように表現される。\n\\[ P(Y_{ij} = 1 | \\theta_i, b_j) = \\frac{1}{1 + exp(-1.702(\\theta_i - b_j))} \\]\nここで，\\(Y_{ij}\\)は被験者\\(i\\)が項目\\(j\\)に正答したかどうかを表すバイナリ変数であり，\\(\\theta_i\\)は被験者\\(i\\)の能力を表す被験者母数である。また，\\(b_j\\)は項目\\(j\\)の困難度(difficulty)を表す項目母数である。というのも，この\\(b_j\\)が大きくなるとロジスティック曲線は右に寄る，また小さくなると左に寄るからである。横軸は被験者母数\\(\\theta\\)であり，縦軸は通過率であるから，曲線が右にシフトすることはより能力が高くなければ通過率が上昇しないことを表すからである。\n\nlogistic_1pl &lt;- function(theta, b) {\n  1 / (1 + exp(-1.702 * (theta - b)))\n}\n\nx &lt;- seq(-4, 4, length.out = 1000)\nnormal_df &lt;- data.frame(\n  x = x,\n  default = logistic_1pl(x, 0), #deafult\n  easy = logistic_1pl(x, -1),\n  hard = logistic_1pl(x, 1)\n)\n\nggplot(normal_df) +\n  geom_line(aes(x = x, y = default, color = \"デフォルト(b=0)\")) +\n  geom_line(aes(x = x, y = easy, color = \"易しい(b=-1)\")) +\n  geom_line(aes(x = x, y = hard, color = \"難しい(b=1)\")) +\n  scale_color_brewer(palette = \"Set2\") + \n  labs(title = \"1pl logistic model\",\n       x = \"theta\",\n       y = \"通過率\",\n       color = \"難易度\") +  # 凡例のタイトルを追加\n  theme_minimal() +\n  theme(legend.position = \"bottom\")  # 凡例を下部に配置\n\n\n\n\n\n\n\n\n\n\n14.3.1.2 2PLモデル\n2パラメータロジスティックモデル(2PLモデル)は，1PLモデルに加えて項目母数\\(a\\)を含める。この母数は識別力と呼ばれる。\n\\[ P(Y_{ij}=1|\\theta_i,a_j,b_j) = \\frac{1}{1+exp(-1.702a_j(\\theta_i-b_j))} \\]\nこれが識別力と呼ばれるのは，ロジスティック曲線の傾きを変えるからである。傾きが強くなって急激に上昇することは，ある\\(\\theta\\)の値で急に正誤の確率が変わることを意味し，逆に傾きが緩くなることは特定の\\(\\theta\\)の値でも正誤の違いが大きくないことを意味するからである。ちなみにカテゴリカル因子分析の文脈では，困難度\\(b_j\\)が閾値に，識別力\\(a_j\\)が因子負荷量に相当する。\n\nlogistic_2pl &lt;- function(theta, a, b) {\n  1 / (1 + exp(-1.702 * a * (theta - b)))\n}\n\nx &lt;- seq(-4, 4, length.out = 1000)\nnormal_df &lt;- data.frame(\n  x = x,\n  default = logistic_2pl(x, 1, 0), #deafult\n  easy = logistic_2pl(x,1.5, -1),\n  hard = logistic_2pl(x,0.5, 1)\n)\n\nggplot(normal_df) +\n  geom_line(aes(x = x, y = default, color = \"デフォルト(b=0,a=1)\")) +\n  geom_line(aes(x = x, y = easy, color = \"b=-1, a=1.5\")) +\n  geom_line(aes(x = x, y = hard, color = \"b=1, a=0.5\")) +\n  scale_color_brewer(palette = \"Set2\") + \n  labs(title = \"1pl logistic model\",\n       x = \"theta\",\n       y = \"通過率\",\n       color = \"モデルと設定\") +  # 凡例のタイトルを追加\n  theme_minimal() +\n  theme(legend.position = \"bottom\")  # 凡例を下部に配置\n\n\n\n\n\n\n\n\n\n\n14.3.1.3 3,4,5PLモデル\n実践的には2PLモデルが最もよく用いられるが，理論的には3，4，5PLモデルまで提案されており， それぞれ次のように表現される。\n\\[ P(Y_{ij}=1|\\theta_i,a_j,b_j,c_j) = c_j + \\frac{1-c_j}{1+exp(-1.702a_j(\\theta_i-b_j))} \\]\n\\[ P(Y_{ij}=1|\\theta_i,a_j,b_j,c_j,d_j) = c_j + \\frac{d_j-c_j}{1+exp(-1.702a_j(\\theta_i-b_j))} \\]\n\\[ P(Y_{ij}=1|\\theta_i,a_j,b_j,c_j,d_j,e_j) = c_j + \\frac{d_j-c_j}{\\{1+exp(-1.702a_j(\\theta_i-b_j))\\}^{e_j}} \\]\nここで\\(c_j\\)は下方漸近線母数，\\(d_j\\)は上方漸近線母数，\\(e_j\\)は非対称性母数と呼ばれている。このように，モデルとしては徐々にパラメータ数を増やして表現しているが，推定すべきパラメータの数が増えるとより大きい標本サイズ(受験者数)が必要となるし，等価など運用シーンでも複雑になることから，あまり用いられるものではない。\n\n\n14.3.1.4 ロジスティックモデルと分析の実際\n項目の特徴を表現するロジスティック曲線は，項目反応関数(Item Responose Function)あるいは項目特性曲線(Item Characteristic Curve)と呼ばれる。ltmパッケージやexametrika パッケージなど，IRTモデルを実行するためのRパッケージは複数あり，これを使って実践例を見てみよう。\nここでは著者が開発したexametrikaパッケージとそのサンプルデータを用いて，実際にIRTを実行してみよう。exametrikaパッケージにはサンプルデータが複数含まれている。今回用いるJ15S500は，500人の被験者が15問の項目に回答したサンプルデータである。\n\npacman::p_load(exametrika)\nresult.2pl &lt;- IRT(J15S500, model = 2, verbose = FALSE)\nprint(result.2pl)\n\nItem Parameters\n       slope location PSD(slope) PSD(location)\nItem01 0.698   -1.683     0.1093         0.266\nItem02 0.810   -1.552     0.1166         0.221\nItem03 0.559   -1.838     0.0988         0.338\nItem04 1.416   -1.178     0.1569         0.113\nItem05 0.681   -2.242     0.1152         0.360\nItem06 0.997   -2.162     0.1499         0.273\nItem07 1.084   -1.039     0.1281         0.130\nItem08 0.694   -0.558     0.1002         0.153\nItem09 0.347    1.630     0.0766         0.427\nItem10 0.492   -1.421     0.0907         0.306\nItem11 1.122    1.020     0.1314         0.124\nItem12 1.216    1.031     0.1385         0.117\nItem13 0.875   -0.720     0.1111         0.133\nItem14 1.200   -1.232     0.1407         0.134\nItem15 0.823   -1.203     0.1127         0.180\n\nItem Fit Indices\n       model_log_like bench_log_like null_log_like model_Chi_sq null_Chi_sq\nItem01       -263.524       -240.190      -283.343       46.669      86.307\nItem02       -252.914       -235.436      -278.949       34.954      87.025\nItem03       -281.083       -260.906      -293.598       40.353      65.383\nItem04       -205.851       -192.072      -265.962       27.558     147.780\nItem05       -232.072       -206.537      -247.403       51.070      81.732\nItem06       -173.930       -153.940      -198.817       39.981      89.755\nItem07       -252.039       -228.379      -298.345       47.320     139.933\nItem08       -313.754       -293.225      -338.789       41.057      91.127\nItem09       -325.692       -300.492      -327.842       50.399      54.700\nItem10       -309.448       -288.198      -319.850       42.500      63.303\nItem11       -250.836       -224.085      -299.265       53.501     150.360\nItem12       -240.247       -214.797      -293.598       50.900     157.603\nItem13       -291.816       -262.031      -328.396       59.571     132.730\nItem14       -224.330       -204.953      -273.212       38.754     136.519\nItem15       -273.120       -254.764      -302.847       36.713      96.166\n       model_df null_df   NFI   RFI   IFI   TLI   CFI RMSEA    AIC    CAIC\nItem01       12      13 0.459 0.414 0.533 0.488 0.527 0.076 22.669 -27.930\nItem02       12      13 0.598 0.565 0.694 0.664 0.690 0.062 10.954 -39.645\nItem03       12      13 0.383 0.331 0.469 0.414 0.459 0.069 16.353 -34.246\nItem04       12      13 0.814 0.798 0.885 0.875 0.885 0.051  3.558 -47.041\nItem05       12      13 0.375 0.323 0.440 0.384 0.432 0.081 27.070 -23.529\nItem06       12      13 0.555 0.517 0.640 0.605 0.635 0.068 15.981 -34.619\nItem07       12      13 0.662 0.634 0.724 0.699 0.722 0.077 23.320 -27.279\nItem08       12      13 0.549 0.512 0.633 0.597 0.628 0.070 17.057 -33.542\nItem09       12      13 0.079 0.002 0.101 0.002 0.079 0.080 26.399 -24.201\nItem10       12      13 0.329 0.273 0.405 0.343 0.394 0.071 18.500 -32.099\nItem11       12      13 0.644 0.615 0.700 0.673 0.698 0.083 29.501 -21.099\nItem12       12      13 0.677 0.650 0.733 0.709 0.731 0.081 26.900 -23.699\nItem13       12      13 0.551 0.514 0.606 0.570 0.603 0.089 35.571 -15.028\nItem14       12      13 0.716 0.692 0.785 0.765 0.783 0.067 14.754 -35.846\nItem15       12      13 0.618 0.586 0.706 0.678 0.703 0.064 12.713 -37.886\n           BIC\nItem01 -27.906\nItem02 -39.621\nItem03 -34.222\nItem04 -47.017\nItem05 -23.505\nItem06 -34.595\nItem07 -27.255\nItem08 -33.518\nItem09 -24.177\nItem10 -32.076\nItem11 -21.075\nItem12 -23.675\nItem13 -15.004\nItem14 -35.822\nItem15 -37.862\n\nModel Fit Indices\n                   value\nmodel_log_like -3890.655\nbench_log_like -3560.005\nnull_log_like  -4350.217\nmodel_Chi_sq     661.300\nnull_Chi_sq     1580.424\nmodel_df         180.000\nnull_df          195.000\nNFI                0.582\nRFI                0.547\nIFI                0.656\nTLI                0.624\nCFI                0.653\nRMSEA              0.073\nAIC              301.300\nCAIC            -457.689\nBIC             -457.330\n\n\n数値的な特徴としては，Item Parametersのところにslope(識別力)，location(困難度)が示されており，またそれぞれの標準誤差が示されている。 続くItem Fit Indicesは項目ごとの適合度，Model Fit Indicesはテスト全体のモデル適合度であるが，IRTモデルはSEMの適合度の観点から言えば非常に当てはまりは悪い。これはバイナリデータに対するモデリングであることなどを考えると，ある程度は仕方がないことであるとも言える。\nIRTの良さは，こうした数値的特徴というよりも，項目分析の時におけるIRTの可視化のしやすさにあると言えるだろう。exametrikaパッケージでは，plot関数を用いて項目特性曲線を描画することができる。\n\nplot(result.2pl, item = 1:5, type = \"IRF\", overlay = TRUE)\n\n\n\n\n\n\n\n\nまた，IRF関数をテストの全項目に対して加算したテスト反応関数(Test Response Function)を描画することもできる。\n\nplot(result.2pl, type = \"TRF\")\n\n\n\n\n\n\n\n\nさらに項目反応関数を変換した項目情報関数(Item Information Function)を描画することもできる。項目情報関数は，その項目において最も分散が大きくなるところ，すなわち\\(\\theta =0.5\\)をピークにする関数であり，以下のように定義される。 \\[ I_j(\\theta) = \\frac{P_j^{\\prime}(\\theta)^2}{P_j(\\theta)(1-P_j(\\theta))} \\]\n要するに，正答と誤答の確率の差が大きいほど，その項目の情報量が大きくなるということである。 この関数をexametrikaパッケージでプロットするには次のようにtype = \"IIF\"と指定する。\n\nplot(result.2pl, item = 4, type = \"IRF\")\n\n\n\n\n\n\n\nplot(result.2pl, item = 4, type = \"IIF\")\n\n\n\n\n\n\n\n\nIIFが示すのは，項目反応理論における信頼性の概念であるとも言える。すなわち，IRTにおいては信頼性が\\(\\theta\\)の関数として表現され，どの領域匂いってその項目が最も効率的に機能するかを評価するのである。\n確認しておくと，古典的テスト理論においては，テストの全体に対する真分散の割合で信頼性をとらえていたのであった。また因子分析においては項目における共通性\\(h_j^2\\)で信頼性をとらえていた。つまりテスト全体から各項目へと進んで行ったのだが，現代テスト理論においては関数・項目の機能性を評価するようにと発展してきたのである。\nすでに述べたように，IRTの観点からは，難易度が高すぎる・低すぎる項目であっても，削除するようなことはない。そうした項目は，高い能力・低い能力を査定する時に必要なのである。実践に際してそうした項目は大きな分散を持ち得ないから，共通性も低くなりがちであるが，だからと言ってそうした項目を削除するようなことはしない。この辺りに，テスト理論と因子分析との思想的な違いがあると言える。\nテスト全体の情報関数は，テストに含まれる項目情報関数の総和で表現される。この関数をexametrikaパッケージでプロットするには次のようにtype = \"TIF\"と指定する。\n\nplot(result.2pl, type = \"TIF\")\n\n\n\n\n\n\n\n\nこれを見ると，この15項目からなるテストは全体として\\(\\theta=-1\\)のあたりをピークとしており，相対的にやや\\(\\theta\\)が低い受験者に対して精緻な情報を提供するようになっていることがわかる。事前に項目母数がわかっている多くの項目プールがあり，それらを組み合わせてテストを作成する場合には，このような情報関数を用いて事前にテストの精度をデザインして適用することができる。\n被験者母数\\(\\theta\\)の推定については，受験者の回答パターンから推定される。一般に標準正規分布を事前分布としたベイズ推定が用いられる7。exametrikaパッケージでは，分析と同時に被験者母数の推定も行われている。\n\nhead(result.2pl$ability)\n\n          ID         EAP       PSD\n1 Student001 -0.66456787 0.5457047\n2 Student002 -0.14853706 0.5626979\n3 Student003  0.01362523 0.5699764\n4 Student004  0.58775676 0.6012839\n5 Student005 -0.97796870 0.5415527\n6 Student006  0.85892497 0.6187224\n\n\n\n\n\n14.3.2 IRTモデルの展開\nIRTモデルは基本的にバイナリデータに対するモデルであるが，多段階反応，多値反応に対するモデルも提案されている。たとえばリッカート尺度のような多段階反応に対しては，段階反応モデル(Graded Response Model)や部分採点モデル(Partial Credit Model)が提案されている。これらを用いることの利点は，心理尺度データに対して順序尺度水準を仮定できるところにある。\n心理学では基本的に，段階評定はせいぜい順序尺度水準の精度しか持ち得ないと考えられていながら，その数学的な利便性から(あるいはそこまでの精度がないとそもそも信用されていなかったから)，間隔尺度水準とみなしてピアソンの積率相関係数を算出し，一般的な因子分析を行ってきた。こうした「みなし」が必要だった理由のひとつが，統計パッケージにGRMやPCMが実装されていなかったからである。昨今では，GRMと2PLモデルとの数学的等価性から同じ潜在変数モデルとして推定する統計ソフトウェア(Mplusなど)もあるし，RにはltmパッケージなどGRM，PCMを提供するものもある。つまり，ツールがないからという言い訳はもう通用しない時代である。\n多段階モデルで分析できるさらなる利点は，適切な反応段階を考えられる点である。リッカート法といえば5件法，7件法であるというのが一般的に考えられているが，このことに特段の理論的根拠はない。それよりも，回答者が5，7段階のカテゴリ反応をしっかりと弁別できるのかどうかを考えるべきである8。\n具体例で見てみよう。ltmパッケージのgrm関数を用いて，サンプルデータScienceを分析してみる。このデータは科学態度に対するデータであり，4段階評定になっている。\n\npacman::p_load(ltm)\ndata(Science)\nresult.grm &lt;- grm(Science)\nprint(result.grm)\n\n\nCall:\ngrm(data = Science)\n\nCoefficients:\n             Extrmt1  Extrmt2  Extrmt3  Dscrmn\nComfort      -10.768   -5.645    3.097   0.411\nEnvironment   -2.154   -0.790    0.627   1.570\nWork          32.102    9.261  -24.402  -0.074\nFuture       -30.602  -11.806   10.455   0.108\nTechnology    -2.462   -0.885    0.642   1.650\nIndustry      -2.870   -1.529    0.286   1.642\nBenefit      -21.232   -5.982   10.297   0.136\n\nLog.Lik: -2998.129\n\n\n結果で示されているのは3つの閾値(Extrmt)と，それぞれの閾値に対する識別力(Dcrmn)である。 段階反応モデルも，ロジスティックモデル同様IRF，IIF，TIFを描画できるから，IRF(ltmパッケージではICCという引数を用いる)を描いてみよう。\n\nplot(result.grm, items = 2, type = \"ICC\")\n\n\n\n\n\n\n\n\nこの図には各カテゴリに対する反応確率が，\\(\\theta\\)の関数として表示されている。これを見ると，科学的態度の\\(\\theta\\)が高くなるにつれて，回答する確率が最も高いカテゴリが1から2，2から3へと変わっていくことが見て取れる。\nしかし次の項目はどうだろうか。\n\nplot(result.grm, items = 1, type = \"ICC\")\n\n\n\n\n\n\n\n\nこれを見ると，反応カテゴリ1，2のピークが存在せず，ほとんど3の反応で被覆されており，\\(\\theta=3\\)を超えたところでやっと反応カテゴリ4がでてくることになる。プロットは\\(-4 \\le \\theta \\le +4\\)の範囲であるから，この幅を負の方向に広げればピークが出てくるのかもしれないが，実際的ではない。データによってはカテゴリ反応のピークが出てこないものもあり，そうしたものは適切な反応段階の設計になっていないことが疑われる。\n昨今では心理尺度の設計に対しても，IRTのアプローチを取ることが推奨されている。因子分析アプローチとのもう一つの違いである，単因子を仮定する点についても拡張され，多次元IRTモデルも提案されている(mirtパッケージなどが提供されている)。もはや，IRTのアプローチを取らない理由がないのである。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#まとめ",
    "href": "chapter14.html#まとめ",
    "title": "14  多変量解析(その1)",
    "section": "14.4 まとめ",
    "text": "14.4 まとめ\nここでは探索的因子分析，検証的因子分析(構造方程式モデリング)，項目反応理論のそれぞれについて，その基本的な考え方と実践的な使い方を紹介した。\nこれらに共通する考え方は，分散共分散行列あるいは相関行列をもとに，潜在変数を仮定した測定モデルを構築するという点である。相関行列が順序尺度水準に対するテトラコリック/ポリコリック/ポリシリアル相関係数であれば，カテゴリカルな因子分析をしていることになるし，それは(多段階)項目反応理論をしていることでもある。つまり尺度水準に対応した相関係数が算出できるアプリケーションであれば，モデルの適用は同じ手順で行うことができる(lavaanでも変数の尺度水準の設定ができる。そのほかのアプリケーションとしてはMplusが有名である)。\nそれぞれのモデルの持つ歴史的背景や理論的系譜を知ることは，モデルの適用に有益な知見をもたらすが，ユーザ視点でいえば使えるものはなんでも使うべきであり，とくに調査協力者(=回答者，受験生)の回答のしやすさといった観点から調査研究を設計することが肝要であろう。数学的限界やソフトウェアの都合によって，ましてや研究者の無理解や怠慢によって，回答しにくい調査デザインを適用することは，調査研究の質を低下させることになることを忘れてはならない。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#課題",
    "href": "chapter14.html#課題",
    "title": "14  多変量解析(その1)",
    "section": "14.5 課題",
    "text": "14.5 課題\nこのチャプターで学んだ多変量解析の手法について、以下の課題に取り組んでください。各課題は、実践的なデータ分析を通じて、理論と実践の両面から理解を深めることを目的としています。 例として，psychパッケージのsmall.msqデータセット(気分状態質問紙)を使用します。このデータセットは14の変数，200ケースです。9\n14の変数は，エネルギー的覚醒状態変数(活動的active, 注意深さalert，覚醒arousal，ねむさsleepy，疲れtired，ウトウトdrowsy)，緊張的覚醒変数(不安anxious，落ち着かないjittery，神経質なnervous，穏やかなcalm，リラックスしたrelaxed，気楽なat.ease)と，性別gender，薬物条件drug からなります。性別と薬物条件を除いた2因子構造であると考えられ，\n\n14.5.1 課題1：探索的因子分析の実践\n探索的因子分析を実施してください。\n分析の手順：\n\n因子数の決定\n因子分析の実行(斜交回転)\n結果の解釈（因子負荷量、共通性、独自性）\n\n\n\n14.5.2 課題2：確認的因子分析の実践\nlavaanパッケージを使って，2因子モデルを推定してください。 モデル指定の際に，ordered = TRUE オプションを入れることで，観測変数を順序尺度水準として推定します。\n分析の手順：\n\n理論的モデルの構築(パス図の作成)\nlavaanパッケージを用いたモデルの推定\nモデルのプロット\n適合度指標の評価\nモデルの修正(必要な場合)\n\n\n\n14.5.3 課題3：多段階項目反応理論の実践\nltmパッケージの段階反応モデル(GRM)を適用してください。一次元性を仮定したモデルなので，エネルギー的覚醒項目セット，緊張的覚醒項目セットに分け，それぞれにGRMモデルを適用します。\n分析の手順：\n\nデータの分割\nGRMの適用\n項目特性曲線の描画\n項目情報関数の描画\n\n\n\n14.5.4 課題4: 多次元多段階項目反応理論の実践\nmirtパッケージ(Multidimensional IRT)を用いて，多次元モデルを実行します。コードは次のようになります。\n\npacman::p_load(mirt)\n# 2因子(model = 2)，段階反応(itemtype = 'graded')を指定\nresult.mirt &lt;- mirt(dat, model = 2, itemtype = \"graded\")\n# 出力に際して斜交回転\nsummary(result.mirt, rotate = \"geominQ\")\n\n# 多次元ICCの描画\nplot(result.mirt, type = 'trace', which.items = 1)\n# 多次元IICの描画\nplot(result.mirt, type = 'info')\n\n\n\n\n\nBernaards, Coen A., and Robert I. Jennrich. 2005. “Gradient Projection Algorithms and Software for Arbitrary Rotation Criteria in Factor Analysis.” Educational and Psychological Measurement 65: 676–96. https://doi.org/10.1177/0013164404272507.\n\n\nアデラ＝マリアイスヴォラヌ, サシャエプスカンプ, ローレンスウォルドープ, and デニーボースブーム. (2022) 2024. 心理ネットワークアプローチ入門:行動科学者と社会科学者のためのガイド. Translated by 樫原潤 and 小杉考司. 勁草書房.\n\n\n宮川雅巳. 1997. グラフィカルモデリング (統計ライブラリー). 朝倉書店.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房. http://ci.nii.ac.jp/ncid/BB27527420.\n\n\n岡太彬訓, and 今泉忠. 1994. パソコン多次元尺度構成法. 共立出版.\n\n\n新納浩幸. 2007. Rで学ぶクラスタ解析. オーム社.\n\n\n西里静彦. 2010. 行動科学のためのデータ解析–情報把握に適した方法の利用. 培風館.\n\n\n足立浩平. 2006. 多変量データ解析法: 心理・教育・社会系のための入門. ナカニシヤ出版.\n\n\n高根芳雄. 1980. 多次元尺度法. 東京大学出版会.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#footnotes",
    "href": "chapter14.html#footnotes",
    "title": "14  多変量解析(その1)",
    "section": "",
    "text": "これは因子の複雑さを表す指標で、各項目(変数)がどれだけ単純に(あるいは複雑に)因子に負荷しているかを表す指標である。値が1に近い場合，その項目は基本的に1つの因子にのみ強く負荷することあらわしている。値が大きくなるほど、その項目が複数の因子に分散して負荷していることになる。この値は，項目\\(j\\)の因子\\(k\\)に対する負荷量を\\(a_{jk}\\)としたとき，\\(\\frac{(\\sum_k a_{jk}^2)^2}{\\sum_k a_{jk}^4}\\)で算出する。↩︎\nらばーん，とは変な名前だと思われるかもしれないが，LAtent VAriable ANalysisすなわち潜在変数分析の意味である。↩︎\nlavaanguiというパッケージを用いれば，モデルの指摘もGUIでできる。↩︎\nStd.allは観測変数も潜在変数もその分散を1に標準化したもの。Std.lvは潜在変数だけその分散を1に標準化したものである。↩︎\n実際，日本最大級の学力テストである大学入学共通試験においてもCATの導入が検討されたが，膨大な項目プールの必要性(予備調査)や，地方や離島などの遠隔地などでも都市部と同じ通信環境，実行環境の準備などを考えると現実的でないということから見送られている現状がある。現行の紙とペン(マークシート)を用いた受験システムは，毎年50万人程度が同時に受けても一桁パーセント以下の誤謬率しかないという驚異の精度で運用されている極めて優れた実践システムであり，その社会的インパクトの大きさから考えても，CATの導入は慎重にならざるを得ない。↩︎\n正規累積関数を使ってモデル化・表現することももちろん可能である。↩︎\n最尤推定にすると，全問正答あるいは全問誤答の場合には，その項目の母数が無限大になってしまうから，実用的でないからである。↩︎\nよく受ける質問に，「先行研究が5段階であったら，5段階でとらなければならないか」とか，「5件法の尺度と7件法の尺度を混ぜて使ってもいいか」というものがある。これらに対する正しい回答は，先行研究のスコアリング・標準化手続きを利用するのであれば先行研究に従わなければならないが，そうでないならば(探索的因子分析など，自らのデータで因子負荷量を決め，スコアリングを行うのであれば)，回答者の反応しやすい段階にデザインするべきであり，回答者の反応しやすさが不明なのであれば多段階項目反応理論を用いて各カテゴリに対する反応も検証するべきである，となる。↩︎\nこれは元々2500以上のサンプルがあるMotivational State Questionnaire (MSQ)の一部。全体はpsychToolsパッケージに入っている。↩︎",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter15.html",
    "href": "chapter15.html",
    "title": "15  多変量解析(その2)",
    "section": "",
    "text": "15.1 距離行列を用いるもの\n先の章では分散共分散行列(相関行列)に基づいた線型モデルを中心に紹介した。 しかし，多変量解析はそれだけではない。 むしろ測定モデルとして潜在変数を仮定する手法だけに固執するあまり，心理測定の仮定に違反するようなデータであっても因子分析を適用したり，モデルの適合度を優先しすぎて不自然な設定に走ったりするような誤用が多く見られる。\n心理学においては因子分析が構成概念を測定していると「純粋に」信じられて多用されてきたが，かつてはさまざまな多変量解析技法が必要に応じて開発，使用されていたのである。 ここでは分析のスタートになる行列の種類で区分し，いくつかの多変量解析モデルを紹介する。\n距離とは，次の四つの公理を満たす数字のことをいう。\n距離行列とは行列の要素が距離を表しているもので，一般に正方・対称行列になる。この点は分散共分散行列や相関行列と同じで，この行列演算によって分散共分散を用いたモデルとは別の解釈が成立する分析を作ることができる。\nちなみにRでは距離行列を作るのにdist関数を用いる。オプションとして特段の指定がなければユークリッド距離が用いられるが，他にも以下のようなオプションがある。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "chapter15.html#距離行列を用いるもの",
    "href": "chapter15.html#距離行列を用いるもの",
    "title": "15  多変量解析(その2)",
    "section": "",
    "text": "非負性: \\(d(x,y) \\geq 0\\) (距離は0以上)\n同一性: \\(d(x,y) = 0 \\Leftrightarrow x = y\\) (距離が0なのは同じ点の場合のみ)\n対称性: \\(d(x,y) = d(y,x)\\) (距離は方向に依存しない)\n三角不等式: \\(d(x,z) \\leq d(x,y) + d(y,z)\\) (遠回りは最短距離以上)\n\n\n\n\n\"euclidean\": ユークリッド距離。\\(d(x,y) = \\sqrt{\\sum_{i=1}^N (x_i-y_i)^2}\\)で表される。\n\"maximum\": チェビシェフ距離。\\(d(x,y) = max(|x_i - y_i|)\\)で表される。\n\"manhattan\": マンハッタン距離。\\(d(x,y)=\\sum (|x_i - y_i|)\\)で表される。\n\"canberra\": キャンベラ距離。\\(d(x,y) = \\sum \\frac{|x_i-y_i|}{|x_i+y_i|}\\)で表される\n\"binary\": バイナリ距離。ジャッカード距離ともいう。0/1のデータに対する距離で，\\(d(x,y) = \\frac{b+c}{a+b+c+d}\\)で表される(aは両方1，bはxが1でyが0，cはxが0でyが1，dは両方0)。両方に共通して1である要素が多いほど距離が小さくなる。\n\"minkowski\": ミンコフスキー距離。一般化された距離とも言われ，係数\\(p\\)でさまざまな距離を表現できる。\\(d(x,y)=\\left(\\sum |x_i-y_i|^p\\right)^{\\frac{1}{p}}\\)で表される。例えば\\(p=1\\)ならばマンハッタン距離，\\(p=2\\)ならユークリッド距離である。\\(p=\\infty\\)の時を特にチェビシェフの距離，または優勢次元距離という。\n\n\n15.1.1 心理学における距離データ\n数字を何とみなすか，によって心理学でも距離データを扱うことはできる。 尺度評定の差分を(得点間の)距離とみなすこともできるし，相関係数も\\(1.0-|r_{jk}|\\)のようにすれば距離とみなすことができる。 社会心理学におけるソシオメトリックデータは，対人関係の選好評定だが，これも対人間の距離とみなすことができるだろう。 実験心理学における刺激の混同率や汎化勾配，2つの刺激が同じか違うかを判断する課題への反応潜時，刺激の代替価・連想価は類似性と考えられるから，これも距離データと言えるだろう(高根 1980)。 距離行列は一個体からの評定や反応からでも生成できるから，小サンプルの実験計画であっても距離行列を得ることができる。\nこのように，類似性あるいは非類似性を距離と見做して用いることができる。 この利点は，回答者の自然な判断に任せられること，つまり「総合的に判断して，似ているか，似ていないか」といった回答をデータにできることである。研究者はついつい複数の類似した項目で多角的に聞かねばならない，と思いがちだが，下位の評定次元を実験者が準備することは回答者の自由度を束縛している側面もあり，また回答者の負担を考えると必ずしもいいことばかりではない。さらに項目によっては社会的な望ましさバイアスなども含まれるから，「総合的に評価してもらいたい」というのはそういったバイアスから逃れられる側面もある。\n距離を対象に考える多変量解析モデルも，そのほかのモデルと同様，要約や分類を目的にしている。また，多次元データを少数の次元に要約するために可視化する手法として用いられることもある。まずは分類を目的としたモデルから見ていこう。\n\n\n15.1.2 クラスター分析\nクラスター分析は類似したものをまとめてクラスター(塊)を形成する分析方法である。 クラスター分析の中でも多くの手法・モデルが考えられており，いくつかの側面から分類することができる。\n\n15.1.2.1 モデルの階層性\n\n15.1.2.1.1 階層的クラスター分析\nまずはクラスターが階層性を持つかどうか。階層的クラスター分析は距離の短いものから順にまとめていき，クラスターのクラスター，クラスターのクラスターのクラスター，といったように順次大きなグループにまとめ上げていく。\n結果はデンドログラムと呼ばれるツリー状のプロットで表されることが一般的で，適当なところで分割して利用する。適切なクラスター数に関する一般的な基準はほとんどなく，実用性に応じてツリーをカットすることが多い。\nirisデータによる実行例を示す。\n\ndata(iris)\nd_matrix &lt;- dist(iris[, -5], method = \"euclidean\")\nresult.h &lt;- hclust(d_matrix, method = \"ward.D2\")\nplot(result.h, main = \"Hierarchical Clustering Dendrogram\")\n\n\n\n\n\n\n\n\n階層的クラスター分析のクラスター同士を上位クラスターにまとめていく方法がいくつか考案されており，Rではhclust関数のmethodオプションで指定することができる。\n\n\"ward.D\" / \"ward.D2\": ウォード法。クラスター内の分散を最小化する\n\"single\": 最短距離法。クラスター間の最短距離でリンク\n\"complete\": 最長距離法。クラスター間の最長距離でリンク\n\"average\": 平均法(群平均法)。クラスター間の平均距離でリンク\n\"mcquitty\": McQuitty法。重み付き群平均法の一種\n\"median\": メディアン法。重み付き群中心法の一種\n\"centroid\": 重心法。\n\n最もよく使われるのがウォード法で，実践的にもこの手法による分類が最も解釈しやすい。なお，ward.Dオプションはバグがあるので用いないことが望ましく，バグを修正したward.D2を用いること1。\n得られたクラスターの結果はcutree関数で任意のクラスター数に分割できる。今回，irisデータは3種類のirisがあることがわかっているので，クラスター数3にしてその分類精度を確認してみよう。\n\nclusters &lt;- cutree(result.h, k = 3)\ntable(clusters, iris$Species)\n\n        \nclusters setosa versicolor virginica\n       1     50          0         0\n       2      0         49        15\n       3      0          1        35\n\n\n\n\n15.1.2.1.2 非階層的クラスター分析\n非階層的クラスター分析として有名なのは，k-means法による分類である。アルゴリズムは次のとおりである。\n\n指定されたクラスター数の重心ベクトルをランダムに生成する。\n各データ点を最も近い重心のクラスターに所属させる。\nクラスターごとに，重心を再計算する。\n2.-3. のステップを繰り返し，変動がなくなると推定終了とする。\n\nこの方法は任意のクラス数に分類できること，大規模なデータであっても比較的早く収束することが利点である。 Rによるサンプルは以下のとおりである。\n\nresult.k &lt;- kmeans(d_matrix, centers = 3)\ntable(result.k$cluster, iris$Species)\n\n   \n    setosa versicolor virginica\n  1      0         49        13\n  2      0          1        37\n  3     50          0         0\n\n\n\n\n\n15.1.2.2 クラスタリングの境界\nここまでのクラスタリングは，各個体がどのクラスターに所属するかが明確に定まっていたが，境界がそこまで明確でない中間的なデータ点もあるかもしれない。各データ点が1つのクラスターにのみ所属するという明確なクラスタリングのことを，ハードクラスタリングとかクリスプクラスタリングという。これに対して，各データ点が複数のクラスターに部分的に所属している，あるいは所属度が例えば0-1などの連続値で表現されるような，緩やかな所属をゆるするクラスタリングもあり，これらを総称してファジィクラスタリングとかソフトクラスタリングと呼ぶ。\nファジィクラスタリングの例として，fuzzy c-means法を挙げる。\n\npacman::p_load(e1071)\nresult.c &lt;- cmeans(d_matrix, centers = 3, m = 2)\nhead(result.c$membership)\n\n            1         2           3\n1 0.002333834 0.9961596 0.001506519\n2 0.003508776 0.9942334 0.002257785\n3 0.006430854 0.9893041 0.004265036\n4 0.005065147 0.9916595 0.003275389\n5 0.002468256 0.9959249 0.001606858\n6 0.015285810 0.9753694 0.009344825\n\ntable(result.c$cluster, iris$Species)\n\n   \n    setosa versicolor virginica\n  1      0         48        13\n  2     50          0         0\n  3      0          2        37\n\n\nfuzzy c-meansはe1071パッケージに含まれている。 モデルの指定の時に，ファジィ度パラメータmを指定する。通常1.5から3程度で，大きいほど曖昧さをゆるす。 出力としてmembershipという所属確率が返される。この所属確率が最大のものをハードな分類として使用することができる。\nmembershipをプロットしてみると，明らかに所属するクラスが明確なものと，曖昧なデータもあることがわかる。 心理学的応用としては，パーソナリティの分類や症状の分類などが考えられるだろう。\n\n\n\n\n\n\n\n\n\n\n\n15.1.2.3 モデルベースか否か\n階層的クラスタリングや，k-means, fuzzy c-meansの非階層的クラスタリングモデルでは，クラスター数の決定について客観的な指標がなかった。そこで，確率モデルとしてクラスター分析を考え，モデル適合度の観点から評価することを考える。\nある変数についてヒストグラムを描き，次のような出力を得たとしよう。\n\n\n\n\n\n\n\n\n\nこのようなデータに対して，正規分布モデルを当てはめるのは適切だろうか。 正規分布は単峰で左右対称であることが特徴だから，無理やり当てはめるとおかしなことになるだろう。 ここには隠れた二つの正規分布があると考え，それぞれが混ざり合って出てきたものと考えたほうが良い。\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n統計モデルとしては，混合正規分布モデル(Gaussian Mixture Model, GMM)と呼ばれるのだが，これは異なる2つの群を生成モデルとしているクラスター分析であると考えることもできる。GMMでは観測されたデータが複数の正規分布を含んでいると考え，各群に想定される正規分布の平均，分散および群の混合率を推定することでデータの潜在的な特徴を明らかにする。今回は簡便のために1変数でのモデルにしたが，複数の変数がある場合は多変量正規分布で考えることになる。\n確率モデルになっているので，尤度を用いてデータとの適合度を計算することができる。潜在的な分布がいくつあるのかをBICなどを基準に選定することで，客観的にクラス数を決定することができるのが利点である。\nパッケージを使って具体的なデータを分類してみよう。\n\npacman::p_load(mclust)\n\n# irisデータから数値変数のみを取得\niris_data &lt;- iris[, 1:4]\n\n# mclustによるクラスタリング\ngmm_result &lt;- Mclust(iris_data)\n\n# 結果の表示\nsummary(gmm_result, parameters = TRUE)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VEV (ellipsoidal, equal shape) model with 2 components: \n\n log-likelihood   n df       BIC       ICL\n       -215.726 150 26 -561.7285 -561.7289\n\nClustering table:\n  1   2 \n 50 100 \n\nMixing probabilities:\n        1         2 \n0.3333319 0.6666681 \n\nMeans:\n                  [,1]     [,2]\nSepal.Length 5.0060022 6.261996\nSepal.Width  3.4280049 2.871999\nPetal.Length 1.4620007 4.905992\nPetal.Width  0.2459998 1.675997\n\nVariances:\n[,,1]\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.15065114  0.13080115   0.02084463  0.01309107\nSepal.Width    0.13080115  0.17604529   0.01603245  0.01221458\nPetal.Length   0.02084463  0.01603245   0.02808260  0.00601568\nPetal.Width    0.01309107  0.01221458   0.00601568  0.01042365\n[,,2]\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.4000438  0.10865444    0.3994018  0.14368256\nSepal.Width     0.1086544  0.10928077    0.1238904  0.07284384\nPetal.Length    0.3994018  0.12389040    0.6109024  0.25738990\nPetal.Width     0.1436826  0.07284384    0.2573899  0.16808182\n\n# 分類結果の可視化\nplot(gmm_result, what = \"classification\")\n\n\n\n\n\n\n\n# 真の種と比較\ntable(iris$Species, gmm_result$classification)\n\n            \n              1  2\n  setosa     50  0\n  versicolor  0 50\n  virginica   0 50\n\n# BICによるモデル選択結果\nplot(gmm_result, what = \"BIC\")\n\n\n\n\n\n\n\n\nmclustでは，各クラスターの共分散行列の構造によって異なるモデルが考慮される。モデル名は3文字のコードで表現され，それぞれの文字が以下の意味を持つ：\n\n1文字目(Volume): 各クラスターの大きさ(体積)\n\nE = Equal(等しい)\nV = Variable(異なる)\n\n2文字目(Shape): 各クラスターの形状\n\nE = Equal(等しい)\nV = Variable(異なる)\n\n3文字目(Orientation): 各クラスターの向き\n\nE = Equal(等しい)\nV = Variable(異なる)\nI = Identity(単位行列，球形)\n\n\n例えば： - EII: 全クラスターが同じ大きさの球形(等分散球形) - VII: 各クラスターが異なる大きさの球形(異分散球形) - EEE: 全クラスターが同じ大きさ・形状・向きの楕円 - VVV: 各クラスターが異なる大きさ・形状・向きの楕円(最も一般的)\nこれらの組み合わせにより，14種類の共分散構造から最適モデルをBICを参考に自動的に選定される。今回はVEV (ellipsoidal, equal shape)の2クラスターモデルが最適として判断された(実際は3種あるが，データからは2種類が最適と判断されている)。\n潜在的に分類する手法は，例えばマーケティング業界では購買層を探索的に見出す手法として使われる。潜在的なクラスが順序尺度水準であることを想定すれば，潜在ランクモデルと呼ばれ，テスト理論の応用モデルとして提案されている。Shojima (2022) ではIRTのような精緻な\\(\\theta_i\\)の推定よりも段階的な推定の方が実践的意義が高いことから，潜在ランクモデルを活用する利点が論じられている。\n\n\n15.1.2.4 バイクラスタリング\nCattelのデータキューブのところで触れたように，Observation \\(\\times\\) Variablesのデータセットがあったとき，変数の共変動から個人を分類することも，個人の共変動から変数を分類することもできる。そしてまた，両者を同時に分類するバイクラスタリング(Biclustering)という手法も提案されている。\nBiclusteringはTwo-Mode Clusteringとも呼ばれ，様々なモデルが提案されているが，ここでは Shojima (2022) のテスト理論の観点から見てみよう。\nテスト理論はデータがバイナリであり，この分析の確率モデルとしてはベルヌーイ分布を置いたIRTが一般的である。しかし上で述べたように，IRTで推定されるような潜在得点\\(\\theta\\)の精度は実質的な意味が見えにくいことがある。すなわち，\\(\\theta\\)が\\(0.01\\)ポイント違うことが，どのような違いに相当するのか。\\(\\theta\\)を\\(0.5\\)ポイント上昇させるために受検者はどのような努力をすれば良いのか。また実用上も，数段階の診断結果や，単純な合否の2段階に分割してのフィードバックをするのであれば，そこまで細かい分類は必要ないかもしれない。\nShojima (2022) のバイクラスタリングでは，テストデータにおける項目を複数のフィールドに，受検者を複数のクラスに分類する。受験者の分類は正答率に応じて順序づけることができ，ランクとして表現することができる(ランクで表現されるモデルは特にランクラスタリングとよばれる)。\n荘島のバイクラスタリングモデルを形式化するために，主要な行列を定義しよう。\\(J\\)を項目数，\\(S\\)を受検者数，\\(C\\)を潜在クラス/ランク数，\\(F\\)を潜在フィールド数とする。\nバイクラスター参照行列\\(\\boldsymbol{\\Pi}_B\\)は次のように定義される：\n\\[\n\\boldsymbol{\\Pi}_B=\\left[\\begin{array}{ccc}\n\\pi_{11} & \\cdots & \\pi_{1F} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\pi_{C1} & \\cdots & \\pi_{CF}\n\\end{array}\\right]=\\left\\{\\pi_{fc}\\right\\}\n\\]\nここで各要素\\(\\pi_{fc}\\)は，クラス/ランク\\(c\\)の受検者がフィールド\\(f\\)の項目に正答する確率を表す。\nクラス所属行列\\(\\mathbf{M}_C\\)とフィールド所属行列\\(\\mathbf{M}_F\\)は次のように定義される：\n\\[\n\\mathbf{M}_C=\\left[\\begin{array}{ccc}\nm_{11} & \\cdots & m_{1C} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nm_{S1} & \\cdots & m_{SC}\n\\end{array}\\right], \\quad\n\\mathbf{M}_F=\\left[\\begin{array}{ccc}\nm_{11} & \\cdots & m_{1F} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nm_{J1} & \\cdots & m_{JF}\n\\end{array}\\right]\n\\]\nランククラスタリングにおけるランク所属行列\\(\\mathbf{M}_R\\)は，クラス所属行列\\(\\mathbf{M}_C\\)に，前後のクラスのつながりを緩やかに繋げるフィルター行列\\(\\mathbf{F}\\)をかけることで得られる。\n\\[\n\\mathbf{M}_R = \\mathbf{M}_C \\mathbf{F}\n\\]\nフィルタ行列は，例えばランク数が6の場合，次のような行列になる。 \\[\n\\mathbf{F}=\\left[\\begin{array}{rrrrrrr}\n0.864 & 0.120 & & & & & \\\\\n0.136 & 0.760 & 0.120 & & & & \\\\\n& 0.120 & 0.760 & 0.120 & & & \\\\\n& & 0.120 & 0.760 & 0.120 & & \\\\\n& & & 0.120 & 0.760 & 0.120 & \\\\\n& & & & 0.120 & 0.760 & 0.136 \\\\\n& & & & & 0.120 & 0.864 \\\\\n\\end{array}\\right]\n\\]\nこれらを踏まえて，尤度関数は次のように定義され，EMアルゴリズムによって推定される。\n\\[\nl(\\mathbf{U}\\mid \\boldsymbol{\\Pi}_B) = \\prod_{s=1}^S\\prod_{j=1}^J\\prod_{f=1}^F\\prod_{c=1}^C \\left(\\pi_{fc}^{u_{sj}} (1-\\pi_{fc})^{1-u_{sj}}\\right)^{z_{sj} m_{sc} m_{jf}}\n\\]\nこれを実装したパッケージexametrikaとそのサンプルコードをみて，実践例を見てみよう。\n\npacman::p_load(exametrika)\nresult.Ranklustering &lt;- Biclustering(J35S515,\n  nfld = 5, ncls = 6,\n  method = \"R\", verbose = F\n)\nplot(result.Ranklustering, type = \"Array\")\n\n\n\n\n\n\n\n\nexametrikaパッケージのバイクラスタリングは，引数にデータ，フィールド数nfld，クラス(ランク)数ncls，および手法(Bならバイクラスタリング，Rならランクラスタリング)をとる。 ここではパッケージに含まれているサンプルデータJ35S515を使っているが，これは35項目からなるテストで515人の受検者からの回答を得たものである。\n分析結果としてアレイプロットが示されている。この図の左は，行ごとに受検者，列ごとに項目からなり，正答を黒い四角(■)，誤答を白い四角(□)で表現したローデータである。右に示されているのは分析結果による同様の表示で，ランクごと，フィールドごとに類似したパターンがまとめられていることがわかる。\nフィールドの分類とランクをみることで，受検者には次のランクに進むにはどの領域の項目に正答すれば良いか，といった情報を提供することができる。また，フィールドやランクへの所属は確率で表現され(ファジィクラスタリング)，受検者にはランクアップOdds, ランクダウンOddsを提供することができる。\nフィールド所属行列，ランク所属行列を可視化したプロファイルの出力を以下に示す。\n\nplot(result.Ranklustering, type = \"FRP\", nc = 2, nr = 3)\n\n\n\n\n\n\n\nplot(result.Ranklustering, type = \"RMP\", students = 1:9, nc = 3, nr = 3)\n\n\n\n\n\n\n\n\nバイクラスタリングは多値モデルも開発されており，心理尺度の新しい分析手法として期待されている。というのも，クラスタリングは表層的な反応パターンによる分類で，因子分析法やIRT(GRM)のようなデータ生成メカニズムを仮定しないことから，潜在変数の意味解釈といった理論的問題を避けることができるからである。また，項目についてのクラスタリングは得点もしくは変化得点を質的な意味に割り当てることが容易であることもその理由として挙げられる。\nなお，パッケージexametrikaには Shojima (2022) に含まれる12のモデル全てを実装している。詳しくはサイトを参照のこと。\n\n\n\n15.1.3 多次元尺度構成法\n多次元尺度構成法(Multidimensional Scaling,MDS)は，一言で言えば距離行列から地図を復元する手法である。 MDSは大きく分けて計量(metric)MDSと非計量(non-metric)MDSがある。前者は距離行列の固有値分解から得られる固有ベクトルを座標とみなす，ストレートな表現方法であり，データが比率尺度水準以上の誤差のない数値であることが求められる。これによって， どうしてそのような解釈が可能なのかについては，Young-Householderの定理というMDSの基礎になった数学定理があるので，興味がある人は調べて見てほしい。\n以下に計量MDSの例を見てみよう。Rはeurodistというヨーロッパ各都市の距離についてのサンプルデータを持っており，これを使って基本関数cmdscaleでMDSを実行してみよう。\n\nresult &lt;- cmdscale(eurodist, k = 2)\n\n# 美しいプロットの作成\nplot(result[, 1], result[, 2],\n  type = \"n\", # 点は描かずにプロット領域だけ作成\n  xlab = \"次元1\", ylab = \"次元2\",\n  main = \"ヨーロッパ都市間の多次元尺度構成法\",\n  cex.main = 1.2,\n  cex.lab = 1.1\n)\n\n# 都市名をラベルとして表示\ntext(result[, 1], result[, 2],\n  labels = rownames(result),\n  cex = 0.8,\n  col = \"darkblue\",\n  font = 2\n)\n\n# グリッドを追加\ngrid(lty = 2, col = \"lightgray\")\n\n\n\n\n\n\n\n\nアテネ(Athens)が右上，ストックホルム(Stockholm)が下に来ていることから，南北が反転していると思われるが，それぞれの相対的な位置は大体復元できていることがわかるだろう。関数cmdscaleは引数kで次元数を指定でき，今回は2次元を指定したが，実際は地球が球体だからk=3とすることが正しいが，事前知識がない場合は可視化のために低次元を指定することが一般的である。\neudodistは実際の距離データであるから，比率尺度水準の誤差のないデータと見做せるだろう。しかし，心理学的な応用場面においては，比率尺度水準のデータや誤差のないデータは考えにくい。このとき用いられるのが，非計量MDSである。これを簡単に言えば，データの持つ大小関係を反映した多次元空間に対象を付置する(地図の座標を与える)ものである。すなわち，対象\\(i\\)と\\(j\\)の距離を\\(d_{ij}\\)と表すとすると， \\[ d_{ij} &lt; d_{kl} \\to \\delta_{ij} &lt; \\delta_{kl} \\]\nという関係が保持されるような座標\\(\\delta_{ij}\\)を求めるものである。非計量MDSのはしりであったKruscalの方法は，データとの適合を表すstress値として， \\[ \\sum_{i&gt;j} e_{ij}^2 = \\sum_{ij} (\\delta_{ij}-d_{ij})^2 \\]\nという最適化関数を最小化するように座標を求める。最適化関数は多くの研究者によって様々に提唱されており，Rでは便利なパッケージsmacofのアルゴリズム(Scaling by Majorizing a COmplicated Function)による実行がいいだろう。\n実例で見てみよう。smacofパッケージの持つFaceExpデータセットを用いる。これは顔表情の類似度評定データで、異なる表情間の知覚的類似性を測定したものである。\n\npacman::p_load(smacof)\n\n# 次元数2から10でストレス値を計算\ndimensions &lt;- 2:10\nstress_values &lt;- numeric(length(dimensions))\n\nfor (i in seq_along(dimensions)) {\n  result_temp &lt;- mds(FaceExp, ndim = dimensions[i], type = \"ordinal\")\n  stress_values[i] &lt;- result_temp$stress\n}\nstress_values\n\n[1] 0.106248100 0.059778016 0.035605225 0.019262481 0.011722294 0.007084647\n[7] 0.007084647 0.007084647 0.007084647\n\n# ストレス値のプロット\nplot(dimensions, stress_values,\n  type = \"b\", pch = 16,\n  xlab = \"次元数\", ylab = \"ストレス値\",\n  main = \"次元数とストレス値の関係（顔表情データ）\",\n  cex.main = 1.1,\n  cex.lab = 1.0,\n  col = \"blue\", lwd = 2\n)\ngrid(lty = 2, col = \"lightgray\")\n\n\n\n\n\n\n\n\nここでは次元数を様々に変えて，適合度指標であるストレス値を得てプロットしている。一般的に，ストレス値が0.05以下なら優秀，0.1以下なら良好，0.2以下なら普通とされる。\nプロットや値を見てみると，3次元でも十分な解が得られそうだ。改めて3次元であることを指定して分析し，プロットしてみよう。なお，mds関数のtype引数に順序尺度水準であることを明記することで，適切な分析が行われる。\n\n# 3次元でのMDS結果\nresult &lt;- mds(FaceExp, ndim = 3, type = \"ordinal\")\nresult\n\n\nCall:\nmds(delta = FaceExp, ndim = 3, type = \"ordinal\")\n\nModel: Symmetric SMACOF \nNumber of objects: 13 \nStress-1 value: 0.06 \nNumber of iterations: 73 \n\n# 3次元MDSの可視化（3つの2次元プロット）\n\n# 次元1 vs 次元2\nplot(result$conf[, 1], result$conf[, 2],\n  type = \"n\",\n  xlab = \"次元1\", ylab = \"次元2\",\n  main = paste(\"次元1 vs 次元2\\nStress =\", round(result$stress, 3)),\n  cex.main = 1.0,\n  cex.lab = 0.9\n)\ntext(result$conf[, 1], result$conf[, 2],\n  labels = rownames(result$conf),\n  cex = 0.7,\n  col = \"darkblue\",\n  font = 2\n)\ngrid(lty = 2, col = \"lightgray\")\n\n\n\n\n\n\n\n# 次元1 vs 次元3\nplot(result$conf[, 1], result$conf[, 3],\n  type = \"n\",\n  xlab = \"次元1\", ylab = \"次元3\",\n  main = paste(\"次元1 vs 次元3\\nStress =\", round(result$stress, 3)),\n  cex.main = 1.0,\n  cex.lab = 0.9\n)\ntext(result$conf[, 1], result$conf[, 3],\n  labels = rownames(result$conf),\n  cex = 0.7,\n  col = \"darkblue\",\n  font = 2\n)\ngrid(lty = 2, col = \"lightgray\")\n\n\n\n\n\n\n\n# 次元2 vs 次元3\nplot(result$conf[, 2], result$conf[, 3],\n  type = \"n\",\n  xlab = \"次元2\", ylab = \"次元3\",\n  main = paste(\"次元2 vs 次元3\\nStress =\", round(result$stress, 3)),\n  cex.main = 1.0,\n  cex.lab = 0.9\n)\ntext(result$conf[, 2], result$conf[, 3],\n  labels = rownames(result$conf),\n  cex = 0.7,\n  col = \"darkblue\",\n  font = 2\n)\ngrid(lty = 2, col = \"lightgray\")\n\n\n\n\n\n\n\n\n\nprint(result$conf)\n\n                                          D1          D2          D3\nGrief at death of mother          0.28389211 -0.35088341 -0.04383041\nSavoring a Coke                  -0.30306573  0.10620276 -0.29595552\nVery pleasant surprise           -0.71552867  0.56031573 -0.04008618\nMaternal love-baby in arms       -0.63827700 -0.09006026  0.02975313\nPhysical exhaustion              -0.01879485 -0.49089080 -0.21059225\nSomething wrong with plane        0.40829233 -0.05617713 -0.38685372\nAnger at seeing dog beaten        0.90768476  0.20575384 -0.18027858\nPulling hard on seat of chair    -0.28180980  0.41937543  0.23764181\nUnexpectedly meets old boyfriend -0.65807066  0.18760808 -0.15097801\nRevulsion                         0.32995517 -0.40195569  0.32170945\nExtreme pain                      0.27856073 -0.05436048  0.46450844\nKnows plane will crash            0.67130146  0.61301964  0.04768550\nLight sleep                      -0.26413984 -0.64794771  0.20727634\n\n\nこのプロットから，顔表情間の知覚的類似性の構造を読み取ることができる。第一次元は快不快（valence）、第二次元は覚醒度（arousal）、第三次元は自然性（naturalness）を表していると解釈できる。\nより客観的な命名がしたい場合，座標と外部変数との相関などを求めて考える。相関係数はベクトルの角度に基づく\\(\\cos(\\theta)\\)であり，軸上に外部変数の補助線を引くなどすればわかりやすい。こうした方法については， グリム and ヤーノルド ([1994] 2016) に詳しい。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "chapter15.html#共頻行列を用いるもの",
    "href": "chapter15.html#共頻行列を用いるもの",
    "title": "15  多変量解析(その2)",
    "section": "15.2 共頻行列を用いるもの",
    "text": "15.2 共頻行列を用いるもの\n続いて共頻行列に基づく分析について考えてみよう。 共頻行列とはカテゴリカルなデータのクロス表であり，カテゴリが同時に生起していることを表す。このことが当該カテゴリの近さを表す共変量だとして分析を行う。\nカテゴリカルなデータであるから，応用範囲は広い。よく知られているのがテキストマイニングにおける応用例である。\n\n15.2.1 テキストマイニングとは\nテキストマイニングとは自然言語データを統計的に処理するための，一連の技法の名称である。小説やWeb上の記事，自由記述回答や逐語録など自由に書かれた自然言語表現を統計的に処理し，そこから意味のある知見を引き出す。\n日本語のテキストマイニングは，基本的に形態素解析＋多変量解析の技術の総称であると言える。日本語は英語をはじめとする単語で分かち書きされる文章とは違い，一連の文字列から品詞ごとに文章を区分する必要がある。この文章から品詞の切り分け，活用前の原型を抽出するなどの操作を形態素解析という。形態素解析には日本語辞書に基づく形態素解析エンジンが必要で，MECABやJANOME，ChaSenなどが知られている。これらをRやPythonで扱うためのパッケージも存在する。\n形態素解析によって，文章の中に当該単語が何回出現したかを表す文書単語行列を作成する。これは一般に，単語の数が非常に多くなるから疎な矩形行列になる。この矩形行列を対象に特異値分解を行なったり，単語\\(\\times\\)単語の(正方)共頻行列にすることで，多変量解析の対象となる。\n多変量解析モデルはどのようなものでもよく，ここで紹介したクラスター分析や多次元尺度法などがよく用いられる。行列のサイズが大きくなりがちなので，一般的な多変量解析では少ない次元での適合度が悪くなりがちである。ある程度は適合度を諦めて可視化のために低次元にするか，自己組織化マップ(Self-Organization Mapping)2などのアプローチで強制的に分類・可視化する方法などがとられることが多い。\n多変量解析で分析することももちろん可能だが，ビッグデータと機械学習の組み合わせにより，形態素ではなくトークン3を単位に用いることも多い。\nここでは形態素解析によるテキストマイニングの例を見てみよう。 Rで形態素解析をするには，RMeCabパッケージを用いることが多い。最新版は作者のGithubサイト4 から導入するのがいいだろう。まずは形態素解析エンジンMecabをインストールし，その後でRMecabをインストールする5。詳しくは作者のサイト6を参照して欲しい。\n最近はこの一連の手間を省いてくれる，gibasaというパッケージがある。gibasaはCRANに登録されており，内部でMeCabのバイナリファイルを含んでいるので，外部ファイルを準備する必要がない仕組みになっている7。ここではこのパッケージを使った例を見てみよう。\nパッケージを読み込んで，適当な文章を形態素解析してみよう。\n\npacman::p_load(tidyverse, gibasa)\ntext &lt;- \"私は昨日，カフェでコーヒーを飲んだ。\"\ndat &lt;- gibasa::tokenize(text)\ndat\n\n# A tibble: 11 × 5\n   doc_id sentence_id token_id token    feature                                 \n   &lt;fct&gt;        &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                                   \n 1 1                1        1 私       名詞,代名詞,一般,*,*,*,私,ワタクシ,ワタクシ……\n 2 1                1        2 は       助詞,係助詞,*,*,*,*,は,ハ,ワ            \n 3 1                1        3 昨日     名詞,副詞可能,*,*,*,*,昨日,キノウ,キノー\n 4 1                1        4 ，       記号,読点,*,*,*,*,，,，,，              \n 5 1                1        5 カフェ   名詞,一般,*,*,*,*,カフェ,カフェ,カフェ  \n 6 1                1        6 で       助詞,格助詞,一般,*,*,*,で,デ,デ         \n 7 1                1        7 コーヒー 名詞,一般,*,*,*,*,コーヒー,コーヒー,コーヒー……\n 8 1                1        8 を       助詞,格助詞,一般,*,*,*,を,ヲ,ヲ         \n 9 1                1        9 飲ん     動詞,自立,*,*,五段・マ行,連用タ接続,飲む,ノン,ノン……\n10 1                1       10 だ       助動詞,*,*,*,特殊・タ,基本形,だ,ダ,ダ   \n11 1                1       11 。       記号,句点,*,*,*,*,。,。,。              \n\n\ntokenize関数によって文字列が品詞ごとに区分され，またその特徴がfeature列に入っていることがわかるだろう。 このfeature列に入っているのはgibasaが内蔵するMeCabの戻り値であり，このラベルを整理するためにはprettify関数を用いる。\nまた，実際のテキストマイニングにおいては助詞や記号は使いにくいし，動詞も活用する前の原型で考えるほうがいいだろう。 これらをパイプ演算子で繋ぎながら処理すると次のようになる。\n\ngibasa::prettify(dat, col_select = c(\"POS1\", \"Original\")) |&gt;\n  dplyr::filter(POS1 %in% c(\"名詞\", \"動詞\", \"形容詞\"))\n\n# A tibble: 5 × 6\n  doc_id sentence_id token_id token    POS1  Original\n  &lt;fct&gt;        &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   \n1 1                1        1 私       名詞  私      \n2 1                1        3 昨日     名詞  昨日    \n3 1                1        5 カフェ   名詞  カフェ  \n4 1                1        7 コーヒー 名詞  コーヒー\n5 1                1        9 飲ん     動詞  飲む    \n\n\nここでは1つの文だけで分析を行ったが，次に実際のテキストマイニングの例として，京都ラーメンに関する感想文を使った分析例を見てみよう。\nここでの処理の流れは次のとおりである。\n\n文章を形態素に分割する\n文章ごとに単語の出現度数を書いた文書\\(\\times\\)単語行列をつくる(dtm)\n文書単語行列から単語\\(\\times\\)単語の距離行列を作る\n距離行列を対象に計量MDSを使って可視化\n\n\n# 京都ラーメンに関する感想文データの作成\nramen_reviews &lt;- c(\n  \"京都ラーメンは意外とコッテリしたのが多いんだよね。\",\n  \"濃厚なスープと細麺の組み合わせが絶妙で美味しかった。\",\n  \"ドロドロとした鶏ガラベースのスープが京都らしくて好み。\",\n  \"老舗のラーメンは深いコクがあり，麺との絡みが良くて好き。\",\n  \"京都駅近くの店で食べた醤油ラーメンのチャーシューが柔らかくて最高。\",\n  \"京都ラーメンといえば天下一品のコッテリが魅力的。\",\n  \"背脂がたっぷりのラーメンも京都で食べると格別だった。\",\n  \"細麺とスープのバランスが絶妙で，また食べたくなる味。\",\n  \"京都らしい濃い醤油ラーメンに九条ネギがよく合っていた。\",\n  \"老舗の店主が作る丁寧なラーメンは心に残る味わいだった。\"\n)\n\n# トークン化して，品詞を選び出し，文章ごとにカウントする\ndat_count &lt;- ramen_reviews |&gt;\n  # テキストを形態素に分解（単語・品詞・活用形など）\n  gibasa::tokenize() |&gt;\n  # 品詞情報と原型を選択・整理\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  # 分析対象を名詞・形容詞に限定\n  dplyr::filter(POS1 %in% c(\"名詞\", \"形容詞\")) |&gt;\n  # データ変換処理\n  dplyr::mutate(\n    doc_id = forcats::fct_drop(doc_id),\n    # 原型がある場合は原型を、ない場合は現在の形を使用\n    token = dplyr::if_else(is.na(Original), token, Original)\n  ) |&gt;\n  # 文書ID別・単語別に出現回数をカウント\n  dplyr::count(doc_id, token)\n\n# 文章単語行列の作成\ndtm &lt;- dat_count |&gt;\n  tidyr::pivot_wider(\n    id_cols = doc_id,\n    names_from = token,\n    values_from = n,\n    values_fill = 0 # 欠損値（その文書にその単語が出現しない）を0で埋める\n  )\n\ndtm\n\n# A tibble: 10 × 45\n   doc_id    の    ん コッテリ ラーメン  京都  多い スープ  濃厚    細\n   &lt;fct&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 1          1     1        1        1     1     1      0     0     0\n 2 2          0     0        0        0     0     0      1     1     1\n 3 3          0     0        0        0     1     0      1     0     0\n 4 4          0     0        0        1     0     0      0     0     0\n 5 5          0     0        0        1     1     0      0     0     0\n 6 6          0     0        1        1     1     0      0     0     0\n 7 7          0     0        0        1     1     0      0     0     0\n 8 8          0     0        0        0     0     0      1     0     1\n 9 9          0     0        0        1     1     0      0     0     0\n10 10         0     0        0        1     0     0      0     0     0\n# ℹ 35 more variables: 組み合わせ &lt;int&gt;, 絶妙 &lt;int&gt;, 美味しい &lt;int&gt;, 麺 &lt;int&gt;,\n#   ガラベース &lt;int&gt;, 好み &lt;int&gt;, 鶏 &lt;int&gt;, コク &lt;int&gt;, 好き &lt;int&gt;, 深い &lt;int&gt;,\n#   老舗 &lt;int&gt;, 良い &lt;int&gt;, チャーシュー &lt;int&gt;, 店 &lt;int&gt;, 最高 &lt;int&gt;,\n#   柔らかい &lt;int&gt;, 近く &lt;int&gt;, 醤油 &lt;int&gt;, 駅 &lt;int&gt;, 天下一品 &lt;int&gt;, 的 &lt;int&gt;,\n#   魅力 &lt;int&gt;, 格別 &lt;int&gt;, 背 &lt;int&gt;, 脂 &lt;int&gt;, バランス &lt;int&gt;, 味 &lt;int&gt;,\n#   ネギ &lt;int&gt;, 九 &lt;int&gt;, 条 &lt;int&gt;, 濃い &lt;int&gt;, 丁寧 &lt;int&gt;, 味わい &lt;int&gt;,\n#   店主 &lt;int&gt;, 心 &lt;int&gt;\n\n\n\ndistance_matrix &lt;- dist(t(dtm[,-1]))\n\n# 多次元尺度構成法（MDS）の実行\nmds_result &lt;- cmdscale(distance_matrix, k = 2) # 2次元での古典的MDS\n\n# ggplotで可視化（ggrepelを使用してラベル重複を回避）\npacman::p_load(ggrepel) # ラベル重複回避のためのパッケージ\n\n# MDS結果をデータフレームに変換\nmds_df &lt;- data.frame(\n  dim1 = mds_result[, 1], # 第1次元の座標\n  dim2 = mds_result[, 2], # 第2次元の座標\n  word = rownames(mds_result) # 単語名\n)\n\n# ggplotで散布図を作成\nggplot(mds_df, aes(x = dim1, y = dim2)) +\n  geom_point(color = \"darkblue\", size = 2, alpha = 0.7) + # 点をプロット\n  geom_text_repel( # ggrepelを使用してラベル重複を回避\n    aes(label = word), # 単語をラベルとして表示\n    size = 3.5, # ラベルサイズ\n    color = \"darkblue\", # ラベル色\n    fontface = \"bold\", # 太字\n    box.padding = 0.3, # ラベル周りの余白\n    point.padding = 0.3, # 点周りの余白\n    max.overlaps = Inf # 重複制限なし\n  ) +\n  labs(\n    title = \"ラーメンレビュー単語の多次元尺度構成法\",\n    x = \"次元1\",\n    y = \"次元2\"\n  ) +\n  theme_minimal() + # シンプルなテーマ\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14), # タイトル中央寄せ\n    panel.grid = element_line(linetype = \"dashed\", color = \"lightgray\") # グリッド線\n  )\n\n\n\n\n\n\n\n\nここでプロットされているのは，単語同士の類似度が近いものは近くに，遠いものは遠くにプロットされる地図である。これをみると，「麺」と「スープ」や「組み合わせ」「バランス」といった言葉が近くに付置されており，関係が深そうなことが読み取れる。\nただし，「九条ネギ」が「九」と「条」に別れているように，機械的に分解することの限界もあり，こうした場合は辞書を変更したり，特定の単語・専門用語などを強制的に取り出すような工夫をする必要がある。形態素解析を経由したテキストマイニングは，多変量解析に入る前の事前クリーニングにかなりの労力を要することが少なくない。\nまた，今回は10件程度の例であったが，基本的にテキストマイニングは膨大なデータや学習された辞書をもちいることが主流になってきている。これは生成AIとの相性もよく(生成AIはすでに自然言語についてある程度学習済みのものとも言える)，形態素解析や統計モデルを経ないテキストマイニングがこれから主流になってくるかもしれない。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "chapter15.html#偏相関行列を用いるもの",
    "href": "chapter15.html#偏相関行列を用いるもの",
    "title": "15  多変量解析(その2)",
    "section": "15.3 偏相関行列を用いるもの",
    "text": "15.3 偏相関行列を用いるもの\n最後に偏相関行列を用いる統計モデルを紹介しておこう。\n(ピアソンの)相関行列は多くの線形モデルで多用されているが，変数\\(X\\)と\\(Y\\)の背後に，両者に影響する\\(Z\\)がある場合，見掛け上相関が高くなっているが，\\(Z\\)の影響を除外する(統計的に統制する)とそれほど相関が高くない，ということもあり得る。\nこの統計的に統制するというのは，\\(X\\)を\\(Z\\)で回帰して得られた残差\\(R_x\\)と，\\(Y\\)を\\(Z\\)で回帰して得られた残差\\(R_y\\)との相関であり，偏相関と呼ばれる。この相関の方が，本来的な関係を捉えていると言えるかもしれない。ここでは\\(X,Y\\)の2変数の例であったが，多変量の相関行列に関しても同様に当該2変数以外の変数で(重)回帰をし，その残差同士の相関を出した偏相関行列を考えることができる。これは逐一回帰分析をしなくても，次のような行列演算で一括して求めることができる。\n相関行列を\\(\\mathbf{R}\\)とすると，偏相関行列\\(\\mathbf{P}\\)の要素は次の式で計算される： \\[p_{ij} = -\\frac{r^{ij}}{\\sqrt{r^{ii} r^{jj}}}\\]\nここで\\(r^{ij}\\)は相関行列の逆行列\\(\\mathbf{R}^{-1}\\)の\\((i,j)\\)要素である。\nさて，因子分析は相関行列から始めて関係の強いところを因子にまとめるというイメージだが，この手順を反転させて，偏相関行列の関係の弱いところの繋がりをカットする，という方法で，純粋な変数間関係だけ残るようにして可視化する方法がある。\nこの分析方法はグラフィカルモデリングと呼ばれる。量的な変数の場合は偏相関行列から，質的な変数の場合でも多元分割行列から，変数間の条件付き独立性を検証する方法で進められる。可視化の手段として，変数間関係をノードとエッジからなるネットワークで表現することから，最近ではネットワーク分析と呼ばれることもある。\n心理学においては，因子が心理学的構成概念だと捉えられることが多い。しかし，構成概念とはそもそも単体でそこに「ある」ものではなく，いろいろな現象の総合的な全体である，というシステム論的な観点によれば，このネットワーク分析的なアプローチの方が適しているかもしれない。この理論的な体系と方法論の合体については，アデラ＝マリア et al. ([2022] 2024) を参考にしてほしい。\n具体的な例で見てみよう。 例えば，性格検査のBig5データを使って相関行列と，その偏相関行列を見てみよう。\n\npacman::p_load(psych, corrplot, RColorBrewer)\n\n# Big5性格データの読み込み（最初の25項目のみ使用）\nbfi &lt;- psych::bfi[, 1:25]\nbfi_clean &lt;- na.omit(bfi) # 欠損値を除去\n\n# 1. 相関行列の計算\ncor_matrix &lt;- cor(bfi_clean)\n# 2. 偏相関行列の計算（行列演算で実行）\nR_inv &lt;- solve(cor_matrix) # 相関行列の逆行列を計算\n\n# 偏相関行列の各要素を計算\npartial_cor_matrix &lt;- matrix(0, nrow = nrow(R_inv), ncol = ncol(R_inv))\nfor(i in 1:nrow(R_inv)) {\n  for(j in 1:ncol(R_inv)) {\n    if(i != j) {\n      # 偏相関の公式: p_ij = -r^ij / sqrt(r^ii * r^jj)\n      partial_cor_matrix[i, j] &lt;- -R_inv[i, j] / sqrt(R_inv[i, i] * R_inv[j, j])\n    }\n  }\n}\n# 対角要素は1に設定\ndiag(partial_cor_matrix) &lt;- 1\n\n# 行名・列名を設定\nrownames(partial_cor_matrix) &lt;- rownames(cor_matrix)\ncolnames(partial_cor_matrix) &lt;- colnames(cor_matrix)\n\n\n# 3. 相関行列と偏相関行列の比較表示\npar(mfrow = c(1, 2))\n\n# 相関行列ヒートマップ\ncorrplot(cor_matrix, \n         method = \"color\",\n         order = \"alphabet\", # 同じ順序で比較\n         tl.cex = 0.6,\n         tl.col = \"black\",\n         col = brewer.pal(n = 8, name = \"RdYlBu\"),\n         title = \"相関行列\",\n         mar = c(0, 0, 2, 0),\n         cl.pos = \"n\") # カラーバーを非表示\n\n# 偏相関行列ヒートマップ\ncorrplot(partial_cor_matrix, \n         method = \"color\", # 色で偏相関の強さを表示\n         order = \"alphabet\", # アルファベット順で表示（相関行列と同じ順序で比較）\n         tl.cex = 0.6, # 変数名のテキストサイズ\n         tl.col = \"black\", # 変数名の色\n         col = brewer.pal(n = 8, name = \"RdYlBu\"), # 青-黄-赤のカラーパレット\n         title = \"偏相関行列\", # 図のタイトル\n         mar = c(0, 0, 2, 0)) # 図の余白設定（下，左，上，右）\n\n\n\n\n\n\n\npar(mfrow = c(1, 1)) #環境を元に戻す\n\n相関行列と偏相関行列を並べてみると，所々相関のパターンが違うところが見える。\nさて，ネットワークの表現をするには相関行列でも偏相関行列でも構わないのだが，一応ここでは偏相関行列を用いたネットワークを見てみよう。\nここではqgraphパッケージを用いている。 この関数のオプションとして，推定にglassoを用いているが，ガウシアングラフィカルモデルの一種で，L1正則化(lasso)を行い重要でない変数間の関係を0にするモデルである。これでBICを基準に不要なパスをカットして描画される。描画のレイアウトオプションはspringだが，これはなるべく自然な配置にしてくれる方法であり，他にもいろいろなオプションがあり得る。\n\npacman::p_load(qgraph)\nBICgraph &lt;- qgraph(\n  partial_cor_matrix, # 偏相関行列を入力データとして使用\n  graph = \"glasso\", \n  sampleSize = nrow(bfi), # サンプルサイズを指定（統計的有意性の判定に使用）\n  tuning = 0, # 正則化パラメータ（0=BIC基準で自動選択、大きいほどスパース）\n  layout = \"spring\", \n  title = \"BIC\", # グラフのタイトル\n  threshold = TRUE, # 弱い結合を除去してスパースなネットワークを作成\n  details = TRUE # 詳細な出力情報を表示\n)\n\nNote: Network with lowest lambda selected as best network: assumption of sparsity might be violated.\n\n\n\n\n\n\n\n\n\nさて，ネットワーク分析は「これが因子！」のような特定の何かに帰着するのではなく，全体像をそのまま見て捉えることが特徴的である。しかしそれでは何がなんだか分かりにくいということもあろう。ということで，ネットワークの中心がどこにあるのか，どのノードとどのノードの結びつきが強いのか，といったことを指標とする。これらは中心性指数と呼ばれ，次のコードで可視化される。\n\ncentralityPlot(\n  list(BIC = BICgraph),\n  include = \"all\"\n)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nここで示されている各指標の意味は次のとおりである。\n\nStrength（強度）\n\n\nノードに接続しているエッジの重みの合計\nネットワークでは、そのノードが他のノードとどれだけ強く関連しているかを表す\n次の式で計算される。\\(strength = \\sum | w_{ij}|\\)（絶対値の和）\n\n\nCloseness（近接中心性）\n\n\n他の全てのノードまでの最短距離の逆数\nそのノードが他の全てのノードにどれだけ「近い」かを測定\n情報伝播や影響の広がりやすさを表すといえる\n距離の合計の逆数であり，次の式で計算される。\\(closeness = \\frac{1}{\\sum d_{ij}}\\)\n\n\nBetweenness（媒介中心性）\n\n\n他のノード間の最短経路上にそのノードが位置する頻度であり，「橋渡し」の役割を果たすノードを特定する。\nネットワークでは症状間の連鎖反応の「ハブ」を表す\n\n\nExpected Influence（期待影響力）\n\n\nstrengthの改良版で、正と負のエッジを区別したもの\n正のエッジは加算、負のエッジは減算。ここでは抑制関係も考慮した真の「影響力」を測定していると考えられる。\n\nネットワークモデルは，推定法もいろいろ工夫されているし，動的なモデル，時系列モデルなども扱える。今後の発展が楽しみな分析方法の一つと言えるだろう。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "chapter15.html#課題",
    "href": "chapter15.html#課題",
    "title": "15  多変量解析(その2)",
    "section": "15.4 課題",
    "text": "15.4 課題\nこの章で紹介した多変量解析手法（クラスター分析，多次元尺度構成法，ネットワーク分析）を使って，以下の課題に取り組んでみよう。\n\n15.4.1 課題1: クラスター分析による消費者セグメンテーション\n100人の消費者の購買行動データ（5つの商品カテゴリへの支出額）を用いて，階層的クラスター分析とk-means法の両方でクラスター分析を実行し，結果を比較せよ。画面には一部しか表示しておらず，全体データはこちらconsumer_data.csvからダウンロード可能です。\n\n\n   消費者ID      食品    衣料品      書籍  電子機器      旅行\n1      C001  7.159287 2.6886403 0.7544844 1.4639778 2.8421412\n2      C002  7.654734 2.5539177 0.1000000 0.8720764 1.6853630\n3      C003 10.338062 1.9380883 1.5028693 0.1656465 0.1134270\n4      C004  8.105763 1.6940373 0.6453996 1.1958188 0.1823988\n5      C005  8.193932 1.6195290 0.6559957 2.2351973 0.1000000\n6      C006 10.572597 1.3052930 1.5127857 1.0397224 1.3629122\n7      C007  8.691374 1.7920827 0.8576135 1.9863715 0.2458933\n8      C008  6.102408 0.7346036 0.3896411 0.2056938 2.8255001\n9      C009  6.969721 4.1689560 1.0906517 1.4555504 4.5201307\n10     C010  7.331507 3.2079620 0.9305543 1.9155258 0.4555634\n\n\n分析手順: 1. データを読み込み，標準化を行う 2. 階層的クラスター分析（ウォード法）でデンドログラムを作成 3. k-means法で3クラスターに分類 4. 両手法の結果を比較し，各クラスターの特徴を解釈する\n\n\n15.4.2 課題2: 多次元尺度構成法による都市間類似性の可視化\n日本の主要10都市間の類似度評定データを用いて，多次元尺度構成法で都市の関係性を2次元平面上に可視化せよ。データはこちらcity_similarity.csvからダウンロード可能です。\n\n\n       東京 大阪 名古屋 札幌 仙台\n東京     10    6      7    2    5\n大阪      6   10      6    2    3\n名古屋    7    6     10    2    4\n札幌      2    2      2   10    7\n仙台      5    3      4    7   10\n\n\n分析手順: 1. 類似度データを距離行列に変換する 2. 古典的MDSで2次元解を求める 3. ggplotとggrepelで結果を可視化する 4. 都市の配置パターンから地理的・文化的特徴を考察する\n\n\n15.4.3 課題3: ネットワーク分析による心理尺度の構造解析\n性格特性の下位尺度間の関係をネットワーク分析で可視化し，中心性指標を用いて重要な特性を特定せよ。データはこちらpersonality_data.csvからダウンロード可能です。\n\n\n   参加者ID 外向性1 外向性2 協調性1 協調性2 誠実性1 誠実性2 神経症傾向1\n1      P001       2       2       4       4       2       2           1\n2      P002       5       5       2       3       3       2           5\n3      P003       5       5       4       4       4       5           2\n4      P004       5       4       2       2       6       5           2\n5      P005       3       6       4       5       6       7           3\n6      P006       3       4       3       4       5       5           6\n7      P007       1       2       5       5       6       6           1\n8      P008       5       7       4       4       7       7           7\n9      P009       2       1       3       4       7       7           1\n10     P010       4       6       4       5       3       2           4\n   神経症傾向2 開放性1 開放性2\n1            1       1       1\n2            3       3       4\n3            2       1       3\n4            2       3       4\n5            6       3       4\n6            4       5       5\n7            4       4       5\n8            7       5       3\n9            4       4       5\n10           4       2       1\n\n\n分析手順: 1. データを読み込み，下位尺度間の偏相関行列を計算する 2. glasso法でスパースなネットワークを推定する 3. 中心性指標（強度，近接性，媒介性）を算出する 4. 最も中心的な特性と周辺的な特性を特定し，性格構造について考察する\n\n\n\n\nShojima, Kojiro. 2022. Test Data Engineering: Latent Rank Analysis, Biclustering, and Bayesian Network. Springer.\n\n\nアデラ＝マリアイスヴォラヌ, サシャエプスカンプ, ローレンスウォルドープ, and デニーボースブーム. (2022) 2024. 心理ネットワークアプローチ入門:行動科学者と社会科学者のためのガイド. Translated by 樫原潤 and 小杉考司. 勁草書房.\n\n\nグリムL. G., and ヤーノルドP. R. (1994) 2016. 研究論文を読み解くための多変量解析入門 基礎篇: 重回帰分析からメタ分析まで. Translated by 小杉考司, 高田菜美, and 山根嵩史. 北大路書房.\n\n\n高根芳雄. 1980. 多次元尺度法. 東京大学出版会.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "chapter15.html#footnotes",
    "href": "chapter15.html#footnotes",
    "title": "15  多変量解析(その2)",
    "section": "",
    "text": "バグがあるのになぜ修正しないのか，と思われるかもしれない。ward.DはRのバージョン3.0.3以前まで用いられていたが，ユーザからの指摘で正しく分散が計算されていないことが発覚。Rの基本関数に誤りがあったことを認め，戒めとするために元のコードを残している。Rはフリーでオープンなソフトウェアであり，無償ではあるが，フリーだからといっていけないのではなく，こうした自浄作用があることを示すための措置と思われる。ちなみに筆者が以前プロプライエタリな統計ソフトを使っていた時にもおかしな挙動を発見したことがある。メーカに指摘すると「次のバージョンでは修正されているのでそちらを新たに購入しろ」という回答であったため，そのソフトウェアから決別することにした。科学的営みにおいて，有償サポートがあることが真実を担保しない例である。↩︎\n自己組織化マップ（SOM）とは，Kohonen Mapとも呼ばれる。フィンランドのKohonenによって開発された教師なし学習アルゴリズムの一種である。高次元データを2次元の格子上にマッピングし，データの構造を保持しながら可視化する手法である。↩︎\nトークンとは，機械学習においてテキストを構成する最小の意味単位であり，必ずしも形態素に限らず類似性をもとに区切られた数値ベクトルである。↩︎\n　RMecabのGithubサイトhttps://github.com/IshidaMotohiro/RMeCab↩︎\n　Mecabの公式サイトhttp://taku910.github.io/mecab/↩︎\n　RMecabのGithubサイトhttps://github.com/IshidaMotohiro/RMeCab↩︎\n　gibasaの解説https://zenn.dev/paithiov909/articles/gibasa-intro↩︎",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "practices.html",
    "href": "practices.html",
    "title": "17  演習問題",
    "section": "",
    "text": "17.1 最終課題\nこのコースでは，確率的に生じるデータを意識しながら，そのメカニズムから生成されるデータを乱数によって具体化し，そうしたサンプルデータに基づいて検定と分析のロジックを学んできました。\n検定には，サンプルサイズ，有意水準，検出力，効果量が相互に関わっており，確率的判断がこれらの関数としてどのように現れるかを確認しました。\nまた仮想データをつくれるところから，QRPsのシミュレーションを行ったり，例数設計が行えることも見てきました。 線型モデルやそのほか発展的なモデルについても，データ生成メカニズムの観点からアプローチできます。\nまた，一般線型モデル，一般化線型モデル，一般化線型混合モデル，階層線型モデルと，線型モデルの展開について見てきました。\nいずれのモデルも，データによりフィットした，より適切なモデルにするための工夫です。もちろん複雑なモデルであればあるほど良い，というわけではありません。むしろモデルは単純な方がいいのですが，単純だからという理由でデータに合わないモデルを適用するのは，適切ではありません。\nモデルが複雑になるにつれて，推定方法も最小二乗法，最尤法，ベイズ法と展開してきました。いずれも同じ母数を推定するための手法ですから，ここに優劣はなく，最終的な結論も同じになるべきです。確率の解釈の違いなどもありますので，主義主張の対立と思われることもありますが，ユーザとしては実用面での有用性から選ぶということでも良いかもしれません。\nより発展的な内容として，確率的プログラミング言語を用いれば，データ生成メカニズムを記述することでモデルパラメータの推定が可能になルコとにも触れました。\n確率的プログラミング言語を利用するには，プログラミングの技術，ベイズ統計の理論，MCMCによる近似の理論と方法についての知識が必要です。ですが，これらの技術を身につけると，分析の可能性はもっと広がります。既存の統計モデルを当てはめるのではなく，あなた自身の考えたモデルを作り上げることもできます。その自由度は無限ともいえるほどで，想像力を働かせて分析モデルをデザインするのは大変楽しいことでもあります。また，そういう観点を得ることができれば，従来の統計モデルがどういう設計をされているのかについても理解が進みます。\nMCMCは確率を乱数で具現化する手法です。MCMCに限らず，確率を乱数で具現化し，具体的なデータとして分析することで，統計モデルを使いこなせるようになりましょう。ツールは使うべきもので，使われるべきものではないからです。",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>演習問題</span>"
    ]
  },
  {
    "objectID": "practices.html#最終課題",
    "href": "practices.html#最終課題",
    "title": "17  演習問題",
    "section": "",
    "text": "無相関検定において，真の状態が母相関\\(\\rho=0.4\\)であったときに，サンプルサイズ\\(n=20\\)のデータをとって検定を行うとします。この時の帰無仮説の分布と，真の状態の分布を重ねて図示し，\\(\\alpha=0.05\\)の臨界値，検出力を可視化する図を描くコードを書いてください。(参考；南風原 (2002) ,Pp.144)\n2要因Betweenデザインの分散分析において，交互作用のみ有意になるようなサンプルデータを作るコードを書いてください。また，サンプルデータが正しくできているかどうかを確認するために，anovakunでの分析結果も出力させてください。\n2つの変数X,Yをもつ3つの群があり，群ごとX,Yの相関を見るとすべて\\(r = -0.3\\)程度の負の相関を持っているが，3つの群をあわせてX,Yの相関を見ると正の相関を示すようなデータセットを作るコードを書いてください。なお，出来上がったデータは群ごとに色分けした散布図で図示するようにしてください。\n\nヒント：群ごとに回帰分析のサンプルデータを作ることを考え，傾きは一貫して\\(\\beta_1=-0.3\\)であるのに対し，群ごとの切片\\(\\beta_0\\)を適当に調整すると良いでしょう。\nねらい：このようなデータは，相関を見るときに可視化することの重要性を伝えるとともに，階層線型モデルの必要性を理解することに役立ちます。\n\nbrmsパッケージを用いて，ベイズ統計の観点から重回帰分析を実行してください。従属変数を自由に設定し，複数の説明変数を含むモデルを構築してください。事前分布の設定，MCMCの収束診断，事後分布の可視化，および最尤推定との比較を含めた包括的な分析を行ってください。データは何を用いても構いません。\n\nヒント：brm()関数でpriorオプションを使って事前分布を明示的に設定し，plot()とpp_check()を用いて結果を評価してください。サンプルデータとしてはmtcarsやiris，psych::bfiなどが利用できます。\nねらい：ベイズ統計の実践的な応用と，頻度主義統計との違いを理解することを目的とします。\n\n探索的因子分析と確認的因子分析の比較分析を行ってください。同一データセットに対して，まず探索的因子分析（EFA）を実行して因子構造を探索し，次にその結果を基に確認的因子分析（CFA）モデルを構築してください。両手法の結果を比較し，それぞれの利点と限界について考察してください。データは何を用いても構いません。\n\nヒント：psychパッケージのfa()関数でEFAを実行し，lavaanパッケージのcfa()関数でCFAを実行してください。適合度指標の比較も含めてください。サンプルデータとしてはpsych::bfi（ビッグファイブ性格検査），psych::ability（認知能力検査），HolzingerSwineford1939（Holzinger & Swinefordの認知テスト）などが利用できます。\nねらい：測定モデルの探索と検証の違い，およびそれぞれの統計的手法の特徴を理解することを目的とします。\n\n階層線型モデル（HLM）またはGLMMを用いて，ネストされたデータ構造を持つ分析を実行してください。個人がグループにネストされている状況を想定し，ランダム切片モデルとランダム傾きモデルの両方を比較してください。また，ICCを計算してグループレベルの効果の大きさを評価してください。データは何を用いても構いません。\n\nヒント：brmsパッケージの(1|group)（ランダム切片）と(1+x|group)（ランダム切片ランダム傾き）の記法を使い分けてください。サンプルデータとしてはlme4::sleepstudy（睡眠研究データ），nlme::Orthodont（歯科矯正データ），mlmRev::Exam（学校別試験成績データ），mlmRev::Exam（機械データ）などが利用できます。\nねらい：階層構造を持つデータの適切な分析手法を理解し，個人レベルとグループレベルの変動を区別する重要性を学ぶことを目的とします。\n\n\n\n\n\n\n南風原朝和. 2002. 心理統計学の基礎: 統合的理解のために. 有斐閣. http://amazon.co.jp/o/ASIN/4641121605/.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>演習問題</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bernaards, Coen A., and Robert I. Jennrich. 2005. “Gradient\nProjection Algorithms and Software for Arbitrary Rotation Criteria in\nFactor Analysis.” Educational and Psychological\nMeasurement 65: 676–96. https://doi.org/10.1177/0013164404272507.\n\n\nGabry, Jonah, Rok Češnovar, and Andrew Johnson. 2023. Cmdstanr: R\nInterface to ’CmdStan’.\n\n\nHadley, Wickham. 2014. “Tidy Data.” Journal of\nStatistical Software 59: 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nHox, J. 2002. Multilevel Analysis: Techniques and Applications.\nLawrence Erlbaum Associates.\n\n\nJASP Team. 2025. “JASP (Version\n0.95.1)[Computer software].” https://jasp-stats.org/.\n\n\nKruschke, John K. 2018. “Rejecting or Accepting Parameter Values\nin Bayesian Estimation.” Advances in Methods and Practices in\nPsychological Science 1 (2): 270–80.\n\n\nMakowski, Dominique, Mattan S. Ben-Shachar, and Daniel Lüdecke. 2019.\n“: Describing Effects and Their Uncertainty, Existence and\nSignificance Within the Bayesian Framework.” Journal of Open\nSource Software 4 (40): 1541. https://doi.org/10.21105/joss.01541.\n\n\nRaudenbush, S.W., and Liu.X. 2000. “Statistical Power and Optimal\nDesign for Multisite Randomnized Trials.” Psychological\nMethods 5: 199–213.\n\n\nRevelle, William. 2021. Psych: Procedures for Psychological,\nPsychometric, and Personality Research. Evanston, Illinois:\nNorthwestern University. https://CRAN.R-project.org/package=psych.\n\n\nRinker, Tyler W., and Dason Kurkiewicz. 2018. pacman: Package Management for\nR. Buffalo, New York. http://github.com/trinker/pacman.\n\n\nRosseel, Yves. 2012. “lavaan: An\nR Package for Structural Equation Modeling.”\nJournal of Statistical Software 48 (2): 1–36. https://doi.org/10.18637/jss.v048.i02.\n\n\nShojima, Kojiro. 2022. Test Data Engineering: Latent Rank Analysis,\nBiclustering, and Bayesian Network. Springer.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nZeileis, Achim. 2005. “CRAN Task Views.” R\nNews 5 (1): 39–40. https://CRAN.R-project.org/doc/Rnews/.\n\n\nアデラ＝マリアイスヴォラヌ, サシャエプスカンプ, ローレンスウォルドープ,\nand デニーボースブーム. (2022) 2024.\n心理ネットワークアプローチ入門:行動科学者と社会科学者のためのガイド.\nTranslated by 樫原潤 and 小杉考司. 勁草書房.\n\n\nキーラン・ヒーリー. (2018) 2021.\nデータ分析のためのデータ可視化入門. Translated by 瓜生真也,\n江口哲史, and 三村喬生. 講談社.\n\n\nグリムL. G., and ヤーノルドP. R. (1994) 2016.\n研究論文を読み解くための多変量解析入門 基礎篇:\n重回帰分析からメタ分析まで. Translated by 小杉考司, 高田菜美, and\n山根嵩史. 北大路書房.\n\n\nクルシュケJ. K. (2014) 2017. ベイズ統計モデリング: R, JAGS,\nStanによるチュートリアル 原著第2版. Translated by 前田和寛 and\n小杉考司. 共立出版.\n\n\nシ. 2016. 計算機言語のまとめノート. 暗黒通信団.\n\n\nシャロン・バーチュ・マグレイン. (2011) 2018.\n異端の統計学ベイズ. Translated by 冨永星. 草思社.\n\n\nランダー，J.P. (2017) 2018. みんなのr 第2版. Translated by\n高柳慎一, 津田真樹, 牧山幸史, 松村杏子, and 簑田高志. マイナビ出版.\n\n\nリーM.D, and ワゲンメーカーズE-J. (2013) 2017.\nベイズ統計で実践モデリング: 認知モデルのトレーニング.\nTranslated by 井関龍太.\n\n\n佐藤坦. 1994. はじめての確率論: 測度から確率へ. 共立出版.\n\n\n南風原朝和. 2002. 心理統計学の基礎: 統合的理解のために. 有斐閣.\nhttp://amazon.co.jp/o/ASIN/4641121605/.\n\n\n———. 2014. 心理統計学の基礎: 続・統合的理解のために. 有斐閣.\n\n\n吉田伸生. 2021. 確率の基礎から統計へ. 新装版. 日本評論社.\n\n\n吉田寿夫, and 村井潤一郎. 2021.\n“心理学的研究における重回帰分析の適用に関わる諸問題.”\n心理学研究 92 (3): 178–87. https://doi.org/10.4992/jjpsy.92.19226.\n\n\n宮川雅巳. 1997. グラフィカルモデリング (統計ライブラリー).\n朝倉書店.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房.\nhttp://ci.nii.ac.jp/ncid/BB27527420.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023.\n数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計.\n技術評論社.\n\n\n岡太彬訓, and 今泉忠. 1994. パソコン多次元尺度構成法. 共立出版.\n\n\n平岡和幸, and 堀玄. 2009. プログラミングのための確率統計.\nオーム社. http://amazon.co.jp/o/ASIN/4274067750/.\n\n\n新納浩幸. 2007. Rで学ぶクラスタ解析. オーム社.\n\n\n松村優哉, 湯谷啓明, 紀ノ定保礼, and 前田和寛. 2021. 改訂2版\nRユーザのためのRStudio[実践]入門:\nTidyverseによるモダンな分析フローの世界. 技術評論社.\n\n\n株式会社ホクソエム, trans. (2016) 2017. Rプログラミング本格入門:\n達人データサイエンティストへの道. 単行本. 共立出版.\n\n\n永田靖, and 吉田道弘. 1997. 統計的多重比較法の基礎.\nサイエンティスト社.\n\n\n池田功毅, and 平石界. 2016.\n“心理学における再現可能性危機：問題の構造と解決策.”\n心理学評論 59 (1): 3–14. https://doi.org/10.24602/sjpr.59.1_3.\n\n\n河野敬雄. 1999. 確率概論. 京都大学学術出版会.\n\n\n浜田宏. 2018. その問題、数理モデルが解決します. ベレ出版. http://amazon.co.jp/o/ASIN/4860645685/.\n\n\n———. 2020. その問題、やっぱり数理モデルが解決します. ベレ出版.\n\n\n浜田宏, 石田淳, and 清水裕士. 2019.\n社会科学のためのベイズ統計モデリング. 朝倉書店. http://amazon.co.jp/o/ASIN/4254128428/.\n\n\n石田基広, 市川太祐, 高柳慎一, and 福島真太朗, trans. (2015) 2016.\nR言語徹底解説. 共立出版.\n\n\n総務省. 2020.\n“統計表における機械判別可能なデータ作成に関する表記方法.”\n統計企画会議申し合わせ. https://www.soumu.go.jp/main_content/000723697.pdf.\n\n\n西内啓. 2017. 統計学が最強の学問である[数学編]:\nデータ分析と機械学習のための新しい教科書. ダイヤモンド社.\n\n\n西里静彦. 2010.\n行動科学のためのデータ解析–情報把握に適した方法の利用. 培風館.\n\n\n豊田秀樹. 2009. 検定力分析入門: Rで学ぶ最新データ解析ー.\n東京図書.\n\n\n———. 2017. もうひとつの重回帰分析. 東京図書.\n\n\n———. 2020. 瀕死の統計学を救え！. 朝倉書店.\n\n\n足立浩平. 2006. 多変量データ解析法:\n心理・教育・社会系のための入門. ナカニシヤ出版.\n\n\n高根芳雄. 1980. 多次元尺度法. 東京大学出版会.\n\n\n高橋康介. 2018. 再現可能性のすゝめ. Edited by 石田基広. Vol. 3.\nWonderful r. 共立出版.",
    "crumbs": [
      "References"
    ]
  }
]