[
  {
    "objectID": "chapter13.html",
    "href": "chapter13.html",
    "title": "13  線形モデルの展開",
    "section": "",
    "text": "13.1 一般線形モデル(General Linear Model)\nこの章では，線形モデルの展開を説明する。線形モデルは，説明変数と目的変数の関係を線形すなわち一次式で表現するモデルである。 線形モデルは，一般化線形モデル(GLM)，一般化線形混合モデル(GLMM)，階層線形モデル(HLM)と拡張していくが，まずは一般線形モデル(LM)についてみておこう。\n線形モデルの基本は回帰分析モデルである。単回帰モデルは次の式で表される。\n\\[\ny_i = \\beta_0 + \\beta_1 x + e_i\n\\]\nここで，\\(y_i\\)は\\(i\\)番目の観測値，\\(\\beta_0\\)は切片，\\(\\beta_1\\)は回帰係数，\\(e_i\\)は誤差項である。 この式を拡張したのが重回帰分析で，次の式で表される。\n\\[\ny_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + e_i\n\\]\nここで，\\(x_1, x_2, \\cdots, x_p\\)は説明変数，\\(\\beta_1, \\beta_2, \\cdots, \\beta_p\\)は回帰係数である。\nこの式をベクトルと行列で表現すると，次のようになる。\n\\[\ny = X\\beta + e\n\\]\nここで，\\(y\\)は\\(n\\)次元のベクトル，\\(X\\)は\\(n \\times p\\)の計画行列(design matrix)，\\(\\beta\\)は\\(p\\)次元のベクトル，\\(e\\)は\\(n\\)次元のベクトルである。\n計画行列は，説明変数のデータをまとめた行列であり，次のようになる。\n\\[\nX = \\begin{pmatrix}\n1 & x_1 & x_2 & \\cdots & x_p\\\\\n1 & x_1 & x_2 & \\cdots & x_p\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_1 & x_2 & \\cdots & x_p\n\\end{pmatrix}\n\\]\n第一列目にあるのは切片にかかる係数であり，第二列目以降にあるのは説明変数にかかる係数である。 ここに係数ベクトル\\(\\beta= (\\beta_0, \\beta_1, \\beta_2, \\cdots, \\beta_p)^T\\)をかけることで，説明変数の線形結合が得られる。\nこの係数ベクトルにおいて，\\(x\\)の値がバイナリ(0/1)の場合，その説明変数はダミー変数と呼ばれる。線型回帰モデルにおいて説明変数\\(X\\)がダミー変数である場合は，横軸が2つの値しかとらないため散布図を描くと奇妙な形になることがわかるだろう。\nこの散布図に対して回帰直線を引けば，2群の平均値を通る直線が引かれる。回帰分析においては，予測式\\(\\hat{y}\\)を中心とした正規分布に従って誤差が生じると仮定されているのであった。ダミーデータの場合も，両群の平均値を中心に正規分布に従って誤差が生じると仮定したことになる。これはいわゆる2群の平均値の差を検定する時の仮定と同じであり，このことから平均値差の検定は説明変数が名義尺度水準の変数である回帰分析と同じ(一般)であることがわかる。回帰分析と平均値差の検定は数学的には同じ式で表現できるから，これを総称して一般線型モデルGeneral Linear model という。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線形モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html#一般化線形モデルglm",
    "href": "chapter13.html#一般化線形モデルglm",
    "title": "13  線形モデルの展開",
    "section": "13.2 一般化線形モデル(GLM)",
    "text": "13.2 一般化線形モデル(GLM)\nかつて心理学実践においては，要因計画法と帰無仮説検定の組み合わせによって研究を行うことが主流であった。平均値差の検定に落とし込むように研究をデザインし，帰無仮説検定でYESかNOか決着がつく。帰無仮説検定はデータの特性や生成メカニズムに依存せず，統計パッケージを用いれば誰にでも扱うことができ，\\(p\\)値という手垢のついてない数値だけで判断できると考えられてきた。\nこのことに関する問題については置くとして，ともかく「正規分布の平均値の差」に持ち込めさえすれば良い，ということが当時の研究者の共通認識であった。そのため，例えば比率のデータやカウントデータなど，正規分布を想定することができないデータに対しても，対数変換・角変換などを行って分布を正規分布のそれに近づけ，検定を行うということが行われていた。\n正規分布は理論的に\\(\\pm \\infty\\)の範囲を取るのに対し，比率のデータは0から1の範囲にしかとらない。カウントデータは正の整数しかとらないデータであるから，こうしたデータに対して一般線型モデルをあてがうのは重大な仮定違反である1。\nこのような背景から，正規分布以外の確率分布を用いた統計モデルが考えられた。これが一般化線形モデル(Generalized Linear Model, GLM)である2。\n確率分布の多くは，位置パラメータとスケールパラメータを持つ。統計モデルは平均的な挙動について考えるモデルだから，位置パラメータに線形モデルが当てがえるように，数式を変形してやれば良い。以下に例として，ベルヌーイ分布とポアソン分布をGLMで表現したものを考えてみよう。\n\n13.2.1 ベルヌーイ分布に対する線形モデル；ロジスティック回帰分析\nベルヌーイ分布はコイントスの表/裏のような2値をとるデータに対して用いられる。ベルヌーイ分布の確率関数は次のように表される。\n\\[\nP(Y = y) = p^y (1-p)^{1-y}\n\\]\nここで，\\(p\\)は成功確率であり，0から1の範囲の値を取る。 こうしたデータは現実にも少なくない。生死，病気の有無，テストの正答/誤答のようなデータが代表的である。こうしたデータを目的変数にして，直線回帰を行うと当然おかしなことが生じる。すなわち，予測値が0と1の範囲を超えることがありえるからである。\nそこで，線形予測子と確率を適切に結びつけるリンク関数を用いる。ロジスティック回帰ではロジット関数（logit function）をリンク関数として使用する：\n\\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x\n\\]\nこの関係を\\(p\\)について解いてみよう。\\(\\eta = \\beta_0 + \\beta_1 x\\)とおくと：\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\eta\n\\]\n両辺の指数を取ると：\n\\[\n\\frac{p}{1-p} = e^{\\eta}\n\\]\n両辺に\\((1-p)\\)をかけると：\n\\[\np = (1-p) \\cdot e^{\\eta}\n\\]\n展開すると：\n\\[\np = e^{\\eta} - p \\cdot e^{\\eta}\n\\]\n\\(p\\)について整理すると：\n\\[\np + p \\cdot e^{\\eta} = e^{\\eta}\n\\]\n\\[\np(1 + e^{\\eta}) = e^{\\eta}\n\\]\n\\[\np = \\frac{e^{\\eta}}{1 + e^{\\eta}}\n\\]\n分子分母を\\(e^{\\eta}\\)で割ると：\n\\[\np = \\frac{1}{e^{-\\eta} + 1} = \\frac{1}{1 + e^{-\\eta}}\n\\]\n\\(\\eta = \\beta_0 + \\beta_1 x\\)を代入すると，ロジスティック関数（逆リンク関数）が得られる：\n\\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\n\\]\nこのようにして得られた確率を用いて，ベルヌーイ分布の確率関数を次のように表現できる。\n\\[ y \\sim \\text{Bernoulli}(p) \\]\nサンプルデータを作って，線形回帰モデルとロジスティック回帰モデルの予測値を比較してみよう。\n\npacman::p_load(tidyverse)\npacman::p_load(patchwork)\n\n# データ生成\nset.seed(17)\nn &lt;- 200\nx &lt;- runif(n, min = -10, max = 10)\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\np &lt;- beta_0 + x * beta_1\nprob &lt;- 1/(1+exp(-p))\ny &lt;- rbinom(n, size = 1, prob = prob)\n\ndf_logistic &lt;- data.frame(x = x, y = y)\n\n# p1: 線形回帰\np1 &lt;- ggplot(df_logistic, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"線形回帰\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ロジスティック回帰\np2 &lt;- ggplot(df_logistic, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"ロジスティック回帰\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# パッチワークで結合\np1 + p2\n\n\n\n\n\n\n\n\n線形回帰の予測値が不適切な範囲にまで延伸するのに対し，ロジスティック回帰はうまく適合していることがわかるだろう。\nデータからロジスティック回帰モデルを推定するには，glm関数を用いることができるが，ここではすでに学んだbrmsパッケージによるベイズ推定でアプローチしてみよう。brm関数の書き方は，glm関数の書き方とほぼ同じであり，推定法をMLからベイズに変えることできる。\n\npacman::p_load(brms)\nresult.bayes.logistic &lt;- brm(\n    y ~ x,\n    family = bernoulli(),\n    data = df_logistic,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nsummary(result.bayes.logistic)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: y ~ x \n   Data: df_logistic (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.88      0.41     0.10     1.70 1.00     2314     2331\nx             1.35      0.26     0.91     1.92 1.00     2164     2080\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.logistic)\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\nresult.ml &lt;- glm(y ~ x, family = binomial(), data = df_logistic)\nsummary(result.ml)\n\n\nCall:\nglm(formula = y ~ x, family = binomial(), data = df_logistic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.8673     0.4058   2.137   0.0326 *  \nx             1.2661     0.2413   5.248 1.54e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.869  on 199  degrees of freedom\nResidual deviance:  45.112  on 198  degrees of freedom\nAIC: 49.112\n\nNumber of Fisher Scoring iterations: 8\n\n\n最尤推定の結果とベイズ推定の結果に多少のズレが見られるが，これはサンプルサイズの小ささによるものである。アウトプット変数が2値しか持たないため，分散がどうしても小さくなりがちであり，正確な推定値を得るためにはより多くのデータが必要である。\nまた，回帰係数の解釈には注意が必要である。普通の回帰分析であれば，説明変数が1単位変わると目的変数がどれだけ変わるかを表すが，ロジスティック回帰ではこのような直接的な解釈はできない。ロジスティック関数によって変換されたものが意味を持つからである。\n線形モデルが表すのは次の関係なのであった。 \\[ \\beta_0 + \\beta_1 x = \\log \\frac{p}{1-p} \\]\nこの\\(\\log \\frac{p}{1-p}\\)はロジット(logit)と呼ばれる。ロジスティック回帰では，確率\\(p\\)をロジットに変換することで線形関係を表現している。逆に言えば，線形モデルで表現されているのはログを取った確率の比であり，この比が説明変数の線形関係によって決まるということである。であるから，結果を解釈するには係数を指数関数\\(e\\)を取り，確率の比として理解する必要がある。\n今回のデータでは説明変数の係数が1.36と推定されたから，\\(e^{1.36} = 3.89\\)である。これは，説明変数が1単位増加すると，成功確率が3.89倍になる，ということを意味する。\n\n\n13.2.2 ポアソン分布に対する線形モデル；ポアソン回帰\n今度はカウント変数に対するモデルを考えてみよう。カウント変数は正の整数をとるデータであり，ポアソン分布を用いることができる。ポアソン分布の確率関数は次のように表される。\n\\[\nP(Y = y) = \\frac{\\lambda^y e^{-\\lambda}}{y!}\n\\]\nここで，\\(\\lambda\\)は平均である。ポアソン分布の形状も確認しておこう。\n\n\n\n\n\n\n\n\n\nこのような正の整数しかとらないデータに対してはポアソン分布で回帰した方がよい。ポアソン回帰では対数関数をリンク関数として使用する：\n\\[\n\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_i\n\\]\nこの関係を\\(\\lambda_i\\)について解くと，指数関数（逆リンク関数）が得られる：\n\\[ \\lambda_i = \\exp(\\beta_0 + \\beta_1 x_i) \\]\nとして\n\\[ y_i \\sim \\text{Pois}(\\lambda_i) \\]\nを考えるのである。\n以下にサンプルデータを作ったポアソン回帰の例を示す。\n\n# データ生成\nset.seed(17)\nn &lt;- 200\nx &lt;- runif(n, min = 0, max = 10)\nbeta_0 &lt;- 0.5\nbeta_1 &lt;- 0.3\nlambda &lt;- exp(beta_0 + beta_1 * x)\ny &lt;- rpois(n, lambda = lambda)\n\ndf_pois &lt;- data.frame(x = x, y = y)\n\n# p1: 線形回帰（不適切な例）\np1 &lt;- ggplot(df_pois, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"カウント変数\", title = \"線形回帰（不適切）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ポアソン回帰（適切な例）\np2 &lt;- ggplot(df_pois, aes(x = x, y = y)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_smooth(method = \"glm\", method.args = list(family = \"poisson\"), se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"カウント変数\", title = \"ポアソン回帰（適切）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# パッチワークで結合\np1 + p2\n\n\n\n\n\n\n\n\n線形回帰では負の値の予測値が出てしまう可能性があるのに対し，ポアソン回帰は指数関数による変換によって適切にカウントデータの特性を捉えていることがわかる。\nポアソン回帰を実行するRコードの例を次に示す。\n\nresult.bayes.pois &lt;- brm(\n    y ~ x,\n    family = poisson(),\n    data = df_pois,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.2 seconds.\n\nsummary(result.bayes.pois)\n\n Family: poisson \n  Links: mu = log \nFormula: y ~ x \n   Data: df_pois (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.49      0.07     0.36     0.63 1.00     1275     1675\nx             0.30      0.01     0.29     0.32 1.00     1471     1943\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.pois)\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\nresult.ml.pois &lt;- glm(y ~ x, family = poisson(), data = df_pois)\nsummary(result.ml.pois)\n\n\nCall:\nglm(formula = y ~ x, family = poisson(), data = df_pois)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.495962   0.071166   6.969 3.19e-12 ***\nx           0.304829   0.009555  31.902  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1457.6  on 199  degrees of freedom\nResidual deviance:  214.3  on 198  degrees of freedom\nAIC: 975.04\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線形モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html#一般化線形混合モデルglmm",
    "href": "chapter13.html#一般化線形混合モデルglmm",
    "title": "13  線形モデルの展開",
    "section": "13.3 一般化線形混合モデル(GLMM)",
    "text": "13.3 一般化線形混合モデル(GLMM)\n線形モデルのさらなる拡張として，一般化線形混合モデル(GLMM)Generalized Linear Mixed Modelがある。ここまでの回帰モデルは，説明変数が目的変数に対して一貫した効果を持つと仮定してきた。この効果を特に固定効果fixed effectというが，GLMMでは固定効果に加えて変量効果random effectを考慮することができる。\n変量効果とは，説明変数が個体ごとに異なる値を持つことを意味する。たとえばWithinデザインの要因計画は個人差を考慮することができるが，これは変量効果を考慮したモデルであるといえる。すなわち，研究として見たい効果はWithin要因の水準ごとの固定効果であり，これとは別に個人の平均値が異なることを想定しているから，個人ごとの平均値を変量効果として考慮しているモデルといえる。\nこのように変量効果は個人ごとに異なる影響を考えることであるが，仮定としてこうした個人差が確率分布，特に正規分布に従っていると考えるのである。個人差は確率分布からランダムに生じるものであり，個人同士の平均的な違いは交換可能であると考えられる。またこうした個人差の分散は，個人の平均値の分散として捉えられる。このように個人差を表す確率分布も混ぜ込むので，混合(Mixed)モデルと呼ばれるのである。\nここでは個人差が平均値，すなわち切片が異なるものとして説明したが，傾きに対して個人差を考えることもできる。変量効果がどこにあるかによって，ランダム切片モデル，ランダム傾きモデル，ランダム切片ランダム傾きモデルなどと呼ばれることがある。\n\n13.3.1 ランダム切片モデル\nランダム切片モデルは，切片が個人ごとに異なるモデルである。切片が個人ごとに異なるということは，切片の個人差が正規分布に従うと考えることである。\n\\[\n\\beta_{0i} = \\beta_0 + u_{0i}\n\\]\nここで，\\(\\beta_0\\)は全体の切片，\\(u_{0i}\\)は個人\\(i\\)の切片の個人差である。個人差は正規分布に従うと考えるから，\n\\[\nu_{0i} \\sim N(0, \\sigma_u)\n\\]\nと表現できる。モデル全体としては\n\\[\ny_{ij} = (\\beta_0 + u_{0i}) + \\beta_1 x_{ij} + e_{ij}\n\\]\nとなる。ここで，\\(e_{ij}\\)は誤差項であり，個人\\(i\\)の\\(j\\)番目の観測値に対する誤差を表す。\n具体的なデータを作って見てみよう。\n\n# データ生成\nset.seed(17)\nn_person &lt;- 10  # 個人数\nn_obs &lt;- 20     # 各個人の観測数\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nsigma_u &lt;- 1    # 個人差の標準偏差\nsigma_e &lt;- 0.5  # 誤差の標準偏差\n\n# 個人ごとのランダム切片\nperson_intercepts &lt;- rnorm(n_person, mean = 0, sd = sigma_u)\n\n# データフレーム作成\ndf_random_intercept &lt;- expand_grid(\n  person = 1:n_person,\n  obs = 1:n_obs\n) %&gt;%\n  mutate(\n    x = runif(n(), min = 0, max = 10),\n    u_0 = person_intercepts[person],\n    y = beta_0 + u_0 + beta_1 * x + rnorm(n(), mean = 0, sd = sigma_e),\n    person_factor = factor(person)\n  )\n\n## データの確認\ndf_random_intercept %&gt;% head()\n\n# A tibble: 6 × 6\n  person   obs     x   u_0     y person_factor\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        \n1      1     1 8.81  -1.02 17.0  1            \n2      1     2 6.07  -1.02 11.5  1            \n3      1     3 7.40  -1.02 15.4  1            \n4      1     4 8.03  -1.02 16.9  1            \n5      1     5 9.02  -1.02 18.3  1            \n6      1     6 0.927 -1.02  2.00 1            \n\n# p1: 線形回帰（全体で一つの回帰線）\np1 &lt;- ggplot(df_random_intercept, aes(x = x, y = y)) +\n    geom_point(alpha = 0.5, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"線形回帰（固定効果のみ）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ランダム切片モデル（個人ごとに異なる切片の回帰線）\np2 &lt;- ggplot(df_random_intercept, aes(x = x, y = y, color = person_factor)) +\n    geom_point(alpha = 0.6, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"ランダム切片モデル\") +\n    theme(\n      text = element_text(family = \"IPAexGothic\"),\n      legend.position = \"none\"\n    )\n\np1 + p2\n\n\n\n\n\n\n\n\nランダム効果を含むモデルを推定するコード例を以下に示す。\n\nresult.bayes.random_intercept &lt;- brm(\n    y ~ x + (1 | person),\n    family = gaussian(),\n    data = df_random_intercept,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.8 seconds.\nChain 2 finished in 0.9 seconds.\nChain 3 finished in 0.9 seconds.\nChain 4 finished in 0.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.9 seconds.\nTotal execution time: 1.1 seconds.\n\nsummary(result.bayes.random_intercept)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x + (1 | person) \n   Data: df_random_intercept (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~person (Number of levels: 10) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.01      0.30     0.60     1.73 1.01      408      786\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.29      0.35     0.61     2.01 1.01      620      461\nx             1.98      0.01     1.96     2.01 1.00     2380     2052\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.53      0.03     0.48     0.59 1.00     1790     2245\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.random_intercept)\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\npacman::p_load(lmerTest)\nresult.ml.random_intercept &lt;- lmer(y ~ x + (1 | person), data = df_random_intercept)\nsummary(result.ml.random_intercept)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x + (1 | person)\n   Data: df_random_intercept\n\nREML criterion at convergence: 356.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3167 -0.5745 -0.1189  0.6343  2.6464 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n person   (Intercept) 0.7462   0.8638  \n Residual             0.2773   0.5266  \nNumber of obs: 200, groups:  person, 10\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   1.26699    0.28414  10.15076   4.459  0.00117 ** \nx             1.98396    0.01361 189.35597 145.774  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.242\n\n## さらに比較のために，普通の回帰分析も行ってみる\nresult.ml.random_intercept.ordinal &lt;- lm(y ~ x, data = df_random_intercept)\nsummary(result.ml.random_intercept.ordinal)\n\n\nCall:\nlm(formula = y ~ x, data = df_random_intercept)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.27275 -0.59838  0.08193  0.50855  2.67596 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.31764    0.14235   9.256   &lt;2e-16 ***\nx            1.97394    0.02464  80.124   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9771 on 198 degrees of freedom\nMultiple R-squared:  0.9701,    Adjusted R-squared:  0.9699 \nF-statistic:  6420 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n変量効果のモデル表記は，固定効果に加えて(1 | person)のように表記する。ここで，1は切片を表し，personは個人を表す。このようにすることで，個人ごとに異なる切片を考慮することができる。\n設定したパラメタ（個人差の標準偏差\\(\\sigma_u\\) = 1，残差の標準偏差\\(\\sigma_e\\) = 0.5）に対して，ベイズ推定では個人差の標準偏差が1.012，残差の標準偏差が0.53と推定されている。ML推定では個人差の標準偏差が0.864，残差の標準偏差が0.527と推定された。両手法ともに設定値に近い値が得られており，適切にパラメタが推定されていることがわかる。\nこのデータに対して，固定効果のみで推定すると切片が1.318，傾きが1.974と推定される。傾きに対しては同じ値になるが切片の変動を考慮するかどうかで推定値が異なっていることがわかる。固定効果のみのモデルの場合，切片に想定された正規分布の平均値のみ推定したことになっている。実践的な意味としては，Within計画をBetween計画と皆して推定しているようなものであり，せっかく綺麗に取り分けられる個人差分散を無視しているようなものである。\n\n13.3.1.1 個人レベルの推定値の取得\nベイズ推定の利点の一つは，個人ごとの推定値とその不確実性を定量化できることである。MCMCサンプルから個人の変量効果を取り出し，事後分布を可視化してみよう。\nbrmsパッケージの関数ranefを使うと面倒なことをしなくても直接取り出してくれるが，MCMCサンプルを取り出して加工して確認することは，MCMCによる推定の理解を深めるのに良い訓練となる。\n\n# 個人ごとの変量効果を取得して表示してみる\nrandom_effects &lt;- ranef(result.bayes.random_intercept)\nprint(random_effects)\n\n$person\n, , Intercept\n\n     Estimate Est.Error       Q2.5      Q97.5\n1  -1.2842574 0.3604082 -2.0328445 -0.6093264\n2  -0.2815002 0.3612608 -1.0005500  0.4143775\n3  -0.4328320 0.3614323 -1.1734005  0.2534434\n4  -1.0881607 0.3642191 -1.8473405 -0.4056347\n5   0.5388313 0.3636371 -0.2012679  1.2376413\n6  -0.2689177 0.3633187 -1.0316235  0.4175829\n7   0.5917789 0.3634518 -0.1563293  1.3063205\n8   1.6501281 0.3597518  0.9219949  2.3568723\n9   0.1666085 0.3624218 -0.5813912  0.8600216\n10  0.2276495 0.3628401 -0.5134040  0.9184448\n\n# 個人の切片の変量効果の事後分布(MCMCサンプル)を取得\nposterior_samples &lt;- as_draws_df(result.bayes.random_intercept) %&gt;% \n  select(starts_with(\"r_person\")) %&gt;% \n  rowid_to_column(\"iter\") %&gt;% \n  pivot_longer(-iter) %&gt;% \n  mutate(person = str_extract(name,pattern=\"\\\\d+\")) %&gt;% \n  mutate(person = factor(person, levels = as.character(1:10))) %&gt;% \n  select(-name)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n# MCMCサンプルを使った要約\nposterior_samples %&gt;% \n  group_by(person) %&gt;% \n  summarise(\n    EAP = mean(value),\n    median = quantile(value, 0.5),\n    q025 = quantile(value, 0.025),\n    q975 = quantile(value, 0.975),\n    sd = sd(value),\n    .groups = \"drop\"\n  )\n\n# A tibble: 10 × 6\n   person    EAP median   q025   q975    sd\n   &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 1      -1.28  -1.28  -2.03  -0.609 0.360\n 2 2      -0.282 -0.273 -1.00   0.414 0.361\n 3 3      -0.433 -0.425 -1.17   0.253 0.361\n 4 4      -1.09  -1.08  -1.85  -0.406 0.364\n 5 5       0.539  0.545 -0.201  1.24  0.364\n 6 6      -0.269 -0.270 -1.03   0.418 0.363\n 7 7       0.592  0.593 -0.156  1.31  0.363\n 8 8       1.65   1.66   0.922  2.36  0.360\n 9 9       0.167  0.171 -0.581  0.860 0.362\n10 10      0.228  0.235 -0.513  0.918 0.363\n\n# 事後分布の描画\np1 &lt;- ggplot(posterior_samples, aes(x = value, fill = person)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ person, scales = \"free_y\") +\n  theme_minimal() +\n  labs(x = \"切片の変量効果\", y = \"事後密度\", title = \"個人ごとの切片変量効果の事後分布\") +\n  theme(text = element_text(family = \"IPAexGothic\"), legend.position = \"none\")\n\nprint(p1)\n\n\n\n\n\n\n\n\nここで抜き出された個人の効果は，全体平均からの偏差であり，実際の切片は固定効果＋変量効果の形で得られている。これについてもcoef関数あるいはMCMCサンプルを加工することで，より具体的にイメージしながら利用できるだろう。\n\n# 各個人の実際の切片（固定効果+変量効果）を取得\nindividual_coefs &lt;- coef(result.bayes.random_intercept)$person\n\n# 実際の切片の事後分布（固定効果+変量効果）\ntotal_intercept_samples &lt;- as_draws_df(result.bayes.random_intercept) %&gt;% \n  select(b_Intercept, starts_with(\"r_person\")) %&gt;% \n  rowid_to_column(\"iter\") %&gt;% \n  pivot_longer(-c(iter, b_Intercept)) %&gt;% \n  mutate(person = str_extract(name, pattern=\"\\\\d+\")) %&gt;% \n  mutate(person = factor(person, levels = as.character(1:10))) %&gt;% \n  mutate(total_intercept = b_Intercept + value) %&gt;% \n  select(iter, person, total_intercept)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n# 実際の切片の事後分布の描画\np2 &lt;- ggplot(total_intercept_samples, aes(x = total_intercept, fill = person)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ person, scales = \"free_y\") +\n  theme_minimal() +\n  labs(x = \"実際の切片（固定効果+変量効果）\", y = \"事後密度\", \n       title = \"個人ごとの実際の切片の事後分布\") +\n  theme(text = element_text(family = \"IPAexGothic\"), legend.position = \"none\")\n\nprint(p2)\n\n\n\n\n\n\n\n# 実際の切片の信頼区間\ntotal_intercept_summary &lt;- total_intercept_samples %&gt;% \n  group_by(person) %&gt;% \n  summarise(\n    EAP = mean(total_intercept),\n    median = quantile(total_intercept, 0.5),\n    q025 = quantile(total_intercept, 0.025),\n    q975 = quantile(total_intercept, 0.975),\n    sd = sd(total_intercept),\n    .groups = \"drop\"\n  )\n\nprint(total_intercept_summary)\n\n# A tibble: 10 × 6\n   person     EAP  median    q025  q975    sd\n   &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1      0.00382 0.00210 -0.284  0.279 0.146\n 2 2      1.01    1.01     0.746  1.27  0.135\n 3 3      0.855   0.857    0.597  1.10  0.128\n 4 4      0.200   0.197   -0.0805 0.484 0.143\n 5 5      1.83    1.83     1.55   2.11  0.142\n 6 6      1.02    1.02     0.753  1.29  0.136\n 7 7      1.88    1.88     1.61   2.14  0.137\n 8 8      2.94    2.94     2.67   3.21  0.137\n 9 9      1.45    1.45     1.18   1.73  0.139\n10 10     1.52    1.52     1.24   1.80  0.144\n\n\nこのように，ベイズ推定では個人ごとの推定値とその不確実性を詳細に分析することができる。\n\n\n\n13.3.2 ランダム傾きモデル\nランダム傾きモデルは，傾きが個人ごとに異なるモデルである。傾きが個人ごとに異なるということは，傾きの個人差が正規分布に従うと考えることである。\n\\[\n\\beta_{1i} = \\beta_1 + u_{1i}\n\\]\nここで，\\(\\beta_1\\)は全体の傾き，\\(u_{1i}\\)は個人\\(i\\)の傾きの個人差である。個人差は正規分布に従うと考えるから，\n\\[\nu_{1i} \\sim N(0, \\sigma_u)\n\\]\nと表現できる。モデル全体としては\n\\[\ny_{ij} = \\beta_0  + (\\beta_1 + u_{1i}) x_{ij} + e_{ij}\n\\]\nとなる。\nこれについても具体的なデータを作って見てみよう。\n\n# データ生成\nset.seed(17)\nn_person &lt;- 10  # 個人数\nn_obs &lt;- 20     # 各個人の観測数\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nsigma_u &lt;- 0.5  # 傾きの個人差の標準偏差\nsigma_e &lt;- 1.5  # 誤差の標準偏差\n\n# 個人ごとのランダム傾き\nperson_slopes &lt;- rnorm(n_person, mean = 0, sd = sigma_u)\n\n# データフレーム作成\ndf_random_slope &lt;- expand_grid(\n  person = 1:n_person,\n  obs = 1:n_obs\n) %&gt;%\n  mutate(\n    x = runif(n(), min = 0, max = 10),\n    u_1 = person_slopes[person],\n    y = beta_0 + (beta_1 + u_1) * x + rnorm(n(), mean = 0, sd = sigma_e),\n    person_factor = factor(person)\n  )\n\n## データの確認\ndf_random_slope %&gt;% head()\n\n# A tibble: 6 × 6\n  person   obs     x    u_1     y person_factor\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        \n1      1     1 8.81  -0.508 12.5  1            \n2      1     2 6.07  -0.508  8.12 1            \n3      1     3 7.40  -0.508 13.9  1            \n4      1     4 8.03  -0.508 15.5  1            \n5      1     5 9.02  -0.508 15.2  1            \n6      1     6 0.927 -0.508  2.88 1            \n\n# p1: 線形回帰（全体で一つの回帰線）\np1 &lt;- ggplot(df_random_slope, aes(x = x, y = y)) +\n    geom_point(alpha = 0.5, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"線形回帰（固定効果のみ）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ランダム傾きモデル（個人ごとに異なる傾きの回帰線）\np2 &lt;- ggplot(df_random_slope, aes(x = x, y = y, color = person_factor)) +\n    geom_point(alpha = 0.6, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"ランダム傾きモデル\") +\n    theme(\n      text = element_text(family = \"IPAexGothic\"),\n      legend.position = \"none\"\n    )\n\np1 + p2\n\n\n\n\n\n\n\n\nランダム傾きモデルでは，個人ごとに回帰直線の傾きが異なることがわかる。これは，説明変数の効果が個人によって異なることを表している。\nランダム傾きモデルを推定するコード例を以下に示す。\n\nresult.bayes.random_slope &lt;- brm(\n    y ~ x + (0 + x | person),\n    family = gaussian(),\n    data = df_random_slope,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.7 seconds.\nChain 3 finished in 0.6 seconds.\nChain 2 finished in 0.7 seconds.\nChain 4 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 0.9 seconds.\n\nsummary(result.bayes.random_slope)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x + (0 + x | person) \n   Data: df_random_slope (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~person (Number of levels: 10) \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(x)     0.51      0.15     0.30     0.86 1.00      527     1003\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.24      0.24     0.79     1.71 1.00     5674     3203\nx             2.04      0.16     1.73     2.36 1.00      605      743\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.60      0.08     1.45     1.76 1.00     1642     1684\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.random_slope)\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\nresult.ml.random_slope &lt;- lmer(y ~ x + (0 + x | person), data = df_random_slope)\nsummary(result.ml.random_slope)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x + (0 + x | person)\n   Data: df_random_slope\n\nREML criterion at convergence: 792.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2526 -0.6127 -0.0716  0.6858  2.6584 \n\nRandom effects:\n Groups   Name Variance Std.Dev.\n person   x    0.1894   0.4352  \n Residual      2.5139   1.5855  \nNumber of obs: 200, groups:  person, 10\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)   1.2415     0.2335 189.2290   5.317 2.96e-07 ***\nx             2.0451     0.1436  10.2643  14.240 4.31e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.250\n\n\n変量効果のモデル表記で，(0 + x | person)は傾きのみにランダム効果を考慮することを表す。0によって切片の固定効果を除き，xで傾きにランダム効果を指定している。それぞれの出力が，理論値のどこを反映したものであるか確認しておこう。\nまた出力結果はランダム切片モデルの時と同様に，関数あるいはMCMCサンプルから推定値やその分布情報を得ることもできる。自分なりに加工して出力結果を確認してみてほしい。\n\n\n13.3.3 ランダム切片ランダム傾きモデル\nランダム切片ランダム傾きモデルは，切片と傾きの両方が個人ごとに異なるモデルである。これは最も複雑な変量効果モデルで，個人ごとに異なる切片と傾きを同時に考慮する。\n\\[\n\\beta_{0i} = \\beta_0 + u_{0i}\n\\] \\[\n\\beta_{1i} = \\beta_1 + u_{1i}\n\\]\nここで，\\(u_{0i}\\)は個人\\(i\\)の切片の個人差，\\(u_{1i}\\)は個人\\(i\\)の傾きの個人差である。 このランダム効果はいずれも個人に係るものであるから相関することが仮定されるため，それぞれに正規分布を仮定するのではなく多変量正規分布から得られるものと仮定する。当然のことながら，この相関係数も推定対象である。\n\\[\n\\begin{pmatrix} u_{0i} \\\\ u_{1i} \\end{pmatrix} \\sim N\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{u0}^2 & \\sigma_{u01} \\\\ \\sigma_{u01} & \\sigma_{u1}^2 \\end{pmatrix} \\right)\n\\]\nモデル全体としては\n\\[\ny_{ij} = (\\beta_0 + u_{0i}) + (\\beta_1 + u_{1i}) x_{ij} + e_{ij}\n\\]\nとなる。\n\n# データ生成\nset.seed(17)\nn_person &lt;- 10  # 個人数\nn_obs &lt;- 20     # 各個人の観測数\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nsigma_u0 &lt;- 1.0  # 切片の個人差の標準偏差\nsigma_u1 &lt;- 0.5  # 傾きの個人差の標準偏差\nrho &lt;- 0.3       # 切片と傾きの相関\nsigma_e &lt;- 0.5   # 誤差の標準偏差\n\n# 切片と傾きの共分散行列\nSigma &lt;- matrix(c(sigma_u0^2, rho*sigma_u0*sigma_u1, \n                  rho*sigma_u0*sigma_u1, sigma_u1^2), \n                nrow = 2)\n\n# 個人ごとのランダム効果（切片と傾き）\nlibrary(MASS)\nrandom_effects &lt;- mvrnorm(n_person, mu = c(0, 0), Sigma = Sigma)\nperson_intercepts &lt;- random_effects[, 1]\nperson_slopes &lt;- random_effects[, 2]\n\n# データフレーム作成\ndf_random_both &lt;- expand_grid(\n  person = 1:n_person,\n  obs = 1:n_obs\n) %&gt;%\n  mutate(\n    x = runif(n(), min = 0, max = 10),\n    u_0 = person_intercepts[person],\n    u_1 = person_slopes[person],\n    y = (beta_0 + u_0) + (beta_1 + u_1) * x + rnorm(n(), mean = 0, sd = sigma_e),\n    person_factor = factor(person, levels = as.character(1:10))\n  )\n\n## データの確認\ndf_random_both %&gt;% head()\n\n# A tibble: 6 × 7\n  person   obs     x   u_0    u_1     y person_factor\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        \n1      1     1  7.52  1.12 -0.351 15.2  1            \n2      1     2  6.34  1.12 -0.351 12.8  1            \n3      1     3  2.48  1.12 -0.351  6.12 1            \n4      1     4  5.51  1.12 -0.351 11.3  1            \n5      1     5  2.35  1.12 -0.351  5.18 1            \n6      1     6  2.59  1.12 -0.351  5.38 1            \n\n# p1: 線形回帰（全体で一つの回帰線）\np1 &lt;- ggplot(df_random_both, aes(x = x, y = y)) +\n    geom_point(alpha = 0.5, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"線形回帰（固定効果のみ）\") +\n    theme(text = element_text(family = \"IPAexGothic\"))\n\n# p2: ランダム切片ランダム傾きモデル（個人ごとに異なる切片と傾きの回帰線）\np2 &lt;- ggplot(df_random_both, aes(x = x, y = y, color = person_factor)) +\n    geom_point(alpha = 0.6, size = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n    theme_minimal() +\n    labs(x = \"説明変数\", y = \"目的変数\", title = \"ランダム切片ランダム傾きモデル\") +\n    theme(\n      text = element_text(family = \"IPAexGothic\"),\n      legend.position = \"none\"\n    )\n\np1 + p2\n\n\n\n\n\n\n\n\nプロットを見ると，個人ごとに色分けしていない図(左；固定効果のみの線形回帰)だけ見て単純な線形回帰でもいいように思えるが，色分けした図(右；ランダム切片ランダム傾きモデル)をみると，より個体差をうまく捉えていることがわかる。データはまず可視化することが重要であることがわかるだろう。\nさて，これについての推定コードの書き方も見ておこう。切片と傾きの両方が変量効果として()の中に記述される。\n\nresult.bayes.random_both &lt;- brm(\n    y ~ x + (1 + x | person),\n    family = gaussian(),\n    data = df_random_both,\n    seed = 12345,\n    chains = 4, cores = 4, backend = \"cmdstanr\",\n    iter = 2000, warmup = 1000,\n    refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 4.4 seconds.\nChain 3 finished in 4.4 seconds.\nChain 1 finished in 4.7 seconds.\nChain 4 finished in 4.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.5 seconds.\nTotal execution time: 4.8 seconds.\n\nsummary(result.bayes.random_both)\n\nWarning: There were 7 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x + (1 + x | person) \n   Data: df_random_both (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~person (Number of levels: 10) \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)        0.96      0.29     0.55     1.70 1.00     1083     2237\nsd(x)                0.34      0.10     0.20     0.59 1.00     1244     1640\ncor(Intercept,x)     0.33      0.30    -0.34     0.80 1.00     1492     2408\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.83      0.33     0.14     1.47 1.00     1360     1763\nx             1.65      0.11     1.43     1.89 1.00     1064     1454\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.53      0.03     0.47     0.58 1.00     4200     2756\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(result.bayes.random_both)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 比較のためにML推定も行っておく\nresult.ml.random_both &lt;- lmer(y ~ x + (1 + x | person), data = df_random_both)\nsummary(result.ml.random_both)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x + (1 + x | person)\n   Data: df_random_both\n\nREML criterion at convergence: 385\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4362 -0.6324 -0.0689  0.5880  2.7101 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n person   (Intercept) 0.62433  0.7901       \n          x           0.07568  0.2751   0.43\n Residual             0.27260  0.5221       \nNumber of obs: 200, groups:  person, 10\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)  0.83552    0.26122 8.76380   3.199   0.0112 *  \nx            1.65027    0.08804 8.98000  18.745 1.65e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.369 \n\n\n変量効果のモデル表記で，(1 + x | person)は切片と傾きの両方にランダム効果を考慮することを表す。1は切片，xは傾きのランダム効果を指定している。\nこのモデルでは切片と傾きの相関も推定される。設定した相関（\\(\\rho\\) = 0.3）がどの程度再現されているか確認してみよう。また，個人レベルの推定値も前述の方法で取り出すことができる。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線形モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html#階層線形モデルhlm",
    "href": "chapter13.html#階層線形モデルhlm",
    "title": "13  線形モデルの展開",
    "section": "13.4 階層線形モデル(HLM)",
    "text": "13.4 階層線形モデル(HLM)\n階層線形モデルは，さらに確率分布をMixしていくモデルである。 個人差変数を変量効果として考えた時，ある個人\\(i\\)から複数の反応\\(j\\)を得てデータとしているのであった。これを個人に「ネストされた」データ(入子状のデータ。マトリョーシカのようなもの)として考えると，階層線形モデルのイメージに近い。\n階層線形モデルが扱うのは，階層構造を持つデータである。例えばある学級での教育効果を考えるとして，調査研究のはんいがひろがれば学級間比較，学校間比較となっていくだろう。学級は学校にネストされているし，学校は地区にネストされているし，地区は市区町村に，市区町村は都道府県にネストされている，という形で階層を考えることができる。\nまた，学級の中の個人，個人の中の課題種別・・・と内側にネストしていくこともできる。ポイントはネストのレベルに対応した確率分布を仮定するから，そのネストレベルに含まれる各要素は質が同じで相互に交換可能なものであると考える点である。例えば記憶実験などにおいて，同程度の「無意味さ」を持った単語を覚えると言ったとき，「めぬそ」「ぬきは」といっった言葉はどちらを刺激として与えられても三文字の無意味な綴りであるという点では等質と皆しているのである。\nまた階層線形モデルの面白いところは，上位の変数が下位の変数に影響を与えるようなモデルや，その逆のようなレベル間関係をモデリングできるところである。たとえば市区町村レベルの住民の数が，学級レベルの教育水準に影響をしているかもしれない。こういったレベル間関係も(理論的には)表現できるのである。\nどこにレベルを想定するか，そのレベルを想定する意味があるかどうかについても検証する必要がある。過度に複雑なモデルにしても意味がないので，そのレベルで一旦まとめる必要があるかどうかを，事前にチェックする必要がある。レベルでまとめることの意義は，級内相関(Interclass Correlation Coefficient)を見ることで判断する。\n\n13.4.1 2レベル階層線形モデル\n最も基本的な階層線形モデルは2レベルモデルである。ここでは学級（クラスター）にネストされた生徒の学習データを例に，2レベル階層線形モデルについて説明する。\n\n13.4.1.1 レベル1（個人レベル）のモデル\n個人\\(i\\)（\\(i = 1, 2, \\ldots, n_j\\)）が学級\\(j\\)（\\(j = 1, 2, \\ldots, J\\)）に属している場合，レベル1のモデルは以下のように表される：\n\\[\nY_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + r_{ij}\n\\]\nここで：\n\n\\(Y_{ij}\\)：学級\\(j\\)の生徒\\(i\\)の学習成績\n\\(X_{ij}\\)：学級\\(j\\)の生徒\\(i\\)の説明変数（例：学習時間）\n\\(\\beta_{0j}\\)：学級\\(j\\)の切片（学級\\(j\\)の平均的な学習成績）\n\\(\\beta_{1j}\\)：学級\\(j\\)の傾き（学級\\(j\\)における説明変数の効果）\n\\(r_{ij}\\)：個人レベルの誤差項，\\(r_{ij} \\sim N(0, \\sigma^2)\\)\n\n\n\n13.4.1.2 レベル2（学級レベル）のモデル\n学級レベルでは，レベル1の係数が学級レベルの変数の関数として表される：\n\\[\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01}W_j + u_{0j}\n\\]\n\\[\n\\beta_{1j} = \\gamma_{10} + \\gamma_{11}W_j + u_{1j}\n\\]\nここで：\n\n\\(W_j\\)：学級\\(j\\)の説明変数（例：学級規模）\n\\(\\gamma_{00}\\)：全体の切片（全学級の平均的な学習成績）\n\\(\\gamma_{01}\\)：学級レベル変数\\(W_j\\)の切片への効果\n\\(\\gamma_{10}\\)：全体の傾き（全学級の平均的な効果の大きさ）\n\\(\\gamma_{11}\\)：学級レベル変数\\(W_j\\)の傾きへの効果\n\\(u_{0j}, u_{1j}\\)：学級レベルの誤差項（変量効果）\n\n変量効果は多変量正規分布に従うと仮定される：\n\\[\n\\begin{pmatrix} u_{0j} \\\\ u_{1j} \\end{pmatrix} \\sim N\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\tau_{00} & \\tau_{01} \\\\ \\tau_{01} & \\tau_{11} \\end{pmatrix} \\right)\n\\]\n\n\n13.4.1.3 統合モデル\nレベル1とレベル2のモデルを統合すると：\n\\[\nY_{ij} = \\gamma_{00} + \\gamma_{01}W_j + \\gamma_{10}X_{ij} + \\gamma_{11}W_j X_{ij} + u_{0j} + u_{1j}X_{ij} + r_{ij}\n\\]\nこの式は以下のように分解できる：\n固定効果（Fixed Effects）：\n\n\\(\\gamma_{00}\\)：全体切片\n\\(\\gamma_{01}\\)：学級レベル変数の主効果\n\\(\\gamma_{10}\\)：個人レベル変数の主効果\n\\(\\gamma_{11}\\)：クロスレベル交互作用効果\n\n変量効果（Random Effects）：\n\n\\(u_{0j}\\)：学級の切片における変量効果\n\\(u_{1j}X_{ij}\\)：学級の傾きにおける変量効果\n\\(r_{ij}\\)：個人レベルの誤差\n\n\n\n13.4.1.4 級内相関係数（ICC）\n階層構造の意義を評価するために，級内相関係数（Intraclass Correlation Coefficient: ICC）を計算する。ICCは同じクラスター内の観測値間の相関を表す：\n\\[\n\\text{ICC} = \\frac{\\tau_{00}}{\\tau_{00} + \\sigma^2}\n\\]\nここで：\n\n\\(\\tau_{00}\\)：学級間分散（between-group variance）\n\\(\\sigma^2\\)：学級内分散（within-group variance）\n\nICCが0に近い場合，階層構造を考慮する必要性は低く，ICCが大きい場合（一般的に0.05以上），階層モデルの適用が推奨される。\n\n\n\n13.4.2 具体例\nここでは，階層線形モデルの具体例としてこれまで用いてきた野球のデータを用いる。このデータには選手の個人成績（身長，体重，安打数など）と，選手が所属するチーム，守備位置といった階層構造が含まれている。\n野球データの特徴として，選手は特定のチームに所属し，そのチーム内で特定の守備位置を担当するという階層構造がある。つまり，守備位置はチームにネストされた構造となっている。チームレベルでは戦術や指導方法が共通し，同じチーム内の守備位置レベルでは求められる技能や体格が類似する傾向がある。このような階層構造を無視して分析すると，個人差を過大評価したり，統計的推論に誤りが生じる可能性がある。\n本例では2020年度のデータに限定し，投手を除く野手のデータのみを使用する。投手は他の野手とは明らかに異なる特性を持つため，分析から除外することで，より一貫性のあるデータセットとする。\nまずは階層構造の意義を確認するため，チームおよび守備位置のICC（級内相関係数）を計算し，階層モデルの適用が適切かどうかを検証する。ICCの計算にはmultilevelパッケージを用いた。\n\npacman::p_load(tidyverse, brms, bayesplot, multilevel)\ndat &lt;- read_csv(\"Baseball.csv\") %&gt;%\n    filter(Year == \"2020年度\") %&gt;%\n    mutate(\n        position = as.factor(position)\n    ) %&gt;%\n    filter(position != \"投手\")\n\n# Calculate ICC using multilevel package\n# チームレベルのICC\nicc_team &lt;- multilevel::ICC1(aov(Hit ~ team, data = dat))\n\n# ネスト構造を考慮したICC計算\n# チーム内ポジションのICC\ndat$team_position &lt;- paste(dat$team, dat$position, sep = \"_\")\nicc_team_position &lt;- multilevel::ICC1(aov(Hit ~ team_position, data = dat))\n\n# 参考：ポジション単独のICC\nicc_position &lt;- multilevel::ICC1(aov(Hit ~ position, data = dat))\n\nprint(paste(\"Team ICC1:\", round(icc_team, 3)))\n\n[1] \"Team ICC1: -0.031\"\n\nprint(paste(\"Team:Position ICC1:\", round(icc_team_position, 3)))\n\n[1] \"Team:Position ICC1: -0.029\"\n\nprint(paste(\"Position ICC1:\", round(icc_position, 3)))\n\n[1] \"Position ICC1: 0.046\"\n\n\n\n# チーム内ポジションのネスト構造を作成\ndat &lt;- dat %&gt;%\n    mutate(team_position = paste(team, position, sep = \"_\"))\n\n# データ構造の確認\ndat %&gt;% \n    group_by(team, position) %&gt;% \n    summarise(n = n(), .groups = \"drop\") %&gt;% \n    head(10)\n\n# A tibble: 10 × 3\n   team    position     n\n   &lt;chr&gt;   &lt;fct&gt;    &lt;int&gt;\n 1 Carp    外野手       9\n 2 Carp    内野手      14\n 3 Carp    捕手         6\n 4 DeNA    外野手       8\n 5 DeNA    内野手      13\n 6 DeNA    捕手         5\n 7 Dragons 外野手      10\n 8 Dragons 内野手      12\n 9 Dragons 捕手         5\n10 Eagles  外野手       9\n\n\n\n# ネスト構造を考慮したポアソンモデル\n# (1 | team) + (1 | team:position) でポジションをチーム内にネスト\nmodel_hit &lt;- brm(\n    Hit ~ Games + height + weight + (1 | team) + (1 | team:position),\n    data = dat,\n    family = poisson(),\n    chains = 4,\n    iter = 10000,\n    cores = 4\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\nWarning: There were 25 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n# Model summary for Hit model\nsummary(model_hit)\n\nWarning: There were 25 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n\nplot(model_hit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Posterior predictive check for Hit model\npp_check(model_hit, ndraws = 100)\n\n\n\n\n\n\n\n\n\npredicted_hit &lt;- fitted(model_hit,\n    newdata = dat,\n    allow_new_levels = TRUE\n) %&gt;% as.data.frame()\n\n\nplot_data &lt;- data.frame(\n    observed = dat$Hit,\n    predicted = predicted_hit$Estimate,\n    team = dat$team,\n    position = dat$position\n)\n\nggplot(plot_data, aes(x = observed, y = predicted, color = team)) +\n    geom_point(alpha = 0.7, size = 2) +\n    geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n    labs(\n        x = \"Observed Hit\", y = \"Predicted Hit\",\n        title = \"Predicted vs Observed Hit Count (2020年度)\",\n        color = \"Team\"\n    ) +\n    facet_wrap(team ~ position, scales = \"free\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線形モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter13.html#footnotes",
    "href": "chapter13.html#footnotes",
    "title": "13  線形モデルの展開",
    "section": "",
    "text": "エクスキューズが許されるなら，当時の統計学，計算機科学は現代に比べると貧弱で，理論的に正しくないことがわかっていても，ユーザにはそれ以上のことができなかったということがある。また心理学で測定されるデータは，あくまでも目に見えない心を表現したラフな近似値で，統計学上の仮定に違反したことで生じる問題よりも，そもそも本質的な問題があるのだから，統計の運用もあくまでもラフな近似値で良い，という風潮があったのかもしれない。↩︎\n一般化線型モデルと一般線型モデルは，一文字「化」が入るかどうかの微妙な違いである。英語ではgeneralとgeneralizedの違いなので，こちらで理解した方が間違いが少なくて良いかもしれない。↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>線形モデルの展開</span>"
    ]
  },
  {
    "objectID": "chapter08.html",
    "href": "chapter08.html",
    "title": "8  平均値差の検定",
    "section": "",
    "text": "8.1 一標本検定\n平均値差の検定は，実験計画の結論を出すために用いられる手段である。無作為割り当てによって個人差や背景要因が相殺され，平均的な因果効果を検証することができるからである。 その結果を一般化するためには，やはり推測統計学の知見が必要であり，サンプルサイズやタイプ1,2エラーが関わってくることに変わりはない。\nまず配置標本検定の例から始める。母平均がわかっている，あるいは理論的に仮定される特定の値に対して，標本平均が統計的に有意に異なっていると言って良いかどうかの判断をするときに用いる。 たとえば7件法のデータを取ったときに，ある項目の平均が中点4より有意に離れていると言って良いかどうか，といった判定をするときに用いる。かりに，サンプルサイズ10で7件法のデータが得られたとしよう。ここでは平均4,SD1の正規乱数を10件生成することで表現する。実際にはこの値を，人に対する尺度カテゴリへの反応として得ているはずである。\npacman::p_load(tidyverse)\nset.seed(17)\nn &lt;- 10\nmu &lt;- 4\nX &lt;- rnorm(n, mean = mu, sd = 1)\nprint(X)\n\n [1] 2.984991 3.920363 3.767013 3.182732 4.772091 3.834388 4.972874 5.716534\n [9] 4.255237 4.366581\n今回，標本平均は4.177であり，これより極端な値が\\(\\mu = 4\\)の母集団から得られるかどうかを検定する。帰無仮説検定の手順にそって進めていくと，以下のようになる。\nこのあと，検定統計量の計算と判定である。これをRはt.test関数で一気に処理できる。\nresult &lt;- t.test(X, mu = mu)\nprint(result)\n\n\n    One Sample t-test\n\ndata:  X\nt = 0.6776, df = 9, p-value = 0.5151\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 3.585430 4.769131\nsample estimates:\nmean of x \n 4.177281\n結果として，今回の検定統計量の実現値は0.678であり，自由度9のt分布からこれ以上の値が出てくる確率は，0.515であることがわかる。これは5%水準と見比べてより大きいので，レアケースではないと判断できる。つまり，母平均4の正規母集団から，4.177の標本平均が得られることはそれほど珍しいものではなく，統計的に有意に異なっていると判断するには及ばない，ということである。\nレポートなどに記載するときは，これら実現値やp値を踏まえて「\\(t(9)=0.66776,p=0.5151 ,n.s.\\)」などとする。ここでn.s.はnot significantの略である。\nさてこの例では，母平均4の正規乱数を生成し，その平均が4と異なるとはいえない，と結論づけた。これは一見，当たり前のことのようであり，無意味な行為におもえるかもしれない。しかし次の例を見てみよう。\nn &lt;- 3\nmu &lt;- 4\nX &lt;- rnorm(n, mean = mu, sd = 1)\nmean(X) %&gt;%\n  round(3) %&gt;%\n  print()\n\n[1] 5.04\n\nresult &lt;- t.test(X, mu = mu)\nprint(result)\n\n\n    One Sample t-test\n\ndata:  X\nt = 5.1723, df = 2, p-value = 0.03541\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 4.174825 5.904710\nsample estimates:\nmean of x \n 5.039768\nここではサンプルサイズ\\(n=3\\)であり，標本平均が5.04であった。このときt値は5%臨界値を上回っており，「母平均4のところから得られる値にしては極端」であるから，統計的に有意に異なる，と判断することになる。乱数生成時は平均を確かに4に設定したが，母平均から取り出したごく一部が，そこから大きく離れてしまうことはあり得るのである。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#一標本検定",
    "href": "chapter08.html#一標本検定",
    "title": "8  平均値差の検定",
    "section": "",
    "text": "帰無仮説は母平均が理論的な値(ここでは尺度の中点4)であること，すなわち\\(\\mu =4\\)であり，対立仮説は\\(\\mu \\neq 4\\) である。\n検定統計量は，正規母集団から得られる標本平均が従う標本分布であり，母分散が未知の場合の区間推定に用いたT統計量になる。\n判断基準は心理学の慣例に沿って5%とする。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#二標本検定",
    "href": "chapter08.html#二標本検定",
    "title": "8  平均値差の検定",
    "section": "8.2 二標本検定",
    "text": "8.2 二標本検定\n続いて二標本の検定について考えよう。実験群と統制群のように，無作為割り当てをすることで平均因果効果をみる際に行われるのが，この検定である。帰無仮説は「群間差はない」であり，対立仮説はその否定である。また，正規母集団からの標本を仮定するので，検定統計量はここでもt分布に従う値になる。帰無仮説検定の手順に沿って，改めて確認しておこう。\n\n帰無仮説は「二群の母平均に差がない」である。二群の母平均をそれぞれ\\(\\mu_1,\\mu_2\\)とすると，帰無仮説は\\(\\mu_1 = \\mu_2\\)，あるいは\\(\\mu_1 - \\mu_2 = 0\\)と表される。対立仮説は\\(\\mu_1 \\neq \\mu_2\\)あるいは\\(\\mu_1-\\mu_2 \\neq 0\\)である。\n検定統計量は，正規母集団から得られる標本平均が従う標本分布であり，母分散が未知の場合の区間推定に用いたT統計量になる。\n判断基準は心理学の慣例に沿って5%とする。\n\nこれを検証するために，サンプルデータを乱数で生成しよう。 まず，各群のサンプルサイズをn1,n2とする。ここでは話を簡単にするため，サンプルサイズは両群ともに10とした。つぎに両群の母平均だが，群1の母平均を\\(\\mu_1\\)，群2の母平均を\\(\\mu_2 = \\mu_1 + \\delta\\)で表現した。この\\(\\delta\\)は差分であり，これが\\(\\delta=0\\)であれば母平均が等しいこと，\\(\\delta \\neq 0\\)であれば母平均が異なることになる。最後に両群の母SDを設定した。\nここでの検定は，この差分\\(d\\)が母平均0の母集団から得られたと判断して良いかどうか，という形で行われる。検定統計量\\(T\\)は，次式で算出されるものである。\n\\[ T = \\frac{d - \\mu_0}{\\sqrt{U^2_p/\\frac{n_1n_2}{n_1+n_2}}}\\]\nここで\\(d\\)は二群の標本平均の差であり，\\(U^2_p\\)はプールされた不偏分散と呼ばれ，二群を合わせて計算された全体の母分散推定量である。各群の標本分散をそれぞれ\\(S^2_1, S^2_2\\)とすると，次式で算出される。\n\\[ U^2_p = \\frac{n_1S^2_1+ n_2S^2_2}{n_1 + n_2 -2} \\]\nこれらの式はつまり，サンプルサイズの違いを考慮するため，一旦両群の標本分散に各サンプルサイズを掛け合わせ，プールした全体のサンプルサイズから各々\\(-1\\)をすることで全体として不偏分散にしている。\nこれを踏まえて，具体的な数字で見ていこう。 その上で乱数でデータを生成し，その標本平均を確認した上で，t.test関数によって検定を行っている。\n\nn1 &lt;- 10\nn2 &lt;- 10\nmu1 &lt;- 4\nsigma &lt;- 1\ndelta &lt;- 1\nmu2 &lt;- mu1 + (sigma * delta)\n\nset.seed(42)\nX1 &lt;- rnorm(n1, mean = mu1, sd = sigma)\nX2 &lt;- rnorm(n2, mean = mu2, sd = sigma)\n\nX1 %&gt;%\n  mean() %&gt;%\n  round(3) %&gt;%\n  print()\n\n[1] 4.547\n\nX2 %&gt;%\n  mean() %&gt;%\n  round(3) %&gt;%\n  print()\n\n[1] 4.837\n\nresult &lt;- t.test(X1, X2, var.equal = TRUE)\nprint(result)\n\n\n    Two Sample t-test\n\ndata:  X1 and X2\nt = -0.49924, df = 18, p-value = 0.6237\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.506473  0.927980\nsample estimates:\nmean of x mean of y \n 4.547297  4.836543 \n\n\n今回の母平均は\\(\\mu_1 = 4, \\mu_2 = 4+1\\)にしているが，標本平均は4.547と4.837であり，標本上では大きな差が見られなかった。結果として，t値は0.4992369であり，自由度18のもとでのp値は0.6236593である。5%水準を上回る値であるから，結論としては対立仮説を採択するには至らない，差があるとはいえない，である。\n今回の設定では母平均に差があるはず(\\(4 \\neq 4 + 1\\))なのだから，これは誤った判断で，タイプ2エラーが生じているケースということになる。研究実践場面では，母平均やその差については知り得ないのだから，このような判断ミスが生じていたかどうかは分かり得ないことに留意しよう。\nなお，ここではわかりやすく2群であることを示すためにX1,X2と2つのオブジェクトを用意したが，実践的にはデータフレームの中で群わけを示す変数があり，formulaの形で次のように書くことが多いだろう。\n\ndataSet &lt;- data.frame(group = c(rep(1, n1), rep(2, n2)), value = c(X1, X2)) %&gt;%\n  mutate(group = as.factor(group))\nt.test(value ~ group, data = dataSet, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  value by group\nt = -0.49924, df = 18, p-value = 0.6237\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.506473  0.927980\nsample estimates:\nmean in group 1 mean in group 2 \n       4.547297        4.836543",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#二標本検定ウェルチの補正",
    "href": "chapter08.html#二標本検定ウェルチの補正",
    "title": "8  平均値差の検定",
    "section": "8.3 二標本検定(ウェルチの補正)",
    "text": "8.3 二標本検定(ウェルチの補正)\n先ほどのt.test関数には，var.equal = TRUEというオプションが追加されていた。これは2群の分散が等しいと仮定した場合の検定になる。t検定は歴史的にこちらが先に登場しているが，2群の分散が等しいかどうかはいきなり前提できるものでもない。等分散性の検定は，Levene検定を行うのが一般的であり，R においては，carパッケージやlawstat パッケージが対応する関数を持っている。ここではcarパッケージの leveneTest関数を用いる例を示す。\n\npacman::p_load(car)\nleveneTest(value ~ group, data = dataSet, center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  1  2.9405 0.1035\n      18               \n\n\nこの結果を見ると，p値から明らかなように，2群の分散が等しいという帰無仮説が棄却できなかったので，等しいと考えてt検定に進むことができる。もしこれが棄却されてしまったら，2群の分散が等しいという帰無仮説が成り立たないのだから，等分散性の仮定を外す必要がある。実行は簡単で，var.equalをFALSEにすれば良い。\n\nresult2 &lt;- t.test(value ~ group, data = dataSet, var.equal = FALSE)\nprint(result2)\n\n\n    Welch Two Sample t-test\n\ndata:  value by group\nt = -0.49924, df = 13.421, p-value = 0.6257\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.5369389  0.9584459\nsample estimates:\nmean in group 1 mean in group 2 \n       4.547297        4.836543 \n\n\nよく見ると，タイトルがWelch Two Sample t-testに変わっている。Welchの補正が入ったt検定という意味である。また自由度が実数(13.421)になっているが，このようにt分布の自由度を調整することで等分散性の仮定から逸脱した場合の補正となる。もちろん報告する際は「\\(t(\\) 13.421 \\()=\\) -0.499, \\(p=\\) 0.626」のように書くことになるから，自由度が実数であれば補正済みであると考えられるだろう。\nしかし，分散が等しいという仮定は，等しくない場合の特殊な場合であるから，最初からWelchの補正がはいった検定だけで十分である。このような考え方から，Rにおけるt.test 関数のデフォルトではvar.equal = FALSEとなっており，特段の指定をしなければ等分散性の仮定をしない。こちらの方が検定を重ねることがないので，より望ましい。\n\n8.3.1 効果量の算出\n今回の例は，仮想データとして\\(\\mu_1 = 4,\\mu_2 = \\mu_1 + \\sigma d\\)であり，明らかに\\(\\mu_1 \\neq \\mu_2\\)なのだが，有意差を検出するには至らなかった。統計的な有意差はあくまでも「統計的な」観点からのものであり，我々が現実に検証したいのは本当に差があるかどうか，いわば「実質的な差」があるかどうかであるのだから，統計的な有意差を得ることを目的にするのははっきりと不適切な目標設定であると言えるだろう1。\nところで，統計的に差があるとはっきり言えるのはどのような時だろうか。これは次の4つのデータの分布を見てもらうとわかりやすい。\n\n\n\n\n\n\n\n\n\n左列は平均差が大きいデータ，右列は小さいデータである。 上段は分散が小さいデータ，下段は大きいデータである。 この4つそれぞれのシーンにおいて，「差がある」と判断しやすいのはどれかを考えてみるとよい。当然，左上のシーンが最も明確に差があると言えるであろう。なぜなら，両群が明確に分かれており，群間の重複がないからである。左下は同じ平均値差であっても，群内の広がりが大きいから群間の重複がみられるため，「差がある」という判断を受けても各群の中には該当しないケースがちらほらみられることだろう。右上パネルのようなケースでは，重複は少ないが差が小さいため，「差がある」と判断できるかどうかが微妙である。右下に至っては，差も小さく分布の重複も大きいから，「差がある」と判断しても該当しないケースが多くなる。たとえば「男性は女性よりも力が強い(体力・筋力に差がある)」というデータがあったとしても，「女性より非力な男性」もかなり多く存在するだろう。そういう反例が多くみられるような場合，統計的に差があるという結果が示されたとしても，受け入れられないのではないだろうか。\nここから明らかなように，差の判断には平均値差だけでなく分散も関わってくる。そこで平均値差を標準偏差で割った，標準化された差が重要になってくるのであり，これが効果量と呼ばれるものである2。\n今回2群の差のデータを作る時に，\\(\\sigma d\\) としたが，平均値差の効果量esは， \\[ es = \\frac{\\mu_1 - \\mu_2}{\\sigma} \\]\nで表現されるから，\\(d\\)が効果量を表していたのである。もちろん我々は母平均，母SDなどを知り得ないのでこれもデータから推定する他ない。幸いRにはeffsizeパッケージなど，効果量を算出するものが用意されている。\n\npacman::p_load(effsize)\ncohen.d(value ~ group, data = dataSet)\n\n\nCohen's d\n\nd estimate: -0.2232655 (small)\n95 percent confidence interval:\n    lower     upper \n-1.165749  0.719218 \n\ncohen.d(value ~ group, data = dataSet, hedges.correction = TRUE)\n\n\nHedges's g\n\ng estimate: -0.2138318 (small)\n95 percent confidence interval:\n     lower      upper \n-1.1162608  0.6885973 \n\n\n平均値差の検定の後は，ここに示したCohenのdやHedgesのgといった効果量を添えて報告することが一般的である。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#対応のある二標本検定",
    "href": "chapter08.html#対応のある二標本検定",
    "title": "8  平均値差の検定",
    "section": "8.4 対応のある二標本検定",
    "text": "8.4 対応のある二標本検定\n実験群と統制群のように異なる2群ではなく，プレポスト実験のように対応がある2群の場合は，t検定の定式化が異なる。対応がないt検定の場合は，群平均の差\\(\\mu_1 - \\mu_2\\)の分布を考えたが，対応がある場合は個々の測定の差，つまり\\(X_{i1} - X_{i2} = D_i\\)を考える。この一つの標本統計量を検定するのだから，一標本検定の一種であるとも言える。またこのDの分布の標準誤差は，標本標準誤差\\(U_D\\)を使った\\(U_D/\\sqrt{n}\\)を使って推定する3。検定統計量\\(T\\)は，次式で算出される。\n\\[ T = \\frac{\\bar{D}}{U_D/\\sqrt{n}} = \\frac{\\sum D_i/n}{\\sqrt{\\frac{\\frac{1}{n-1}\\sum(D_i-\\bar{D})^2}{n}}}\\]\n検定にあたっては，t.test関数の引数pairedをTRUEにするだけで良い。\n\n8.4.1 仮想データの組成\n仮想データを作って演習してみよう。データの組成については，2種類のアプローチで説明が可能である。ひとつは次のシミュレーションで表されるような形である。\n\nn &lt;- 10\nmu1 &lt;- 4\nsigma &lt;- 1\nd &lt;- 1\nX1 &lt;- rnorm(n, mu1, sigma)\nX2 &lt;- X1 + sigma * d + rnorm(n, 0, sigma)\nt.test(X1, X2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  X1 and X2\nt = -1.8036, df = 9, p-value = 0.1048\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.4339112  0.1617193\nsample estimates:\nmean difference \n     -0.6360959 \n\n\nすなわち，第一の群が\\(\\mu_1\\)を平均にばらついた実現値として得られ，第二の群はその実現値に一定の効果\\(\\sigma *d\\)が加わり，その測定にさらに誤差がつく形である。この方法は具体的なデータ生成プロセスをそのまま模したような形でデータを作っているが，測定誤差を二重に計上している点が気になるかもしれない。\nもう一つの考え方は，プレポスト型のデータに限らず，何らかの形で「対応がある」ことも表現できるものである。対応があるということは，2つのデータがそれぞれ独立した一変数正規分布から得られているのではなく，二変数正規分布から得られると考えるのである。二変数正規分布は，それぞれの変数は正規分布しているが，両者の間に相関があると考えるものである。変数が一つだけの正規分布は \\[X \\sim N(\\mu,\\sigma)\\] で表現されているのに対し，複数の変数を同時に生成する多変数(多次元)正規分布Multivariate Normal Distributionは，以下のように表現される。 \\[ \\mathbf{X} \\sim MVN(\\mathbf{\\mu},\\mathbf{\\Sigma})\\]\nここで\\(\\mathbf{X}\\)や\\(\\mathbf{\\mu}\\)は\\(n\\)次元ベクトルであり，\\(\\mathbf{\\Sigma}\\)は分散共分散行列を表している。二変数の場合は以下のように書くことができる。\n\\[\\mathbf{\\Sigma} = \\begin{pmatrix} \\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2 \\end{pmatrix} = \\begin{pmatrix} \\sigma_1^2 & \\rho_{12}\\sigma_1\\sigma_2 \\\\ \\rho_{21}\\sigma_2\\sigma_1 & \\sigma_2^2 \\end{pmatrix}\\]\n共分散\\(\\sigma_{ij}\\)は相関係数\\(\\rho_{ij}\\)を用いて書けることからわかるように，変数間に相関があることを想定してデータを生成するのである。この組成に従った仮想データの作成は以下のとおりである。\n\npacman::p_load(MASS) # 多次正規乱数を生成するのに必要\nn &lt;- 10\nmu1 &lt;- 4\nsigma &lt;- 1\nd &lt;- 1\nmu &lt;- c(mu1, mu1 + sigma * d)\nrho &lt;- 0.4\nSIG &lt;- matrix(c(sigma^2, rho * sigma * sigma, rho * sigma * sigma, sigma^2), ncol = 2, nrow = 2)\nX &lt;- mvrnorm(n, mu, SIG)\nt.test(X[, 1], X[, 2], paired = TRUE)\n\n\n    Paired t-test\n\ndata:  X[, 1] and X[, 2]\nt = -2.4313, df = 9, p-value = 0.0379\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.96934592 -0.07095313\nsample estimates:\nmean difference \n       -1.02015 \n\n\n効果量については，対応のないt検定の場合と同じで良い。\n\ncohen.d(X[, 1], X[, 2])\n\n\nCohen's d\n\nd estimate: -1.04088 (large)\n95 percent confidence interval:\n      lower       upper \n-2.04204357 -0.03971697 \n\ncohen.d(X[, 1], X[, 2], hedges.correction = TRUE)\n\n\nHedges's g\n\ng estimate: -0.9968994 (large)\n95 percent confidence interval:\n     lower      upper \n-1.9510179 -0.0427809 \n\n\n\n\n8.4.2 検定の方向性\nここまでの検定では，主に「差があるかどうか」といった仮説に対応するものを扱ってきた。差があるかどうか，というのはその差がプラスの方向にでているのか，マイナスの方向に出ているのかといったことを問題にしていない。そこで検定統計量の分布についても，分布の両裾を考えて有意水準を設定していた。\nしかしプレポスト実験などでは，効果が「上がった」のか「下がった」のか，ということが大きな関心時でもあることが多いだろう。効果がある，ただし逆効果である，というのでは意味がないからである。このように方向性をもった仮説を検証する場合は，検定統計量の分布も一方向だけ考えればよく，t.test関数にはalternativeオプションをつかって表現する。\nt.test(x,y,alternatives = \"less\") とすると\\(x &lt; y\\)の帰無仮説を検証することになるし，alternatives = \"greater\"とすると\\(x &gt; y\\)の帰無仮説を検証することになる。デフォルトではalternatives = \"two.sided\"であり，両側検定が選ばれている。\nただし，両裾から片裾にかわるということは，検定統計量が超えるかどうかの判断をする臨界値が小さくなることでもある。必然的に，片裾(片側検定)のほうが緩やかな基準で検定をしていることにもなる。デフォルトで普段から厳しく検定しているから大丈夫だろう，というのも一つの考え方だが，やはり本来の研究仮説に適した帰無仮説の設定をするべきだろう。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#課題",
    "href": "chapter08.html#課題",
    "title": "8  平均値差の検定",
    "section": "8.5 課題",
    "text": "8.5 課題\n\n平均が50、標準偏差が10の正規分布からランダムに選んだ30個のサンプルを用意し，このサンプルの平均が母集団の平均と異なるかどうかを検定してください。検定結果を，心理学のフォーマット(心理学会編「論文執筆投稿の手引き」)に準拠した書き方で，結果を記述してください。\n以下のデータセットを使用して，2つの独立した群の平均に差があるかどうかをt検定してください。検定結果を，心理学のフォーマット(心理学会編「論文執筆投稿の手引き」)に準拠した書き方で，結果を記述してください。 \\[ group1 =\\{45, 50, 55, 60, 65 \\} \\] \\[ group2 = \\{57, 60, 62, 77, 75 \\} \\]\n多次元正規分布を用いた仮想データ生成方で，対応のあるt検定の練習をしましょう。サンプルサイズを\\(n=20\\)とし, 平均ベクトル\\(\\mu = (12, 15)\\), 分散共分散行列\\(\\Sigma = \\begin{pmatrix} 4 & 2.8 \\\\ 2.8 & 4\\end{pmatrix}\\)の多次元正規分布から作られた乱数を使って，対応のあるt検定をしてください。検定結果を，心理学のフォーマット(心理学会編「論文執筆投稿の手引き」)に準拠した書き方で，結果を記述してください。\n自由度が10, 20, 30のt分布のグラフを，標準正規分布のグラフとともに描画してください。自由度が増えるとt分布がどのように変化するでしょうか。\n自由度が15のt分布において、有意水準5%の片側検定と両側検定の臨界値(検定の判断基準となる理論値)を求めてください。\n\n\n\n\n\n豊田秀樹. 2009. 検定力分析入門: Rで学ぶ最新データ解析ー. 東京図書.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter08.html#footnotes",
    "href": "chapter08.html#footnotes",
    "title": "8  平均値差の検定",
    "section": "",
    "text": "たとえば物理学などのシーンでは，測定の精度が高く，単一の物理世界を対象にした検証を行うのだから，予測が真であるか偽であるかを確率的に考えるような必要はない。そのような世界における検証–あえて理論的な正しさが明確な世界，と表現するが–であれば，統計的な差があるかどうかの情報はあくまでも理論を支持するおまけ情報にすぎない。いわば統計的検定の結果を報告するのは，論文を書くためのレトリックである。例えば，ニュートンの運動法則やアインシュタインの相対性理論などの物理法則の検証では，測定誤差の範囲内で理論値と実験値が一致するかどうかが重要であり，統計的な有意性は二次的な情報に過ぎない。翻って，人間を対象にした小サンプルの科学である心理学は，統計的な判断に頼らざるを得ないという側面はあるだろう。しかしだからと言って，実質的な差が本質的であることを忘れてしまっては本末転倒である。↩︎\n統計的な有意差よりも効果量，効果量よりも実質的な差のほうが意味のある差であることを忘れてはならない。詳しくは 豊田 (2009) を参照。↩︎\n対応があるケースを考えているので，当然\\(n\\)は前後の群で同数である。↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html",
    "href": "chapter09.html",
    "title": "9  多群の平均値差の検定",
    "section": "",
    "text": "9.1 分散分析の基礎\n心理学実験においては，古典的に分散分析モデルが多用されてきた。平均値差を見ることで実験の効果，因果関係を明らかにできるように，巧妙に実験デザインが組み立てられる。その精緻さは理論的一貫性という意味である種の美しさを持ち，多くの研究者が魅了されてきた。\n分散分析にのせることを目的にした実験計画であり，実験デザインの不自由さ(とにかく分散分析をしなければならない!)が批判的に論じられることもあるが，心理学が測定している対象が平均値差以上の精度で議論できる性質でないという反論もあるだろう。\n今や分散分析を超えたより高度な統計モデルがあり，現在の研究においては分散分析はもはや過去のものにすぎないかもしれないが，以後のモデルも分散分析を基本としたその発展系であるので，改めて基本を押さえておくことも重要である。\n分散分析は「分散」の分析であるかのような名称であるが，平均値差を検定するためのものである。なぜ「分散」を冠するかといえば，効果量のところで見たように，平均値差の判断には群内分散の情報が必要だからである。\n多群の平均値の差，その散らばりを群間分散といい，群に含まれるデータの散らばりを群内分散とよぶ。分散分析は群内分散に対する群間分散の比が十分に大きいと考えられる場合，群間に統計的な有意差があると判断する。分散の比を表す確率分布はF分布と呼ばれる。F分布は群間・群内それぞれの自由度を母数にもつ。\nまた，実験計画はBetweenデザインとWithinデザインに区分される。t検定でみたような，対応のない独立した群を対象にしたデザインがBetween，群間に相関が想定される対応のある群を対象にしたデザインがWithinである。Withinデザインは同じ個体から複数回の反応を得る(ex. period 1-2-3…)ため，反復測定デザインRepeated measured designともよばれることがある。この場合，群内分散から個人内の分散すなわち個人差を取り出すことができるため，これが分離できないBetweenデザインよりも基本的にWithinデザインのほうが目的となる変動を捉えやすい。ただし，反復測定による個体への負担を考えると，毎回Withinデザインでいいというわけにもいかないところが難点である。\n\\[Between Design: \\text{全変動} = \\text{群間変動}+ \\text{群内変動(誤差)}\\] \\[Within Design:  \\text{全変動} = \\text{群間変動}+ \\text{個人差変動} + \\text{誤差}\\]\n分散分析は要因が複数ある場合も考えられるから，要因AがBetween，要因BがWithinといった場合は混合計画と呼ばれることがある。慣例的に，要因Factorとその要因に含まれる水準Levelを同時に表現し，\\(\\text{間}2 \\times \\text{間}3\\)の分散分析(二要因の分散分析で，いずれもBetweenデザインであり，水準数がそれぞれ2と3)，といった言い方をすることがある。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#分散分析のステップ",
    "href": "chapter09.html#分散分析のステップ",
    "title": "9  多群の平均値差の検定",
    "section": "9.2 分散分析のステップ",
    "text": "9.2 分散分析のステップ\nt検定において等分散性の仮定が成立するかどうかが事前に問題になったように，Betweenデザインにおいても分散の等質性は仮定されており，Leveneの検定などで事前に検証しておくべきである。またWithinデザインにおいては，データの組成に関わる分散共分散行列の非対角要素が全て等しいことが望ましいが，実践的にはそこまでの仮定が成立しているとは考えにくい。ただし分散分析としては，等分散性の仮定よりも，より緩やかな球面性の仮定が成立していればよいとされており，これを事前に検定することが一般的である。Welchの補正のように，球面性の仮定が成立していない場合は，自由度を補正することで検定の精度が維持される。\n分散分析は多要因・多水準の平均値差の検定である。各水準ごとにt検定を繰り返せば良いのではないか，というアイデアは誰しも思いつくことであろうが，この方法は検定の目的である\\(\\alpha\\)水準の制御ができなくなるという問題を含む。そこで多水準の場合は分散分析を行うことで，すべての要因・水準の母平均が同じであるという帰無仮説を検定し，効果の有無をまず明確にする。この帰無仮説が棄却されたらどこかに差があるわけだから，以後は慎重に\\(\\alpha\\)水準を制御しつつ事後的な検定にすすむ。\n水準間の差をみるための事後的な検定は，下位検定とも呼ばれる。その方法は多岐に渡り，ゴールドスタンダードは存在せず，往々にして分析者が利用しているソフトウェアが対応する手法が選択される。要因・水準が多くなると検証すべき組み合わせも多くなり，下位検定の手続きも非常に煩雑になる。統計ソフトウェアはそれこそ機械的に，幾重にも細かく分散分析表を分解して下位検定をつづけていってくれるが，いくら制御されているとはいえ検定を繰り返していることに変わりはないし，各下位検定の結果を一貫した総合的解釈をするのは困難である。実験計画はシンプルであるほうが望ましいし，複雑なモデルになるようであれば分散分析を超えた，階層線形モデルやベイジアンアプローチなどを取る方が良いだろう。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#anova君を使う",
    "href": "chapter09.html#anova君を使う",
    "title": "9  多群の平均値差の検定",
    "section": "9.3 ANOVA君を使う",
    "text": "9.3 ANOVA君を使う\n分散分析をRで実行するには，基本関数であるaovやcarパッケージなどを用いることができる。 もっとも，その出力は必ずしも親切ではないし，下位検定や効果量の算出などは別のパッケージ，別の関数を用いる必要がある。\n筆者がお勧めするのは，大正大学の井関龍太が開発したanovakunである。パッケージ化されていないので，リンク先からソースコードを読み込んでanovakun関数を実行する必要があるが，さまざまな実験デザインに対応し，また下位検定や効果量，球面性の補正などおよそ分散分析で必要な手法は網羅されている。以下ではこれを用いた実践を行う。\nanovakunの読み込みは，ソースコードをプロジェクトフォルダにダウンロードしてsource関数で読み込むか，インターネットに繋がっている状態でリンク先から直接ソースファイル(anovakun_489.txt)1をsource関数で読み込むといいだろう。\n\nsource(\"https://riseki.cloudfree.jp/?plugin=attach&refer=ANOVA%E5%90%9B&openfile=anovakun_489.txt\")\n\n読み込みが終わるとEnvironタブにanovakun関数が含まれていることを確認しよう。\n\n9.3.1 ANOVA君の入力とデータ\nANOVA君は伝統的にワイド型データから読み込むようになっている。すなわち，一行に1オブザベーション入っている形式である。Between計画の場合は，データの前に水準数を表すインデックスと最終的な従属変数の形に整形したデータが必要である。Within計画の場合は1行に1Obs.なのだから，反復した水準の数だけ右にデータを入れていく形に整形する。\nしかしChapter 3.7 で述べたように，昨今は計算機にとって優しい型，ロング型での入力もおおく，ANOVA君もversion 4.4.0からロング型での入力も許すようになった。その場合はオプションlong=TRUE とロング型であることを明記する必要がある。\nANOVA君を使う時は，関数anovakunに，データ，要因計画の型，各要因の水準の順で入力する。ここで要因計画の型とは，文字列でBetween/Withinの違いを明示することになる。被験者のラベルを表す小文字のsを挟んで，左側に間(Between)要因，右側に内(Within)要因を入れる。例えば一要因Between計画の場合は\"As\"，二要因Within計画の場合は\"sAB\"，間1内2の混合計画であれば\"AsBC\"のようにする。\n続いて入力する水準数は，要因の数だけ必要である。ただし，ロング型で入力した場合は自動的に水準数が計算されるので入力の必要がない。\nこのテキストでは，データの持ち替えについてすでに触れているので，色々扱いやすいロング型に整形して利用していくものとする。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#betweenデザイン",
    "href": "chapter09.html#betweenデザイン",
    "title": "9  多群の平均値差の検定",
    "section": "9.4 Betweenデザイン",
    "text": "9.4 Betweenデザイン\n\n9.4.1 1way-ANOVA\nもっとも単純な一要因3水準，Between計画の例から始めよう。仮想データの生成を行うことで，分散分析のメカニズムと共に見ていくことにする。\n\nset.seed(123)\n# 各群のサンプルサイズ\nn1 &lt;- 5\nn2 &lt;- 4\nn3 &lt;- 6\n# 母平均，効果量，母SD\nmu &lt;- 10\ndelta &lt;- 1\nsigma &lt;- 3\n# 群平均\nmu1 &lt;- mu - (delta * sigma)\nmu2 &lt;- mu\nmu3 &lt;- mu + (delta * sigma)\n# データセット\nX1 &lt;- rnorm(n1, mu1, sigma)\nX2 &lt;- rnorm(n2, mu2, sigma)\nX3 &lt;- rnorm(n3, mu3, sigma)\n## 組み上げる\ndat &lt;- data.frame(\n  ID = 1:(n1 + n2 + n3),\n  group = as.factor(rep(LETTERS[1:3], c(n1, n2, n3))),\n  value = c(X1, X2, X3)\n)\n## データの確認\ndat\n\n   ID group     value\n1   1     A  5.318573\n2   2     A  6.309468\n3   3     A 11.676125\n4   4     A  7.211525\n5   5     A  7.387863\n6   6     B 15.145195\n7   7     B 11.382749\n8   8     B  6.204816\n9   9     B  7.939441\n10 10     C 11.663014\n11 11     C 16.672245\n12 12     C 14.079441\n13 13     C 14.202314\n14 14     C 13.332048\n15 15     C 11.332477\n\n### 実行\nanovakun(dat, \"As\", long = TRUE, peta = TRUE)\n\n\n[ As-Type Design ]\n\nThis output was generated by anovakun 4.8.9 under R version 4.5.0.\nIt was executed on Fri Jun 13 11:32:26 2025.\n\n \n&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;\n\n------------------------------\n group   n     Mean    S.D. \n------------------------------\n     A   5   7.5807  2.4331 \n     B   4  10.1681  3.9548 \n     C   6  13.5469  1.9483 \n------------------------------\n\n\n&lt;&lt; ANOVA TABLE &gt;&gt;\n\n== This data is UNBALANCED!! ==\n== Type III SS is applied. ==\n\n--------------------------------------------------------------\n Source       SS  df      MS  F-ratio  p-value      p.eta^2 \n--------------------------------------------------------------\n  group  98.3840   2 49.1920   6.5897   0.0117 *     0.5234 \n  Error  89.5804  12  7.4650                                \n--------------------------------------------------------------\n  Total 187.9644  14 13.4260                                \n                  +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\n\n&lt;&lt; POST ANALYSES &gt;&gt;\n\n&lt; MULTIPLE COMPARISON for \"group\" &gt;\n\n== Shaffer's Modified Sequentially Rejective Bonferroni Procedure ==\n== The factor &lt; group &gt; is analysed as independent means. == \n== Alpha level is 0.05. == \n \n------------------------------\n group   n     Mean    S.D. \n------------------------------\n     A   5   7.5807  2.4331 \n     B   4  10.1681  3.9548 \n     C   6  13.5469  1.9483 \n------------------------------\n\n-------------------------------------------------------\n Pair     Diff  t-value  df       p   adj.p          \n-------------------------------------------------------\n  A-C  -5.9662   3.6062  12  0.0036  0.0108  A &lt; C * \n  B-C  -3.3789   1.9159  12  0.0795  0.0795  B = C   \n  A-B  -2.5873   1.4117  12  0.1834  0.1834  A = B   \n-------------------------------------------------------\n\n\noutput is over --------------------///\n\n\n出力結果は大きく分けて記述統計&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;と，分散分析表&lt;&lt; ANOVA TABLE &gt;&gt;，下位検定&lt;&lt; POST ANALYSES &gt;&gt;に分けられる。記述統計はデータが正しく読み込めているかどうかのチェックに使おう。\n一番のメインは分散分析表であり，平方和sum of squaresを自由度dfで割った，1自由度あたりのデータの散らばりを，群間と群内(誤差)との比で検証しているのが見て取れる。群間平方和が\\(98.38\\)，群内平方和が\\(89.58\\)であり，それぞれ自由度\\(2\\)(\\(3\\)水準\\(-1\\))と\\(12\\)(\\(\\sum_{j=1}^3 n_j-1\\))から生じているので，平均平方Mean Squaresがそれぞれ\\(49.19\\)と\\(7.47\\)である。この比が\\(6.5897\\)で，自由度\\(F(2,12)\\)のF分布においてこの値以上の極端な数字が出る確率が5%を下回っている(実に\\(p=0.0117\\)である)ため，統計的に有意であると判断できる。 分散分析表のTotalのところで，全体のSSが群間SS+群内SSに一致していること，自由度も全体df=群間df+群内dfになっていることを確認しておこう。\nまた，anovakun関数の引数としてpeta = TRUEを指定したが，これは偏\\(\\eta^2\\)(partial eta)と呼ばれる効果量を出力するためのオプションである。\n今回は分散分析の時点で統計的な有意差が認められたため(\\(F(2,12)=6.59, p &lt; 0.05, \\eta^2=0.52\\))，続いて下位検定が表示されている。ANOVA君は下位検定についても複数のオプションを持っているが，デフォルトではShafferの修正Bonferroni検定が行われる。詳しくは専門書(永田 and 吉田 1997)を参照してほしいが，概略を説明すると，検証すべき仮説の数で有意水準を分割するというBonferroniの方法を，競合する仮説の数も考慮して分母を調整するというものである。\nこの計算の結果，A群とC群の間にのみ統計的な有意差が確認された(\\(t(12)=3.61,p&lt;0.05\\))と言える。\n\n\n9.4.2 2way-ANOVA\n二要因の場合も見ておこう。ANOVA君の表記方法は要因計画の型が変わるだけで大きな変更はないが，交互作用interactionを考える必要があるところがポイントである。これも仮想データの組成を見ることでその意義がわかりやすくなるだろう。間2\\(\\times\\)間2の実験デザインを例に，まずは各水準の理論的平均値がどのようにつくられるかをみておこう。\n\nset.seed(123)\n# 各群のサンプルサイズ\nn &lt;- 10\n# 全体平均，効果量，母SD\nmu &lt;- 10\ndelta1 &lt;- 1\ndelta2 &lt;- 0 # ここではあえて要因Bの効果を0にしている\ndelta3 &lt;- 2\nsigma &lt;- 3\n# 効果の計算\neffectA &lt;- delta1 * sigma # Factor A\neffectB &lt;- delta2 * sigma # Factor B\neffectAB &lt;- delta3 * sigma # interaction\n# 各群の平均\nmu11 &lt;- mu + effectA + effectB + effectAB\nmu12 &lt;- mu + effectA - effectB - effectAB\nmu21 &lt;- mu - effectA + effectB - effectAB\nmu22 &lt;- mu - effectA - effectB + effectAB\n\n効果の現れ方は相対的だから，要因Aが第一水準に+effectAの形で現れたら，第二水準には-effectA の形で現れる。要因Bについても同様である。交互作用については組み合わせにおいて生じるから，要因Aの第一水準と要因Bの第一水準の組み合わせのところに+effectABを充てる。ここでも効果は相対的に現れるという条件を守るために，要因Aの第一水準の中で+effectABの効果を相殺するために，要因Aの第一水準と要因Bの第二水準の組み合わせの符号が反転する。同様に，要因Bの第一水準の中で相殺するために要因Aの第二水準と要因Bの第一水準には-effectABが加わる。\nこのようにして考えられる理論的平均値に対して，外乱要因である誤差が生じて実現値が得られる。 組み上げて得られたデータを確認しておこう。\n\nX11 &lt;- rnorm(n, mean = mu11, sd = sigma)\nX12 &lt;- rnorm(n, mean = mu12, sd = sigma)\nX21 &lt;- rnorm(n, mean = mu21, sd = sigma)\nX22 &lt;- rnorm(n, mean = mu22, sd = sigma)\ndat &lt;- data.frame(\n  ID = 1:(n * 4),\n  FactorA = rep(1:2, each = n * 2),\n  FactorB = rep(rep(1:2, each = n), 2),\n  value = c(X11, X12, X21, X22)\n)\ndat\n\n   ID FactorA FactorB      value\n1   1       1       1 17.3185731\n2   2       1       1 18.3094675\n3   3       1       1 23.6761249\n4   4       1       1 19.2115252\n5   5       1       1 19.3878632\n6   6       1       1 24.1451950\n7   7       1       1 20.3827486\n8   8       1       1 15.2048163\n9   9       1       1 16.9394414\n10 10       1       1 17.6630141\n11 11       1       2 10.6722454\n12 12       1       2  8.0794415\n13 13       1       2  8.2023144\n14 14       1       2  7.3320481\n15 15       1       2  5.3324766\n16 16       1       2 12.3607394\n17 17       1       2  8.4935514\n18 18       1       2  1.1001485\n19 19       1       2  9.1040677\n20 20       1       2  5.5816258\n21 21       2       1 -2.2034711\n22 22       2       1  0.3460753\n23 23       2       1 -2.0780133\n24 24       2       1 -1.1866737\n25 25       2       1 -0.8751178\n26 26       2       1 -4.0600799\n27 27       2       1  3.5133611\n28 28       2       1  1.4601194\n29 29       2       1 -2.4144108\n30 30       2       1  4.7614448\n31 31       2       2 14.2793927\n32 32       2       2 12.1147856\n33 33       2       2 15.6853770\n34 34       2       2 15.6344005\n35 35       2       2 15.4647432\n36 36       2       2 15.0659208\n37 37       2       2 14.6617530\n38 38       2       2 12.8142649\n39 39       2       2 12.0821120\n40 40       2       2 11.8585870\n\n\nもちろん実際には，計画に応じたデータセットが得られているはずであり，各群のサンプルサイズが異なるなどの事情もあるだろう。しかしこうして，理論的にデータの組成を見ておくことで，サンプルサイズを変えたり効果量を変えたりしながら，どのように結果が変わってくるかを確認しながら進めることができる2。\nそれではこの仮想データを分析してみよう。\n\nanovakun(dat, \"ABs\", long = TRUE, peta = TRUE)\n\n\n[ ABs-Type Design ]\n\nThis output was generated by anovakun 4.8.9 under R version 4.5.0.\nIt was executed on Fri Jun 13 11:32:26 2025.\n\n \n&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;\n\n-----------------------------------------\n FactorA  FactorB   n     Mean    S.D. \n-----------------------------------------\n       1        1  10  19.2239  2.8614 \n       1        2  10   7.6259  3.1142 \n       2        1  10  -0.2737  2.7924 \n       2        2  10  13.9661  1.5819 \n-----------------------------------------\n\n\n&lt;&lt; ANOVA TABLE &gt;&gt;\n\n---------------------------------------------------------------------------\n           Source        SS  df        MS  F-ratio  p-value      p.eta^2 \n---------------------------------------------------------------------------\n          FactorA  432.7854   1  432.7854  61.4190   0.0000 ***   0.6305 \n          FactorB   17.4478   1   17.4478   2.4761   0.1243 ns    0.0644 \nFactorA x FactorB 1668.9825   1 1668.9825 236.8545   0.0000 ***   0.8681 \n            Error  253.6721  36    7.0464                                \n---------------------------------------------------------------------------\n            Total 2372.8878  39   60.8433                                \n                               +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\n\n&lt;&lt; POST ANALYSES &gt;&gt;\n\n&lt; SIMPLE EFFECTS for \"FactorA x FactorB\" INTERACTION &gt;\n\n----------------------------------------------------------------------\n      Source        SS  df        MS  F-ratio  p-value      p.eta^2 \n----------------------------------------------------------------------\nFactorA at 1 1900.7730   1 1900.7730 269.7492   0.0000 ***   0.8823 \nFactorA at 2  200.9950   1  200.9950  28.5243   0.0000 ***   0.4421 \nFactorB at 1  672.5693   1  672.5693  95.4480   0.0000 ***   0.7261 \nFactorB at 2 1013.8610   1 1013.8610 143.8826   0.0000 ***   0.7999 \n       Error  253.6721  36    7.0464                                \n----------------------------------------------------------------------\n                          +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\noutput is over --------------------///\n\n\n基本的な結果の見方については，一要因のときと同じである。今回は要因Aと交互作用の効果を作り，正しく検出されている。下位検定については，要因Aが2水準であったためこちらの主効果の検証は必要なく(記述統計を見て群平均比較をすればよい)，交互作用についての単純効果の検証が行われている。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#withinデザイン",
    "href": "chapter09.html#withinデザイン",
    "title": "9  多群の平均値差の検定",
    "section": "9.5 Withinデザイン",
    "text": "9.5 Withinデザイン\nWithinデザインは対応のあるt検定の時と同じように，多次元正規分布からの生成として考えよう。すなわち各個体から得られるデータが相関しているという仮定をおくのである。以下のサンプルコードを読んで，データ生成過程を確認しよう。なお共分散は\\(\\rho_{xy}=\\frac{s_{xy}}{s_xs_y}\\)より\\(s_{xy}=\\rho_{xy}s_xs_y\\)として整形している。\n\npacman::p_load(tidyverse)\npacman::p_load(MASS)\nset.seed(42)\n# 各群のサンプルサイズ\nn &lt;- 10\n# 全体平均，効果量，母SD\nmu &lt;- 10\ndelta &lt;- 1\ns1 &lt;- s2 &lt;- s3 &lt;- 1\nrho12 &lt;- 0.1\nrho13 &lt;- 0.3\nrho23 &lt;- 0.8\nmus &lt;- c(mu, mu + s1 * delta, mu - s1 * delta)\n# 共分散行列の生成\nSigma &lt;- matrix(NA, ncol = 3, nrow = 3)\nSigma[1, 1] &lt;- s1^2\nSigma[2, 2] &lt;- s2^2\nSigma[3, 3] &lt;- s3^2\nSigma[1, 2] &lt;- Sigma[2, 1] &lt;- rho12 * s1 * s2\nSigma[1, 3] &lt;- Sigma[3, 1] &lt;- rho13 * s1 * s3\nSigma[2, 3] &lt;- Sigma[3, 2] &lt;- rho23 * s2 * s3\n# データの生成\nX &lt;- mvrnorm(n, mus, Sigma) %&gt;% as.data.frame()\n# データの確認\nX\n\n          V1        V2       V3\n1  10.625304  9.418518 7.493325\n2  12.437964 11.249993 8.806719\n3   8.604481 11.182418 8.722798\n4   9.390742 10.181310 8.786312\n5   9.567609 10.147592 9.194809\n6  10.651739 11.005419 8.917299\n7   9.125913  9.805634 7.511082\n8   7.770294 12.462671 8.790231\n9   6.909722  9.863405 7.429485\n10 11.267590 10.798088 8.754522\n\n# Long型に整形\nX &lt;- X %&gt;%\n  rowid_to_column(\"ID\") %&gt;%\n  pivot_longer(-ID) %&gt;%\n  print()\n\n# A tibble: 30 × 3\n      ID name  value\n   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1 V1    10.6 \n 2     1 V2     9.42\n 3     1 V3     7.49\n 4     2 V1    12.4 \n 5     2 V2    11.2 \n 6     2 V3     8.81\n 7     3 V1     8.60\n 8     3 V2    11.2 \n 9     3 V3     8.72\n10     4 V1     9.39\n# ℹ 20 more rows\n\n# 分析の実行\nanovakun(X, \"sA\", long = TRUE, peta = TRUE, GG = TRUE)\n\n\n[ sA-Type Design ]\n\nThis output was generated by anovakun 4.8.9 under R version 4.5.0.\nIt was executed on Fri Jun 13 11:32:27 2025.\n\n \n&lt;&lt; DESCRIPTIVE STATISTICS &gt;&gt;\n\n-----------------------------\n name   n     Mean    S.D. \n-----------------------------\n   V1  10   9.6351  1.6609 \n   V2  10  10.6115  0.9057 \n   V3  10   8.4407  0.6777 \n-----------------------------\n\n\n&lt;&lt; SPHERICITY INDICES &gt;&gt;\n\n== Mendoza's Multisample Sphericity Test and Epsilons ==\n\n-------------------------------------------------------------------------\n Effect  Lambda  approx.Chi  df      p         LB     GG     HF     CM \n-------------------------------------------------------------------------\n   name  0.0068      8.8720   2 0.0118 *   0.5000 0.5988 0.6392 0.5547 \n-------------------------------------------------------------------------\n                              LB = lower.bound, GG = Greenhouse-Geisser\n                             HF = Huynh-Feldt-Lecoutre, CM = Chi-Muller\n\n\n&lt;&lt; ANOVA TABLE &gt;&gt;\n\n--------------------------------------------------------------\n  Source      SS  df      MS  F-ratio  p-value      p.eta^2 \n--------------------------------------------------------------\n       s 16.4609   9  1.8290                                \n--------------------------------------------------------------\n    name 23.6422   2 11.8211  10.7022   0.0009 ***   0.5432 \ns x name 19.8819  18  1.1045                                \n--------------------------------------------------------------\n   Total 59.9849  29  2.0684                                \n                  +p &lt; .10, *p &lt; .05, **p &lt; .01, ***p &lt; .001\n\n\n&lt;&lt; POST ANALYSES &gt;&gt;\n\n&lt; MULTIPLE COMPARISON for \"name\" &gt;\n\n== Shaffer's Modified Sequentially Rejective Bonferroni Procedure ==\n== The factor &lt; name &gt; is analysed as dependent means. == \n== Alpha level is 0.05. == \n \n-----------------------------\n name   n     Mean    S.D. \n-----------------------------\n   V1  10   9.6351  1.6609 \n   V2  10  10.6115  0.9057 \n   V3  10   8.4407  0.6777 \n-----------------------------\n\n----------------------------------------------------------\n  Pair     Diff  t-value  df       p   adj.p            \n----------------------------------------------------------\n V2-V3   2.1708   9.5342   9  0.0000  0.0000  V2 &gt; V3 * \n V1-V3   1.1945   2.3896   9  0.0406  0.0406  V1 &gt; V3 * \n V1-V2  -0.9764   1.6250   9  0.1386  0.1386  V1 = V2   \n----------------------------------------------------------\n\n\noutput is over --------------------///\n\n\n上記コードについていくつか解説をしておこう。 今回，各群の分散は同じにしつつ，変数間相関に大きな違いを持たせた。あえて球面性の仮定が成立しないような例を得たかったからで，出力の&lt;&lt; SPHERICITY INDICES &gt;&gt;をみると統計量\\(\\lambda\\)のあとの\\(p\\)値が5%を下回っており，「球面性が成立している」という帰無仮説が棄却されていることがわかる。この時いくつかの補正法があるが，今回はGreenhouse-Geisserの補正を当てることにしている。それがanovakun関数のなかのGG=TRUEの箇所である。\nこれを踏まえて分散分析表が示されている。ここでも全体平方和SS，自由度dfが各行の要素の和になっていることが確認できるが，その因子名のところにsが含まれていることが確認できる。これが個体ごとの変動を表しており，誤差から個人差を取り除いて効果の検証ができていることがわかる。\n分散分析は加法的，線形的な分解であるからわかりやすく，要因が複雑に組み合わさることがあっても基本的に今回のパーツを組み上げることで理解できる。言い換えると，データが先にある実践的な場合には平方和をひとつひとつ丁寧に紐解いていくことで理解できる。実にanovakunの前進であるanova4では4要因，anovakunでは26要因までのデザインを分析することが可能である。もっとも4要因計画にもなると3次の交互作用まで考えられ，主効果と合わせてこれらの交互作用効果を解釈するのは困難である。anoakunは2次以上の交互作用が見られた場合，自動的に下位検定を行ってくれないので，要因の水準ごとにデータを分割して，分散分析表を解体しつつ分析する必要がある。3\nしかしもちろん，これには検定の多重性の問題が関わってくるから，あまり推奨される手法ではない。ごく少ない要因で，主効果の有無を検証することを主眼においた丁寧な実験デザインを組み立てることを試みるべきである。\nまたここでは，仮想データを作ることで，得られたデータの背後にある生成メカニズムに注目した。「与えられたデータを分解する」のが分散分析であるのに対し，リバースエンジニアリングからアプローチしたのである。こうすることで，分散分析の見えない仮定に注意が向くことを期待している。簡便のために，いくつかのパラメータを均質化するなどしたが，実践的には群ごとのサンプルサイズが異なることも少なくないだろうし，群間の分散や共分散が均質であることを前提とするのは難しいだろう。これを考慮した細かい作り込みも，リバースエンジニアリングによって生成メカニズムがわかっている場合には応用が可能である。さらに，どこの水準間にどのような効果があると仮定されるか，といった精緻な仮説があるのなら，そこだけをターゲットにした分析を行うことも可能である。\n分散分析は，あくまでも大雑把な全体的傾向を見るためのものであることに留意しよう。心理学のデータがより精緻な仮定に耐えうる精度を持つものになれば，分散分析は過去の遺物となるかもしれない。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#課題",
    "href": "chapter09.html#課題",
    "title": "9  多群の平均値差の検定",
    "section": "9.6 課題",
    "text": "9.6 課題\n\n以下のデータセットは一要因4水準Between計画で得られたものです。分散分析を行って，要因の効果があるかどうか，水準間に差があるとすればどこに見られるかを報告してください。なおこのデータセットはこちらex_anova1.csvからダウンロード可能です。\n\n\n\n   ID group value\n1   1     A 14.37\n2   2     A 15.11\n3   3     A 16.11\n4   4     A 11.17\n5   5     A 14.51\n6   6     A  7.85\n7   7     A 10.65\n8   8     B 16.45\n9   9     B 11.76\n10 10     B 19.11\n11 11     B 19.62\n12 12     C  2.92\n13 13     C  6.27\n14 14     C  1.82\n15 15     C -0.10\n16 16     C  5.30\n17 17     C  1.57\n18 18     D  8.33\n19 19     D  2.71\n20 20     D  5.97\n21 21     D  4.97\n22 22     D  1.65\n23 23     D  8.73\n24 24     D  5.93\n25 25     D  4.27\n\n\n\n以下のデータセットは一要因4水準Within計画で得られたものです。分散分析を行って，要因の効果があるかどうか，水準間に差があるとすればどこに見られるかを報告してください。なおこのデータセットはこちらex_anova2.csvからダウンロード可能です。\n\n\n\n      V1    V2    V3    V4\n1  11.32 12.99  9.34 -0.14\n2  10.77 13.84 14.74  3.52\n3   9.86 12.26 12.56  2.60\n4   8.74 11.59 14.27  0.68\n5  11.12 12.93 12.92  1.13\n6   9.65 16.55 12.60  2.32\n7   9.72 14.64  9.69 -1.34\n8  12.02 11.18 14.43  2.64\n9  10.00 10.79  9.19 -1.09\n10 10.04 15.53 13.38  1.82\n11 10.20 11.56 11.02 -0.05\n12  7.81  9.29 12.20 -3.25\n\n\n\n間(3)\\(\\times\\)間(3)の分散分析モデルの仮想データセットを作りましょう。そのデータに分散分析を適用し，仮定した要因の効果がみられるか(あるいは効果がないと仮定した場合に正しく検出されないか)を確認しましょう。\n【発展課題】二要因混合計画分散分析(間\\(\\times\\)内)の仮想データセットを作りましょう。そのデータに分散分析を適用し，仮定した要因の効果がみられるか(あるいは効果がないと仮定した場合に正しく検出されないか)を確認しましょう。\n\n\n\n\n\n永田靖, and 吉田道弘. 1997. 統計的多重比較法の基礎. サイエンティスト社.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter09.html#footnotes",
    "href": "chapter09.html#footnotes",
    "title": "9  多群の平均値差の検定",
    "section": "",
    "text": "2024.03.17時点での最新バージョンが4.8.9である。リンク先URLは，公式サイトからソースファイルのリンクをコピーして貼り付けると良い。↩︎\nかつては分散分析は手計算でできる分析モデルであり，得られたデータを平方和に分解していくプロセスをたどりながら分散分析のメカニズムが体得されるという教育が多く見られた。ただしその方法は計算に時間がかかること，ミスが混在しやすいことに加え，手元のデータが唯一無二のものであるという印象を強くすることが懸念される。推測統計学においては，手元のデータはあくまでも実現値に過ぎないと考えるのであり，乱数を生成して幾つでも自在に作り出せる経験を得た方が教育効果として良いのではないか，と筆者は考えている。↩︎\nanovakunの補助関数anovatanを用いることで，注目したい要因ごとにデータを分割してくれる。詳しくは公式サイトのマニュアルを参照してほしい。↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>多群の平均値差の検定</span>"
    ]
  },
  {
    "objectID": "chapter10.html",
    "href": "chapter10.html",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "10.1 疑わしき研究実践 Questionable Research Practicies\nここまでシミュレーションを通じて仮想データを生成し，帰無仮説検定のステップをリバースエンジニアリングしながら検定を「モデル」の観点から確認してきた。\nシミュレーションは仮想世界を作ることであり，いかようにもデータを作ることができるのだから，たとえば実践的にタブーとされていることを仮想的に検証してみることができる。このアプローチで，QRPsが具体的にどのように問題になるのかを体験してみよう。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#疑わしき研究実践-questionable-research-practicies",
    "href": "chapter10.html#疑わしき研究実践-questionable-research-practicies",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "10.1.1 検定の繰り返し\n帰無仮説検定は確率を伴った判断なので，「差がないのにあると判断してしまった(タイプ1エラー)」とか，「差があるのに検出できなかった(タイプ2エラー)」といった問題が生じうる。すでに述べたように，タイプ2エラーの方は本質的に知り得ないので(差がどの程度あるか，事前にわかっていることがない)，せめてタイプ1エラーは制御することを目指すことになる。\nこうした検定は合理的に行われるべきもので，なんとか有意に「したい」といった研究者のお気持ちとは独立しているはずである。しかし(もしかすると)意図せぬところで，この制御に失敗してしまっている可能性がある。\nひとつは検定の繰り返しに関する問題である。たとえば分散分析において，「主効果が出てから下位検定で各ペアの検証をするんだから，最初から各ペアのt検定を繰り返せばいいじゃないか」と考える人がいるかもしれない。これで本当に問題ないのか，シミュレーションで確認してみよう。\n以下のコードは，有意差のないデータセットを作り，1.分散分析を行なって有意になるかどうか，2.各ペアについて繰り返しt検定を行い，どこかに有意差が検出されるかどうか，を比較している。分散分析はANOVA君ではなく，R固有のaov関数を用いた1。また，「どこかに有意差が検出される」をif文を使って書いているところを，注意深く確認しておいてほしい。\n\npacman::p_load(tidyverse)\npacman::p_load(broom) # 分析結果をtidyに整形するパッケージ。ない場合はinstallしておこう\n\n\nalpha &lt;- 0.05 # 有意水準を0.05に設定\nn1 &lt;- n2 &lt;- n3 &lt;- 10 # 各グループのサンプルサイズを10に設定\nmu &lt;- 10 # 平均値を10に設定\nsigma &lt;- 2 # 標準偏差を2に設定\n\nmu1 &lt;- mu2 &lt;- mu3 &lt;- mu # 各グループの平均値を同じに設定\n\nset.seed(12345) # 乱数のシードを設定して再現性を確保\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\n\nanova.detect &lt;- rep(NA, iter) # ANOVA検出結果の保存用ベクトルを初期化\nttest.detect &lt;- rep(NA, iter) # t検定検出結果の保存用ベクトルを初期化\n\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  X1 &lt;- rnorm(n1, mu1, sigma) # グループ1のデータを生成\n  X2 &lt;- rnorm(n2, mu2, sigma) # グループ2のデータを生成\n  X3 &lt;- rnorm(n3, mu3, sigma) # グループ3のデータを生成\n\n  dat &lt;- data.frame( # データフレームを作成\n    group = c(rep(1, n1), rep(2, n2), rep(3, n3)), # グループ番号を追加\n    value = c(X1, X2, X3) # データを追加\n  )\n  result.anova &lt;- aov(value ~ group, data = dat) %&gt;% tidy() # ANOVAを実行し結果を整形\n  anova.detect[i] &lt;- ifelse(result.anova$p.value[1] &lt; alpha, 1, 0) # 有意差があるかを判定して保存\n\n  # t検定を繰り返す\n  ttest12 &lt;- t.test(X1, X2)$p.value # グループ1と2のt検定\n  ttest13 &lt;- t.test(X1, X3)$p.value # グループ1と3のt検定\n  ttest23 &lt;- t.test(X2, X3)$p.value # グループ2と3のt検定\n\n  ttest.detect[i] &lt;- ifelse(ttest12 &lt; alpha | ttest13 &lt; alpha | ttest23 &lt; alpha, 1, 0) # いずれかのt検定で有意差があれば保存\n}\n\nttest.detect %&gt;% mean() # t検定で有意差が検出された割合を計算\n\n[1] 0.109\n\nanova.detect %&gt;% mean() # ANOVAで有意差が検出された割合を計算\n\n[1] 0.04\n\n\n結果を見ると，t検定で有意差が検出された確率が0.109であり，設定した\\(\\alpha\\)水準を大きく上回っていることがわかる。有意でないところに有意差を見出しているのだから，これはタイプ1エラーのインフレである。分散分析で検出された結果は0.04であり，正しく\\(\\alpha\\)水準がコントロールできている。\n検定を繰り返すことの問題は，確率的判断にある。5%の水準でタイプ1エラーが起こるということは，95%の確率で正しく判断できるということだが，2回検定を繰り返すとその精度は\\((1-0,05)^2=0.9025\\)であり，3回検定を繰り返すと\\((1-0.05)^3=0.857375\\)と，どんどん小さくなっていってしまう。検定はタイプ1エラーのハンドリングが目的であったことを忘れてはならない。\n\n\n10.1.2 ボンフェロー二の方法\n一つの論文のなかに複数の研究(Study1, Study2,…)があり，それぞれで検定による確率的判断を行っているとしよう。それぞれ別のデータセットに対する検定であっても，一つの露文の中で確率的判断が繰り返されていることに違いはない。このような場合は，どのようにして有意水準をコントロールすれば良いのだろうか。\n最も単純明快な方法のひとつは，分散分析の下位検定でもみられたBonferroniの補正である。すなわち，検定の回数で有意水準を割ることで，検定を厳しくするのである。5%水準の検定を5回繰り返すのなら，\\(0.05/5=0.01\\)とすることで全体的なタイプ1エラー率を抑制するのである。これが正しく機能するかどうか，シミュレーションで確認してみよう。\n反復してデータを生成することになるので，仮想データ生成関数を別途事前に準備しておこう。\n\n# シミュレーション用の関数を定義\nstudyMake &lt;- function(n, mu, sigma, delta) {\n  X1 &lt;- rnorm(n, mu, sigma) # グループ1のデータを生成\n  X2 &lt;- rnorm(n, mu + sigma * delta, sigma) # グループ2のデータを生成（平均値が異なる）\n  dat &lt;- data.frame( # データフレームを作成\n    group = rep(1:2, each = n), # グループ番号を追加\n    value = c(X1, X2) # データを追加\n  )\n  result &lt;- t.test(X1, X2)$p.value # グループ間のt検定を実行\n  return(result) # p値を返す\n}\n\nこの関数は，引数としてサンプルサイズn，平均値mu，標準偏差sigma，効果量deltaをとり，2群のt検定の結果である\\(p\\)値を返す関数である。\n\n# 使用例；t検定の結果のp値が戻ってくる\nstudyMake(n = 10, mu = 10, sigma = 1, delta = 0)\n\n[1] 0.9444895\n\n\nこれ一回で1分析するので，これを複数回行って一つの研究とし，一つの論文のなかでnum_studies回の研究を行ったとしよう。今回はnum_studies = 3としている。Rのreplicate関数で研究回数繰り返した\\(p\\)値ベクトルを得て，どこかに差が検出されるかどうかをチェックする。「どこかに」を表現するためにany関数を使って判定する。判定する有意水準として，\\(\\alpha\\)と補正をかけた\\(\\alpha_{adj}\\)の2つを用意した。\n\nset.seed(12345) # 乱数のシードを設定して再現性を確保\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\nalpha &lt;- 0.05 # 有意水準を0.05に設定\nnum_studies &lt;- 3 # 研究の数を3に設定\nalpha_adjust &lt;- alpha / num_studies # 多重検定補正後の有意水準を計算\n\nFLG.detect &lt;- rep(NA, iter) # 検出結果を保存するベクトルを初期化\nFLG.detect.adj &lt;- rep(NA, iter) # 補正後の検出結果を保存するベクトルを初期化\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  p_values &lt;- replicate(num_studies, studyMake(n = 10, mu = 10, sigma = 1, delta = 0)) # 各研究のp値を生成\n  FLG.detect[i] &lt;- ifelse(any(p_values &lt; alpha), 1, 0) # 補正前の有意差検出を判定して保存\n  FLG.detect.adj[i] &lt;- ifelse(any(p_values &lt; alpha_adjust), 1, 0) # 補正後の有意差検出を判定して保存\n}\n\nFLG.detect %&gt;% mean() # 補正前の有意差検出率を計算\n\n[1] 0.145\n\nFLG.detect.adj %&gt;% mean() # 補正後の有意差検出率を計算\n\n[1] 0.049\n\n\n結果を見ると，\\(\\alpha\\)水準のまま検定を行うと，論文全体でのタイプ1エラー率が0.145と5%を上回っており，3つの研究のどこかで間違った判断をしていることがわかる。補正すると0.049と正しく制御されている。\n一連の研究をまとめた一つの論文に，複数の研究が含まれていることは少なくない。各検定結果をまとめて総合考察とすることも一般的である。総合考察は各分析結果から全体的な結論を導くのだが，その要素のどこかに間違いがあると，全体の論立てが崩れてしまうことにもなりかねない。いわば腐った支柱が紛れ込んでいる土台の上に家屋を建てるようなもので，研究の積み重ねを目的とする科学活動の一環である以上，正しく制御されていることは重要である。\n\n\n10.1.3 N増し問題\n人間を対象にした研究を行って，データを一生懸命取る。その結果，効果があると見られた操作/介入から統計的な有意差が検出されなければ，「悔しい」という心情になることは理解できる。もう少し実験を工夫すれば，もう少しデータが違えばよかったのでは，と思うかもしれない。ではもう少し頑張ってデータを増やしてみればどうだろうか。\n実はこの考え方はQRPsのひとつである。検定は真偽判定をする競技のようなものなので，ゲームの途中でプレイヤーの人数が変わるのはよろしくない。このことをシミュレーションで確認してみよう。\n以下のコードは，t検定のデータを最初n1=n2=10で作成して行っている。タイプ1エラーの検証をするので，効果量は\\(0\\)である。ここでt検定を行い，もしその\\(p\\)値が\\(\\alpha\\)よりも大きかったら，つまり有意であると判断されなかったら，同じ方法でデータを1件追加する。そしてまたt検定を行う。このサンプルの追加は，効果量\\(0\\)なので，偶然のお許しが出るまでいつまで経っても終わることがないため，上限を100にしてある。上限に達したら流石に諦めてもらうとして，さてそうしたQRPsな努力の結果，\\(\\alpha\\)水準はどれぐらいに保たれているだろうか。\n\niter &lt;- 1000 # シミュレーションの繰り返し回数を1000に設定\nalpha &lt;- 0.05 # 有意水準を0.05に設定\np &lt;- rep(0, iter) # p値を保存するベクトルを初期化\nadd.vec &lt;- rep(0, iter) # 増やした人数を保存するベクトルを初期化\n\nset.seed(123) # 乱数のシードを設定して再現性を確保\n\nn1 &lt;- n2 &lt;- 10 # 各グループのサンプルサイズを10に設定\nmu &lt;- 10 # 平均値を10に設定\nsigma &lt;- 2 # 標準偏差を2に設定\ndelta &lt;- 0 # 平均の差を0に設定\n\n## シミュレーション本体\nfor (i in 1:iter) { # 1000回のシミュレーションを繰り返すループ\n  # 最初のデータを生成\n  Y1 &lt;- rnorm(n1, mu, sigma) # グループ1のデータを生成\n  Y2 &lt;- rnorm(n2, mu + sigma * delta, sigma) # グループ2のデータを生成\n  p[i] &lt;- t.test(Y1, Y2)$p.value # t検定を実行しp値を保存\n  # データを追加する\n  count &lt;- 0 # 追加したデータの数をカウント\n  ## p値が5%を下回るか、データが100になるまでデータを増やし続ける\n  while (p[i] &gt;= alpha && count &lt; 100) { # 条件を満たすまでループを繰り返す\n    # 有意でなかった場合、変数ごとに1つずつデータを追加\n    Y1_add &lt;- rnorm(1, mu, sigma) # グループ1に新しいデータを1つ追加\n    Y2_add &lt;- rnorm(1, mu + sigma * delta, sigma) # グループ2に新しいデータを1つ追加\n    Y1 &lt;- c(Y1, Y1_add) # グループ1のデータを更新\n    Y2 &lt;- c(Y2, Y2_add) # グループ2のデータを更新\n    p[i] &lt;- t.test(Y1, Y2)$p.value # 新しいデータでt検定を再度実行しp値を更新\n    count &lt;- count + 1 # データを追加した回数をカウント\n  }\n  add.vec[i] &lt;- count\n}\n\n## 結果\nifelse(p &lt; 0.05, 1, 0) |&gt; mean() # p値が5%未満の割合を計算\n\n[1] 0.306\n\nhist(p)\n\n\n\n\n\n\n\nhist(add.vec)\n\n\n\n\n\n\n\n\n結果をみると，0.306とかなり逸脱して，誤った結論に辿り着いていることがわかる。努力の結果得られた有意差は，偶然の賜物でもあり，誤った研究実践による幻想にすぎない。加えたデータのヒストグラムからわかるように，悲しいかな，75%もの割合で上限100まで達してしまう。百害あって一利なしとはこのことである。\n\n\n10.1.4 サンプルサイズを事前に決めないことの問題\nサンプルサイズを事前に決めずに検定する，という状況を別の角度から見てみよう。クルシュケ ([2014] 2017) は「コインフリップを24回して，うち7回表が出た」というシーンを例に挙げて説明している。7/24は半分を下回っているから，やや裏が出やすいコインであるように思える。帰無仮説として，このコインは公平である(表と裏が出る確率が半々である)，というのを検証したいとする。\nこの24回中7回成功，という話の背後に「24施行する」ということを決めていたかどうか(サンプルサイズを事前に決めていたか)ということを考えてみよう。\nまずは正直に，最初から24回コインフリップすることを決めていたとする。コインフリップはベルヌーイ試行2 であり，それを繰り返すので二項分布に従うと考えられる。そこで，二項検定として次のように計算できるだろう。\n\nN &lt;- 24\n# 7回表が出る確率\npbinom(7, N, 0.5) * 2\n\n[1] 0.06391466\n\n\n二項分布のp値を出すにはpbinomを使いった。また帰無仮説として，このコインフリップは公平であると考えているのだから，\\(\\theta=0.5\\)が帰無仮説の状態である。この\\(\\theta=0.5\\)とした時に，\\(N=24,k=7\\)という結果になる確率を計算し，かつ両側検定(公平でない，が対立仮説なので裏が7回でもよい)であることを考えて確率を2倍した。p値は0.0639147であるから，5%水準では有意であると判定できない。これぐらいの確率はあるということだ。\nしかしここで第二の状況を考えてみよう。24回コインフリップすることを決めていたのではなくて，7回成功するまでコインフリップを続けたところ，結果的に24回で終わったということだった，とするのである。このようなシーンの確率分布は負の二項分布と呼ばれ，pnbinomで次のように計算できる。\n\nk &lt;- 7\n# 24回以上必要な確率\npnbinom(k, 24, 0.5) * 2\n\n[1] 0.003326893\n\n\nこの結果から，\\(\\theta=0.5\\)の時に7回表がでるまでに24施行も必要とする確率は，0.0033269だから，5%水準で有意である。つまり，滅多にこんなことが起きないので，\\(\\theta=0.5\\)という帰無仮説が疑わしいことになる。ここではシーンが異なるとp値が違っている，ということに注意してほしい。\nさらに第3のシーンを考えよう。これは「何回やるかは決めてないけど，まあ5分ぐらいかな」と試行にかける時間だけ決めていたという状況である。結果的に24回になったけど，もしかすると23回だったかもしれないし，25回や20回，30回だったかもしれない。これをシミュレーションするために，「24がピークになるような頻度の分布」をポアソン分布を使って生成する3。\n4 ポアソン分布は正の整数を実現値に取る分布で，カウント変数の確率分布として用いられる。パラメータは\\(\\lambda\\)だけであり，期待値と分散が\\(\\lambda\\)に一致する，非常にシンプルな分布である。\n\nset.seed(12345)\niter &lt;- 100000 # 発生させる乱数の数\n## 24回がピークに来るトライアル回数\ntrial &lt;- rpois(iter, 24)\nhist(trial)\n\n\n\n\n\n\n\n\nこの各トライアルにおいて，二項分布で成功した回数を計算し，トライアル回数で割ることによって，表が出る確率のシミュレーションができる。その時の割合は，\\(7/24\\)よりもレアな現象だろうか?\n\nresult &lt;- rep(NA, iter)\nfor (i in 1:iter) {\n  result[i] &lt;- rbinom(1, trial[i], 0.5) / trial[i]\n}\n## 7/24よりも小さい確率で起こった?\nlength(result[result &lt; (7 / 24)]) / iter\n\n[1] 0.02262\n\n\nこれを見ると，両側検定にしても0.04524なので，ギリギリ有意になるかどうか，というところだろうか。\nさて判断にこまった。「24回やる」と決めていたのであれば\\(\\theta=0.5\\)は棄却されないし，「7回成功するまで」と決めていたのであれば\\(\\theta=0.5\\)は棄却される。「5分間」と決めていても棄却されるが，そもそもこうした実験者の意図によって判断が揺らいで良いものだろうか?というのが クルシュケ ([2014] 2017) の指摘する疑問点である。\n問題は，「24回中7回成功」という事実に，二項分布，負の二項分布，あるいは組み合わさった分布のような，確率分布の情報が含まれていないことにある。この確率分布はデータが既知で母数が未知だから尤度関数であり，データ生成メカニズムであるとも言えるだろう。想定するメカニズムが明示されない検定は，ともすれば事後的に「実は負の二項分布を想定していたんですよ，へへ」ということも可能になってしまう。こうした点からも，研究者の自由度をなるべく少なくする研究計画の事前登録制度が必要であることがわかる。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#サンプルサイズ設計",
    "href": "chapter10.html#サンプルサイズ設計",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "10.2 サンプルサイズ設計",
    "text": "10.2 サンプルサイズ設計\nどのようにデータを取り，どのように分析・検定し，どのような基準で判断するかを事前に決めることに加え，事前にサンプルサイズを見積もっておく必要があるだろう。サンプルはとにかく多ければ多いほど良いか，というとそうではなく，過剰にサンプルを集めることは研究コストの増大であり，回答者の負担増でしかない。またサンプルサイズが大きくなると有意差を検出しやすくなるが，必要なのは有意差ではなく実質的に効果を見積もることであり，有意差が見つかれば良いというものではないことに注意が必要である。もちろん上で見てきたように，有意差が検出できるかどうかを指標にしてサンプルサイズを変動させてしまうのは，明らかに誤った研究実践である。\n事前にサンプルサイズを決定するのに必要なのは，これまでのリバースエンジニアリングの演習例からもわかるように，効果量の見積もりである。5 これをどのように定めるかについては，先行研究を考えるとか，研究領域で「これぐらい差がないと意味がないよね」とコンセンサスが取れている程度で決めることになる。6\n\n10.2.1 対応のないt検定\nサンプルサイズの設計には，これまで使ってきた検定統計量に非心度non-centrality parameterというパラメータを加えて考える必要がある。\n具体的に，対応のないt検定を例にサンプルサイズ設計の方法を見てみよう。t検定は言葉の通り，t分布を用いて帰無仮説の元での検定統計量の実現値が問題になるのであった。帰無仮説は\\(\\mu_0 = \\mu_1-\\mu_2 = 0\\)であり，検定統計量は次式で表されるのであった。\n\\[ T = \\frac{d - \\mu_0}{\\sqrt{U^2_p/\\frac{n_1n_2}{n_1+n_2}}}\\]\nこの分子において，\\(d - \\mu_0 = (\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)\\)の第二項を,\\(\\mu_1-\\mu_2 = 0\\)と仮定するから，\\(0\\)が中心の標準化されたt分布が用いられたのである。帰無仮説はこのように理論的に特定できる比較点をもとに置かれているのであって，帰無仮説下ではない現実の世界では，検定統計量は母平均の差\\(\\mu_1 - \\mu_2\\)に応じた分布から生じている。このように中心がずれているt分布のことを非心分布といい，ズレの程度が非心度パラメータである。t検定における非心度\\(\\lambda\\)は，以下の式で表される。\n\\[ \\lambda = \\frac{(\\mu_1-\\mu2) - \\mu_0}{\\sigma\\sqrt{n}} \\]\nこの非心度の分だけ，非心t分布はt分布からズレていることになる。Rではdt関数にncpパラメータがあり，デフォルトではncp=0になっていた。これを変えて描画してみよう。\n\n# データの準備\ndf &lt;- 10 # 自由度を指定\n# ggplotでプロット\nggplot(data.frame(x = c(-5, 5)), aes(x = x)) +\n  stat_function(fun = dt, args = list(df = df, ncp = 0), aes(color = \"ncp=0\")) +\n  stat_function(fun = dt, args = list(df = df, ncp = 3), aes(color = \"ncp=3\")) +\n  labs(\n    title = \"非心t分布\",\n    x = \"x\",\n    y = \"密度\",\n    color = \"ncp\"\n  )\n\n\n\n\n\n\n\n\nncp=0の時は，中心が0にある帰無仮説の世界であり，これを使ってタイプ1エラー，つまり\\(\\alpha\\)が算出されたのであった。ncpを効果量で表現すれば，母平均の差がゼロでない時の分布が描けるのだから，タイプ2エラー，つまり\\(\\beta\\)が計算できる。\n自由度df = 10，非心度ncp = 3の例で考えてみよう。 タイプ1エラーになるのは，自由度10のt分布で上2.5%の臨界値以上の実現値が得られた時である。\n\nqt(0.975, df = 10, ncp = 0)\n\n[1] 2.228139\n\n\nこのとき，実際はncp = 3ほどずれていたのだから，タイプ2エラーが生じる確率は次のとおりである。\n\nqt(0.975, df = 10, ncp = 0) %&gt;% pt(df = 10, ncp = 3)\n\n[1] 0.2285998\n\n\n当然，ncp = 0から離れるほどタイプ2エラーは生じにくくなる。非心度は母効果量\\(\\delta = \\frac{(\\mu_1 - \\mu_2)-\\mu_0}{\\delta}\\)を使って，\\(\\lambda = \\delta \\sqrt{n}\\)で表すことができる。\nこれを使って，t検定のサンプルサイズを設計してみよう。話を簡単にするために，サンプルサイズは2群で等しいものとする。\n検定統計量の式を思い出して，\\(\\sqrt{n}\\)にあたるところは2群のサンプルサイズから計算される，プールされた標本サイズから得られることに注意しよう7。\n\nalpha &lt;- 0.05\nbeta &lt;- 0.2\ndelta &lt;- 0.5\n\nfor (n in 10:1000) {\n  df &lt;- n + n - 2\n  lambda &lt;- delta * (sqrt((n * n) / (n + n)))\n  cv &lt;- qt(p = 1 - alpha / 2, df = df) # Type1errorの臨界値\n  er &lt;- pt(cv, df = df, ncp = lambda) # Type2errorの確率\n  if (er &lt;= beta) {\n    break\n  }\n}\nprint(n)\n\n[1] 64\n\n\nここでは，サンプルサイズを10から徐々に増やしていき，1000までの間で目標とする\\(\\beta\\)まで抑えられたところで，カウントしていくforループをbreakで脱出する，というかたちで組んでいる。結果的に，各群64名，合計128名のサンプルがあれば，目標が達成できることがわかる。サンプルサイズが2群で異なる場合など，詳細は@kosugi2023 に詳しい。\n\n\n10.2.2 シミュレーションによるサンプルサイズ設計\n非心F分布を使えば分散分析でもサンプルサイズができるし，そのほかの検定についても同様に非心分布を活用すると良い。しかし，非心分布の理解や非心度の計算など，ケースバイケースで学ぶべきことは多い。\nそこで，電子計算機の演算力をたのみに，データ生成のシミュレーションを通じて設計していくことを考えてみよう。サンプルサイズや効果量を定めれば，仮想データを作ることができるし，それ対して検定をかけることもできる。仮想データの生成と検定を反復し，タイプ2エラーがどの程度生じるかを相対度数で近似することもできるだろう。であれば，その近似をサンプルサイズを徐々に変えることで繰り返してサンプルサイズを定めることもできる。\n以下は，母相関が\\(\\rho = 0.5\\)とした時に，検出力が80%になるために必要なサンプルサイズを求めるシミュレーションコードである。\n\npacman::p_load(MASS)\nset.seed(12345)\nalpha &lt;- 0.05\nbeta &lt;- 0.2\nrho &lt;- 0.5\nsd &lt;- 1\nSigma &lt;- matrix(NA, ncol = 2, nrow = 2)\nSigma[1, 1] &lt;- Sigma[2, 2] &lt;- sd^2\nSigma[1, 2] &lt;- Sigma[2, 1] &lt;- sd * sd * rho\n\niter &lt;- 1000\n\nfor (n in seq(from = 10, to = 1000, by = 1)) {\n  FLG &lt;- rep(0, iter)\n  for (i in 1:iter) {\n    X &lt;- mvrnorm(n, c(0, 0), Sigma)\n    cor_test &lt;- cor.test(X[, 1], X[, 2])\n    FLG[i] &lt;- ifelse(cor_test$p.value &gt; alpha, 1, 0)\n  }\n  t2error &lt;- mean(FLG)\n  print(paste(\"n=\", n, \"のとき，betaは\", t2error, \"です。\"))\n  if (t2error &lt;= beta) {\n    break\n  }\n}\n\n[1] \"n= 10 のとき，betaは 0.681 です。\"\n[1] \"n= 11 のとき，betaは 0.639 です。\"\n[1] \"n= 12 のとき，betaは 0.612 です。\"\n[1] \"n= 13 のとき，betaは 0.566 です。\"\n[1] \"n= 14 のとき，betaは 0.563 です。\"\n[1] \"n= 15 のとき，betaは 0.471 です。\"\n[1] \"n= 16 のとき，betaは 0.462 です。\"\n[1] \"n= 17 のとき，betaは 0.419 です。\"\n[1] \"n= 18 のとき，betaは 0.402 です。\"\n[1] \"n= 19 のとき，betaは 0.385 です。\"\n[1] \"n= 20 のとき，betaは 0.353 です。\"\n[1] \"n= 21 のとき，betaは 0.344 です。\"\n[1] \"n= 22 のとき，betaは 0.312 です。\"\n[1] \"n= 23 のとき，betaは 0.285 です。\"\n[1] \"n= 24 のとき，betaは 0.256 です。\"\n[1] \"n= 25 のとき，betaは 0.265 です。\"\n[1] \"n= 26 のとき，betaは 0.21 です。\"\n[1] \"n= 27 のとき，betaは 0.227 です。\"\n[1] \"n= 28 のとき，betaは 0.176 です。\"\n\nprint(n)\n\n[1] 28\n\n\nここではシミュレーション回数1000，上限1000，刻み幅を1にしているが，状況に応じて変更すると良い。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#課題",
    "href": "chapter10.html#課題",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "10.3 課題",
    "text": "10.3 課題\n\n一要因3水準のBetweenデザインの分散分析において、1.分散分析で有意差が見られる場合と、2.任意の2水準の組み合わせのどこかで有意差が見られる場合を考えたとき、タイプ1エラーはどのように異なるかをシミュレーションで確かめてみましょう。設定として、n1=n2=n3=10、標準偏差も各群等しくsigma = 1としてみましょう。\nN増し問題は相関係数の検定の時も生じるでしょうか。母相関が\\(\\rho = 0.0\\)のとき、サンプルサイズを10から始めて、有意になるまでデータを追加する仮想研究を1000回行ってみましょう。データ追加の上限は100、有意水準は\\(\\alpha = 0.05\\)として、最終的に有意になる割合を計算してみましょう。\n\\(\\alpha = 0.05, \\beta = 0.2\\)とし、効果量\\(\\delta = 1\\)とした時の対応のないt検定のサンプルサイズ設計をしたいです。1.非心t分布を使った解析的な方法と、2.シミュレーションによる近似的な方法の両方で、同等の結果が出ることを確認しましょう。\n\n\n\n\n\nクルシュケJ. K. (2014) 2017. ベイズ統計モデリング: R, JAGS, Stanによるチュートリアル 原著第2版. Translated by 前田和寛 and 小杉考司. 共立出版.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter10.html#footnotes",
    "href": "chapter10.html#footnotes",
    "title": "10  疑わしき研究実践とサンプルサイズ設計",
    "section": "",
    "text": "ANOVA君は結果をコンソールに直接出力し，戻り値を持たない。ここでは結果の\\(p\\)値が必要だったので，このようにした。↩︎\n表(1)が出るか，裏(0)が出るか，という2値の結果変数だけを持つ施行のことで，この確率変数がベルヌーイ分布にし違う。ベルヌーイ分布は，表が出る確率\\(\\theta\\)をパラメータに持つ。\\(P(X=k) = \\theta^k(1-\\theta)^{1-k},\\text{ただし}k=\\{0,1\\}\\)という確率変数である。1/0というのが生死，男女，成功失敗などさまざまなメタファに適用できるので応用範囲が広い。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nもちろん基準となる有意水準\\(\\alpha\\)，検出力\\(1-\\beta\\)も定める必要があるが，慣例的に\\(\\alpha = 0.05\\)であり，\\(1-\\beta =0.8\\)ぐらいが必要とされている。↩︎\nこの「最低限検出したい効果」のことをSmallest Effect Size of Interest, SESOIと呼ぶ。小杉, 紀ノ定, and 清水 (2023) も参照。↩︎\nt統計量の実現値の式にある分母，\\(\\sqrt{U_p^2/\\frac{n_1n_2}{n_1+n_2}}\\)に見られる，プールされた不偏分散を割るための標本サイズであり，2群の母分散が等しいと仮定して計算するなら，\\(\\sigma^2(\\frac{1}{n_1} + \\frac{1}{n_2}) = \\sigma^2(\\frac{n_1+n_2}{n_1n_2}) = \\sigma^2 / \\frac{n_1n_2}{n_1 + n_2}\\)から得られる。↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>疑わしき研究実践とサンプルサイズ設計</span>"
    ]
  },
  {
    "objectID": "chapter11.html",
    "href": "chapter11.html",
    "title": "11  重回帰分析の基礎",
    "section": "",
    "text": "11.1 回帰分析の基礎\nここでは回帰分析を扱う。説明変数\\(x\\)と被説明変数\\(y\\)の関数関係\\(y=f(x)\\)に，次の一次式を当てはめるのが単回帰分析である。\n\\[ y_i  = \\beta_0 + \\beta_1 x_i + e_i = \\hat{y}_i + e_i\\]\n一次式を\\(\\hat{y}\\)とまとめたものを予測値といい，予測値と実測値\\(y\\)の差分\\(e_i\\)を残差residualsという。\n空間上の一次直線の切片，傾きを求めるというのが基本的な問題であり，二点であれば一意に定めることができるが，データ分析の場面では3点以上の多くのデータセットの中に直線を当てはめることになるので，なんらかの外的な基準が必要になる。この時，「残差の分散が最も小さくなるように」と考えるのが最小二乗法の考え方であり，「残差が正規分布に従っていると考え，その尤度が最も大きくなるように」と考えるのが最尤法の考え方である。前者は記述統計的な，後者は確率モデルとしての感が過多になっていることに注意してほしい。また確率モデルの推定方法としては，事前分布を用いたベイズ推定が用いられることもある。\n最小二乗法による推定値は，次の式で表される。証明は他書(小杉 2018; 西内 2017)に譲るが，ロジックとして残差の二乗和\\(\\sum e_i^2 = \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2\\)を最小にすることを考え，この式を展開するか偏微分を用いて極小値を求めることで算出できるとだけ伝えておこう。いずれにせよ，平均値\\(\\bar{x},\\bar{y}\\)や分散・共分散\\(s_x,s_y,r_{xy}\\)など標本統計量から推定できるのはありがたいことである。\n\\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x},\\quad \\beta_1 = r_{xy} \\frac{s_y}{s_x}\\]\nまた，ここでは\\(x,y\\)ともに連続変数を想定しているが，説明変数\\(x\\)が二値，あるいはカテゴリカルなものであれば\\(y\\)の平均値を通る直線を探すことになる。直線の傾きが0であれば「平均値が同じ」という線形モデルであり，これは平均値差の検定における帰無仮説と同等である。このように，t検定やANOVAは回帰分析の特殊ケースとも考えられ，まとめて一般線形モデルと呼ばれる。一般線形モデルは，被説明変数が連続的で，線形モデルによる平均値に正規分布に従う残差が加わったものとして考えるという意味で統一的に表現される。\nANOVAの場合は，二つ以上の要因による効果を考えることもあった。交互作用項を考慮しなければ，2要因のモデルは次のように表現することができる。\n\\[ y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + e_i \\]\nこのように説明変数が複数ある回帰分析を特に重回帰分析Multiple Regression Analysisと呼ぶ。一次式なので，ある変数に限れば線形性が担保されているから，これも線形モデルの仲間である。重回帰分析を用いる場合は，説明変数同士を比較して「どちらの説明変数の方が影響力が大きいか」ということが論じられることが多いが，係数は当然\\(x_n, y\\)の単位に依存するため，素点の回帰係数は使い勝手が悪い。そこですべての変数を標準化した標準化係数が用いられることが多い。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#回帰分析の特徴",
    "href": "chapter11.html#回帰分析の特徴",
    "title": "11  重回帰分析の基礎",
    "section": "11.2 回帰分析の特徴",
    "text": "11.2 回帰分析の特徴\n以下，具体的なデータを用いて回帰分析の特徴を見てみよう\n\n11.2.1 パラメータリカバリ\n回帰分析のモデル式にそってデータを生成し，分析によってパラメータリカバリを行ってみよう。\n説明変数については制約がないので一様乱数から生成し，平均0，標準偏差\\(\\sigma\\)の誤差とともに被説明変数を作る。\n\npacman::p_load(tidyverse)\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データの生成\nx &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x + e\n\ndat &lt;- data.frame(x, y)\n# データの確認\nhead(dat)\n\n          x          y\n1 -4.248450 -11.120952\n2  5.766103  18.736432\n3 -1.820462  -3.805302\n4  7.660348  25.071541\n5  8.809346  30.026546\n6 -9.088870 -25.355175\n\ndat %&gt;% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y~x\") # 線形モデルの描画\n\n\n\n\n\n\n\n\nこのデータに基づいて回帰分析を実行した結果が以下のとおりである。\n\nresult.lm &lt;- lm(y ~ x, data = dat)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82796 -0.61831  0.03553  0.69367  2.68062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.021928   0.045010   44.92   &lt;2e-16 ***\nx           3.002194   0.007919  379.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.006 on 498 degrees of freedom\nMultiple R-squared:  0.9965,    Adjusted R-squared:  0.9965 \nF-statistic: 1.437e+05 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\nここでは\\(\\beta_0 =2, \\beta_1=3, \\sigma = 1\\)と設定しており，ほぼ理論通りの係数がリカバリーできていることを出力から確認しておこう。 もちろんリカバリの精度は，データの線形性の強さに依存するから，残差の分散が大きかったりサンプルサイズが小さくなると，必ずしもうまくリカバリできないことがあることは想像に難くないだろう。\n\n\n11.2.2 残差の正規性と相関関係\nlm関数が返した結果オブジェクトには，表示されていない多くの情報が含まれている。例えば予測値や残差も含まれているので，これを使って回帰分析の特徴を見てみよう。\n\ndat &lt;- bind_cols(dat, yhat = result.lm$fitted.values, residuals = result.lm$residuals)\nsummary(dat)\n\n       x                  y                yhat            residuals       \n Min.   :-9.99069   Min.   :-28.216   Min.   :-27.9721   Min.   :-2.82796  \n 1st Qu.:-5.08007   1st Qu.:-13.074   1st Qu.:-13.2294   1st Qu.:-0.61831  \n Median :-0.46887   Median :  0.301   Median :  0.6143   Median : 0.03553  \n Mean   :-0.09433   Mean   :  1.739   Mean   :  1.7387   Mean   : 0.00000  \n 3rd Qu.: 4.65795   3rd Qu.: 15.963   3rd Qu.: 16.0060   3rd Qu.: 0.69367  \n Max.   : 9.98809   Max.   : 32.638   Max.   : 32.0081   Max.   : 2.68062  \n\n\n予測値\\(\\hat{y}\\)はfitted.valuesとして保存されている。これの平均値が被説明変数\\(y\\)の平均値に一致していることが確認できる。回帰分析は説明変数\\(x\\)を伸ばしたり(\\(\\beta_1\\)倍する)ズラしたり(\\(\\beta_0\\)を加える)しながら，被説明変数\\(y\\)に当てはめるのであり，位置合わせがなされた予測値の中心が被説明変数の中心と一致することは理解しやすいだろう1。\n次に，残差の平均が0になっていることも確認しておこう。これが0でない\\(c\\)であれば，回帰係数が常に\\(c\\)だけズレていることになるので，そのような系統的ズレは最適な線形の当てはめにおいて除外されているべきだからである2。\nまた，回帰分析において残差は正規分布に従うことが仮定されていた。これを検証するにはQ-Qプロットを見ると良い。\n\ndat %&gt;%\n  ggplot(aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\n\nQ-Qプロットとは2つの確率分布を比較するためのグラフであり，横軸には理論的分布の分位点が，縦軸に実データが並ぶもので，右上がりの直線上にデータが載っていれば分布に従っている，と判断するものである。直線から逸脱している点は理論的分布からの逸脱と考えられる。今回の結果はほとんどが正規分布の直線上にあることから，大きな逸脱がないことが認められる。\nデータ生成メカニズムによっては，被説明変数が二値的であったり，順序的であったり，カウント変数であったり，と正規分布がそぐわないものもあるだろう。そのようなデータに無理やり回帰分析を当てはめることは適切ではない。いかなる時もデータは可視化して，モデルを当てはめることの適切さをチェックすることを忘れたはならない。\nちなみに出力結果を直接plot関数に入れてもよい。ここから残差と予測値の相関関係や，Q-Qプロット，標準化残差のスケールロケーションプロット，レバレッジと標準化残差3 などがプロットされる。\n\nplot(result.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差と予測値のプロットからも想像できるが，両者の相関はゼロである。図で確認しておこう。\n\npacman::p_load(GGally) # 必要ならインストールしよう\nggpairs(dat)\n\n\n\n\n\n\n\n\nこの関係から明らかなように，残差は説明変数や予測値と相関しない4。説明変数と残差に相関関係があるとすると，説明変数でまだ説明できていない分散が残っていることになるし，予測値と残差に相関がないことは予測値が高いか低いかにかかわらず，残差が一様に分布していることを意味する。このことを踏まえて，重回帰分析の特徴を理解していこう。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#重回帰分析の特徴",
    "href": "chapter11.html#重回帰分析の特徴",
    "title": "11  重回帰分析の基礎",
    "section": "11.3 重回帰分析の特徴",
    "text": "11.3 重回帰分析の特徴\n重回帰分析においては，回帰係数は偏回帰係数partial regression coefficientsと呼ばれる。この「偏」の一文字が意味することを考えていこう。\n\n11.3.1 回帰係数と偏回帰係数\n単回帰分析の回帰係数は，説明変数\\(x\\)が一単位上昇した時の被説明変数の変化量，と解釈すればよい。これに対して重回帰分析の偏回帰係数を，「説明変数\\(x_1\\)が一単位上昇した時の被説明変数の変化量」とすることはできない。というのも，説明変数が複数(\\(x_2,x_3,\\ldots\\))あり，他の説明変数の次元についての変化を考慮していない変化量になっているからである。\n重回帰分析において，説明変数が完全に無相関で直交しているのであれば，\\(x_1\\)の変化と\\(x_2\\)の変化を独立して説明できるが，往々にしてそのようなことはない。偏回帰係数は当該変数以外の変動を統制した回帰係数である。\n上で単回帰係数において，説明変数と残差が相関しないことを確認した。言い換えれば，説明変数で説明でき分散は全て説明し尽くされており，残差は説明変数で説明できない被説明変数の分散，つまり説明変数の影響を除外した被説明変数の分散と考えることができる(被説明変数の分散=説明変数が説明する分散+残差の分散)。\nここで第二の変数\\(x_2\\)があったとする。第一の変数\\(x_1\\)で\\(y\\)を説明した残差\\(e_y\\)と，第一の変数で第二の変数を説明した残差\\(e_{x2}\\)との相関を偏相関partial correlationという。これは第一の変数\\(x_1\\)からの影響を両者から取り除いているので，\\(x_1\\)で統制した相関係数ということができる。偏相関は単純な相関が「見せかけの関係」でないことを検証するための重要な指標である。\n偏相関係数を計算してみよう。\n\npacman::p_load(MASS)\npacman::p_load(psych)\nSigma &lt;- matrix(c(1, 0.3, 0.5, 0.3, 1, 0.8, 0.5, 0.8, 1), ncol = 3)\nX &lt;- mvrnorm(1000, c(0, 0, 0), Sigma, empirical = TRUE) %&gt;% as.data.frame()\n## 相関行列\ncor(X)\n\n    V1  V2  V3\nV1 1.0 0.3 0.5\nV2 0.3 1.0 0.8\nV3 0.5 0.8 1.0\n\n## 回帰分析をして残差を求める\nresult.lm1 &lt;- lm(V2 ~ V1, data = X)\nresult.lm2 &lt;- lm(V3 ~ V1, data = X)\ncor(result.lm1$residuals, result.lm2$residuals)\n\n[1] 0.7867958\n\n## 偏相関を求めるR関数で確認\npsych::partial.r(X)[2, 3]\n\n[1] 0.7867958\n\n\n最後はpsychパッケージの偏相関行列を求める関数で検証した。確かに残差同士の相関係数が偏相関係数になっていることが確認できたと思う。\nそして，ここでは残差同士の相関係数として算出しているが，残差をつかった回帰分析の係数が偏回帰係数になるのである。このデータセットの第一変数を従属変数にした重回帰分析の結果から，これを確認してみよう。\n\nresult.mra &lt;- lm(V1 ~ V2 + V3, data = X)\n# 回帰係数を取り出す\nresult.mra$coefficients\n\n  (Intercept)            V2            V3 \n-5.218050e-17 -2.777778e-01  7.222222e-01 \n\n# 残差をつかって偏回帰係数を確認する\nresult.lm3 &lt;- lm(V1 ~ V3, data = X)\nresult.lm4 &lt;- lm(V2 ~ V3, data = X)\nresult.lm5 &lt;- lm(result.lm3$residuals ~ result.lm4$residuals)\n#\nresult.lm5$coefficients\n\n         (Intercept) result.lm4$residuals \n       -7.925711e-18        -2.777778e-01 \n\n\n重回帰分析の結果result.mraのV2からV1への回帰係数は-0.2778である。また，V3でV1,V2を統制した残差同士をつかい，回帰係数を求めた結果は-0.2778と，同じ値になっていることが確認できただろう。\nV3からV1への偏回帰係数も同様で，V2で両者を統制した残差同士による回帰係数になっている。このように，重回帰分析の回帰係数は，他の説明変数で統制した値になっており，日本語で説明するなら「他の変数の値が同じであると想定した条件つきの，当該変数の影響力」とでもいうべき値になっている。\nなぜこのような持って回った説明をするかというと，つい「条件付きの」という話を忘れて報告，解釈してしまうことが多いからで，吉田 and 村井 (2021) の論文での指摘は議論を呼んだのは記憶に新しい5。 たとえば今回の例でも，回帰係数が-0.2778であったのに対し，V1とV2の単相関が0.3 であったことを思い出そう。符号が反転しているため，解釈は真逆になってしまう。実際の単相関は正の関係であるから，条件付きであることを忘れて「V2は負の影響，V3は正の影響」と表現してしまうと，ミスリーディングなことになるからである。\nまた，豊田 (2017) は重回帰分析のこうした誤用を避けるために，独立変数を事前に直交化したデザインで行うコンジョイント分析の積極的な利用を提案している。我々が重回帰分析をうまく使いこなせないのであれば，そうした手法も有用であるだろう。\n\n\n11.3.2 多重共線性\n偏回帰係数の解釈が難しい理由の一つは，説明変数同士に相関関係がみられることにある。 特に，説明変数間の相関関係が高くなることは多重共線性Multicollinearityの問題という。この問題は，回帰係数の標準誤差がインフレを起こすことを指す。\n例えば先ほどの例で，説明変数V2とV3は相関係数\\(0.8\\)を持っていた。この時の回帰係数の標準誤差を確認しておこう。\n\nsummary(result.mra)\n\n\nCall:\nlm(formula = V1 ~ V2 + V3, data = X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.59118 -0.54717 -0.03692  0.55044  2.90735 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5.218e-17  2.690e-02   0.000        1    \nV2          -2.778e-01  4.486e-02  -6.192 8.65e-10 ***\nV3           7.222e-01  4.486e-02  16.100  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8507 on 997 degrees of freedom\nMultiple R-squared:  0.2778,    Adjusted R-squared:  0.2763 \nF-statistic: 191.7 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n標準誤差は0.0449であり，それほど大きくない標準誤差で問題がないようである。しかし両者の相関係数がより高くなり，一方が他方に線形的に従属してしまうと係数の推定値が不安定になるため，注意が必要である。\nこのインフレを確認するための指標がVariance Infration Factor: VIFである。Rではcarパッケージにあるvif関数に重回帰モデルを入れることでこの指標が算出される。一般にVIFが3，あるいは10を超えると多重共線性が生じており，解釈に注意が必要と言われている6。\n\npacman::p_load(car) # なければ入れておこう\nvif(result.mra)\n\n      V2       V3 \n2.777778 2.777778 \n\n\n幸い，今回の値はこれらの基準を下回っていたので許容範囲内である。\n\n\n11.3.3 変数の投入順序\n重回帰分析の場合は複数の説明変数があるが，これを投入するときに全ての変数を同時に投入するか，順番をつけて投入するかといった手法の違いがある。前者を強制投入法と呼ぶこともある。 順番をつけて投入する方法は，逐次投入と呼ばれる。この場合は，適合度指標などを参考に変数を追加あるいは削除して，適合度が統計的に有意に向上するかどうかを考えながら進めていく。\n重回帰係数の予測値\\(\\hat{y}\\)と，被説明変数\\(y\\)の相関係数\\(R_{y\\hat{y}}\\)は重相関係数と呼ばれ，予測がうまくいっているかどうかを表す適合度の一つである。相関係数なので\\(-1\\)から\\(+1\\)までの値を取りうるが，\\(-1\\)は逆に完全に合致していることになるので，この相関係数の符号は大して情報を持たない。そこでこれを二乗した\\(R_{y\\hat{y}}^2\\)を考える。これは決定係数とも呼ばれ，説明変数の分散のうち予測値の分散が占める割合を表している。7\n説明変数の逐次投入は，説明変数を持たないヌルモデルから一つずつ追加していくForward Selection，全ての変数を投入してから一つずつ減らしていくBackward Selectionがある。Forwardのほうは追加することによって\\(R^2\\)が有意に増加するか，Backwardのほうは削除しても有意に\\(R^2\\)が減らないか，を確認しながら進めることになる。この方法は手元のデータに最も適した説明変数のペアを選出できる方法ではあるが，検定を繰り返していることの問題と，手元のデータ以外に一般化する時の根拠の乏しさから，用いられないこともある。\n逐次投入法には別の観点からの手法もある。それが階層的回帰分析である。この手法は，重回帰分析における交互作用項の投入を検討する文脈で発展した。重回帰分析では，説明変数同士の相関がない，もしくは小さい方が望ましい。しかし，交互作用とは分散分析における組み合わせの効果を表すものであり，実験デザインによっては交互作用効果が重要な変動であることも少なくない。回帰分析と分散分析は，一般線形モデルという形で統一的に理解されるが，回帰分析でも連続的に変化する組み合わせの効果を考えることができる。交互作用があるということは説明変数間に相関があることを意味するため，回帰分析の大前提に抵触する可能性があり，その投入には慎重を期する必要がある。\nこうした文脈から，まずは要因の効果を投入し，次に交互作用項を投入してモデル適合度の有意な改善がみられるかどうかを検証する手順が推奨されている。この逐次投入法を特に階層的回帰分析と呼ぶ。ここでの「階層」とは，手順が重要度順に進められていることを意味し，データの特徴に関するものではないことに注意が必要である8。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#係数の標準誤差と検定",
    "href": "chapter11.html#係数の標準誤差と検定",
    "title": "11  重回帰分析の基礎",
    "section": "11.4 係数の標準誤差と検定",
    "text": "11.4 係数の標準誤差と検定\n\n11.4.1 係数の検定\nサンプルが母集団から得られた確率変数であるのだから，(偏)回帰係数もまた確率変数である。すなわち，サンプルが変わるごとに変化し，その揺らぎがある確率分布に従うと考えられる。これを確認するためには，データ生成過程をモデリングし，反復することで近似させて理解するのがいいだろう。\n\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データ生成関数\ndataMake &lt;- function(n, beta0, beta1, sigma) {\n  x &lt;- runif(n, -10, 10)\n  e &lt;- rnorm(n, 0, sigma)\n  y &lt;- beta0 + beta1 * x + e\n  dat &lt;- data.frame(x, y)\n  return(dat)\n}\n\n# 結果オブジェクトの準備\niter &lt;- 2000\nbeta0.est &lt;- rep(NA, iter)\nbeta1.est &lt;- rep(NA, iter)\n# simulation\nfor (i in 1:iter) {\n  sample &lt;- dataMake(n, beta0, beta1, sigma)\n  result.lm &lt;- lm(y ~ x, data = sample)\n  beta0.est[i] &lt;- result.lm$coefficients[1]\n  beta1.est[i] &lt;- result.lm$coefficients[2]\n}\n\ndata.frame(x = beta0.est) %&gt;% ggplot(aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\ndata.frame(x = beta1.est) %&gt;% ggplot(aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n図から明らかなように，回帰係数も確率的に分布する。ただしその平均は理論値に近似している。\n\nmean(beta0.est)\n\n[1] 1.999257\n\nmean(beta1.est)\n\n[1] 2.999798\n\n\nこの分布の幅が回帰係数の標準誤差である。\n\nsd(beta0.est)\n\n[1] 0.04580387\n\nsd(beta1.est)\n\n[1] 0.007659277\n\n\n回帰係数はt分布に従い，その自由度はサンプルサイズからモデルで用いる係数の数を引いたものになる。先ほどのヒストグラムを基準化し，理論分布を重ねて描画してみることで確認しておこう。\n\ndata.frame(x = beta1.est) %&gt;%\n  scale() %&gt;%\n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 100) +\n  stat_function(fun = function(x) dt(x, df = n - 2), color = \"red\", linewidth = 2)\n\n\n\n\n\n\n\n\nこのt分布を用いて，係数が0の母集団から得られたサンプルなのかどうかの検定が行われる。\n\n\n11.4.2 モデル適合度の検定\n一方で，出力の最後にはF統計量による検定も行われていたことを確認しておこう。次に示すのは重回帰分析の例である。\n\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 0\nbeta2 &lt;- 0\nsigma &lt;- 1\nx1 &lt;- runif(n, -10, 10)\nx2 &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + e\nsample &lt;- data.frame(y, x1, x2)\nresult.lm &lt;- lm(y ~ x1 + x2, data = sample)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = sample)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85235 -0.68275 -0.01436  0.67809  2.70488 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.996999   0.045263  44.120   &lt;2e-16 ***\nx1          -0.006453   0.007970  -0.810    0.418    \nx2          -0.003928   0.007795  -0.504    0.615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.012 on 497 degrees of freedom\nMultiple R-squared:  0.001893,  Adjusted R-squared:  -0.002124 \nF-statistic: 0.4713 on 2 and 497 DF,  p-value: 0.6245\n\n\n上の例では，統計量\\(F\\)が，自由度\\(F(\\) 2,497 \\()\\)のもとで，0.4713であり，統計的に有意ではないと判断される(p=0.6245,n.s.)。\nこれは重相関係数に対する検定であり，母集団においてモデル全体としての説明力が0である，という帰無仮説を検証しているものである。この有意性検定には，説明変数の数\\(p\\)，サンプルサイズ\\(n\\)，重相関係数\\(R^2\\)を用いて，以下の式で用いられる検定統計量Fを利用する(南風原 2014)。\n\\[ F= \\frac{R^2}{1-R^2}\\cdot\\frac{n-p-1}{p} \\]\nここで右辺第一項目はCohenの効果量(\\(f^2=\\frac{R^2}{1-R^2}\\)) といわれ，サンプルサイズ設計においてはこの指標が利用される。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#サンプルサイズ設計",
    "href": "chapter11.html#サンプルサイズ設計",
    "title": "11  重回帰分析の基礎",
    "section": "11.5 サンプルサイズ設計",
    "text": "11.5 サンプルサイズ設計\n重回帰分析のサンプルサイズ設計は，変数の効果の大きさ(回帰係数)が事前にわかっているのであれば，nを徐々に増やしていくシミュレーションによって行える。しかしそのようなケースは稀であり，実際には\\(R^2\\)の検定を用いて，ある効果量と検出力の下で，正しく検出できるサイズを算出することになる。\nサンプルサイズの算出には，非心F分布を用いる。この時の非心度は，効果量\\(f^2\\)に\\(n\\)をかけたものになる。これを使ってサンプルサイズ設計をする例は以下のとおりである。\n\nf2 &lt;- 0.15 # 効果量\nalpha &lt;- 0.05 # タイプ1エラー率\nbeta &lt;- 0.2 # タイプ2エラー率\np &lt;- 5 # 説明変数の数\n\nfor (n in 10:500) {\n  lambda &lt;- f2 * n\n  df1 &lt;- p\n  df2 &lt;- n - p - 1\n  cv &lt;- qf(p = 1 - alpha, df1, df2)\n  t2error &lt;- pf(q = cv, df1, df2, ncp = lambda)\n  if (t2error &lt; beta) {\n    break\n  }\n}\n\nprint(n)\n\n[1] 92\n\n\nこの設定では，n = 92以上であればモデルとして影響力がないとは言えない，ということがわかる。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#まとめ",
    "href": "chapter11.html#まとめ",
    "title": "11  重回帰分析の基礎",
    "section": "11.6 まとめ",
    "text": "11.6 まとめ\n重回帰分析は，人文社会科学で多用される技術ではあるが，技術が先行して理解が伴わないまま利用されているケースも少なくない。繰り返しになる点もあるが，以下に注意点をまとめておく。\n\n偏回帰係数の意味；重回帰分析における偏回帰係数は，ほかの変数を統制した上での値であり，あたかも各係数が独立直交しているかのように解釈するのは適切ではない。\n誤差の正規性；誤差は正規分布に従っているという仮定があり，二値データや整数しかとらないカウントデータなどに盲目的にモデルを適用してはならない。誤差の正規性が満たされているかどうかは，分析後にQ-Qプロットを用いて確認する。\n誤差の均質性；誤差はモデル全体にわたって同じ正規分布に従っているというのもモデルの仮定である。すなわち，独立変数に応じて誤差分散が変わるといった均質でないデータの場合は，正しく推定されない。誤差の均質性については，分析後のQ-Qプロットを用いて確認する。\n誤差間の独立性；誤差はモデル全体にわたって同じ正規分布から独立に生成されている(i.i.d)というのがモデルの仮定である。時系列データのように，誤差間に対応(自己回帰)がみられるデータの場合は回帰分析は適切な手法とならない。状態空間モデルなど，誤差間関係を適切にモデリングしたものを当てはめる必要がある。\nモデルの適切な定式化；モデルには被説明変数に影響を与えるすべての変数が正しく含まれている必要がある。例えば，影響を与えることがわかっている変数\\(X_o\\)を意図的に除外して分析をしたとする。そのモデルに含まれる変数\\(X_a\\)が被説明変数\\(y\\)に影響を与えていたとしても，\\(X_a\\)と\\(X_o\\)に相関があれば，\\(X_o\\)の影響力が\\(X_a\\)を通じて\\(y\\)に伝播するkから，\\(X_a\\)の影響力が課題に評価されることになる。自らの仮説のために，意図的に変数を選択するのはQRPsに該当する。\n説明変数間の相関関係；説明変数のうちにあまりにも相関関係が高い変数ペアがあれば，多重共線性の疑いが生じる。多重共線性は推定値の不安定さとなって現れる。このような場合は，説明変数を主成分分析で合成変数にまとめるといった対応が考えられる。また，高い相関ではないが交互作用効果が見たいといった場合は，逐次投入など慎重に個々の影響を考えながら投入するようにする(階層的重回帰分析)。なお交互作用項は，各変数の平均からの偏差をかけ合わせたものにすることが一般的である。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#課題",
    "href": "chapter11.html#課題",
    "title": "11  重回帰分析の基礎",
    "section": "11.7 課題",
    "text": "11.7 課題\n\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。画面には一部しか表示しておらず，全体(\\(n=100\\))はこちらex_regression1.csvからダウンロード可能です。このデータセットを用いて重回帰分析を行い，結果を出力してください。\n\n\n\n           y        x1         x2\n1  1.8685595 -4.248450  1.9997792\n2 -0.5728781  5.766103 -3.3435292\n3  1.0321850 -1.820462 -0.2277393\n4 10.0468488  7.660348  9.0894765\n5 -1.1968078  8.809346 -0.3419521\n6  9.6719213 -9.088870  7.8070044\n\n\n\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。画面には一部しか表示しておらず，全体(\\(n=300\\))はこちらex_regression2.csvからダウンロード可能です。このデータセットを用いて重回帰分析を行ってください。結果のプロットから，上に挙げた重回帰分析の仮定に反しているところを指摘してください。\n\n\n\n          y         x1           x2\n1  3.586304 -0.4248450  0.132767341\n2  8.599252  0.5766103  0.922713561\n3  2.397115 -0.1820462  0.053684622\n4  3.505236  0.7660348  0.007801881\n5  6.517720  0.8809346  0.633076091\n6 -1.394231 -0.9088870 -0.895346802\n\n\n\n\\(R^2=0.3\\)を目標として，説明変数の数\\(p=10\\)の重回帰分析を行う際に，必要なサンプルサイズはいくつになるか，計算してみましょう。ここで，\\(\\alpha = 0.05,\\beta=0.2\\)とします。\n\n\n\n\n\n南風原朝和. 2014. 心理統計学の基礎: 続・統合的理解のために. 有斐閣.\n\n\n吉田寿夫, and 村井潤一郎. 2021. “心理学的研究における重回帰分析の適用に関わる諸問題.” 心理学研究 92 (3): 178–87. https://doi.org/10.4992/jjpsy.92.19226.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房. http://ci.nii.ac.jp/ncid/BB27527420.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023. 数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計. 技術評論社.\n\n\n西内啓. 2017. 統計学が最強の学問である[数学編]: データ分析と機械学習のための新しい教科書. ダイヤモンド社.\n\n\n豊田秀樹. 2017. もうひとつの重回帰分析. 東京図書.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter11.html#footnotes",
    "href": "chapter11.html#footnotes",
    "title": "11  重回帰分析の基礎",
    "section": "",
    "text": "もちろん証明できる。\\(\\beta_0 = \\bar{y} - \\beta_1\\bar{x},\\beta_1 = r_{xy} \\frac{s_y}{s_x}\\)より，\\(\\bar{\\bar{y}} = \\frac{1}{n}\\sum(\\bar{y} - \\beta_1\\bar{x} + \\beta_1x_i) = \\bar{y} - \\beta_1\\bar{x} + \\beta_1\\frac{1}{n}\\sum x_i = \\bar{y}\\)である。↩︎\nもちろん証明できる。\\(\\bar{e} = \\frac{1}{n}\\sum e_i = \\frac{1}{n} \\sum (y_i - \\hat{y}_i) = \\bar{y} = \\bar{\\hat{y}} = 0\\)である。↩︎\n縦軸の標準化された残差の大きな値は解釈に注意が必要な外れ値である可能性が高い。レバレッジも同様に回帰係数に大きな影響を与える値の指標であり，この図の端に位置する変数は注意が必要，と考える。↩︎\nもちろん証明できる。詳細は 小杉 (2018) を参照すること。↩︎\n論文が早期公開された後，心理学会が主催するオンラインシンポジウムでは著者とこの論文で取り上げられた論文の著者が登場して，議論が交わされた(日本心理学会YouTubeライブ・話題の論文について著者と語るシリーズ,2021年7月2日20時-21時40分)。平日の夜という設定，早期公開版における議論であったにも関わらず，1700名近い視聴者が参加した。↩︎\n2変数重回帰分析モデルで，VIFが3であれば説明変数間の相関は\\(r=0.81\\)程度である。VIFが10であれば\\(r=0.97\\)にもなる。詳しくは 小杉, 紀ノ定, and 清水 (2023) を参照。↩︎\nもちろん証明できる。小杉 (2018) を参照。↩︎\nこれに対して，データの階層性(ex.学級\\(\\subset\\)市区町村\\(\\subset\\)都道府県)を考慮する線形モデルのことを，階層線形モデルHierarchical Linear Model:HLM という。↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>重回帰分析の基礎</span>"
    ]
  },
  {
    "objectID": "chapter12.html",
    "href": "chapter12.html",
    "title": "12  ベイズ統計の活用",
    "section": "",
    "text": "12.1 ベイズ統計の位置付け\nここまで心理統計の基本的な話題をRで実践しながら見てきたが，ここでベイズ統計の話題を導入しよう。 わざわざこのように断る理由は，ベイズ統計学が推測統計学の枠組みの一種であるにもかかわらず，その歴史や解釈によって様々な誤解が生じているからである。一説にはベイズ統計学には100を超える派閥・解釈のスタイルがあるとも言われており，迂闊に議論をすることが憚られるような言説が交わされることも少なくない。\n筆者はファッションベイジアンを自認しており，主義主張を戦わせることはあまり好ましいことではないと考えている。そのような調子の良い立場の人間が語ることである，という事前情報を十分理解した上で，以下の解説を読んでほしい。\nベイズ統計学の生まれは古く，ベイズの定理にまで遡れば実に300年近く前になる。ベイズの定理を見出したとされるトーマス・ベイズは，18世紀の牧師であり，1763年に死んでいる。その古い人物の名を冠した定理が，現在は「ベイズ統計学」と学問全体を覆うほどになっているのは，それがこれまで紹介してきた統計モデルと考え方や解釈の仕方を大幅に変えているからである。\nまた，ベイズ統計学はその長い歴史に対して，応用的な価値が見出されたのが比較的最近である。筆者の感覚で言えば，2010年以降になって，ベイズ統計学の応用が急速に広まったように思う。古くから知られていた理論が長らくその名をひそめていたのは，ベイズ統計学を応用できるシーンが限定的であったこと，その研究が軍事的な機密を含むことで公開されることが少なかったこと，そして冗談のように聞こえるかもしれないが，アメリカ統計学会の重鎮がベイズ統計学を嫌っていたこと，がその理由と考えられる。このあたりの事情については(シャロン・バーチュ・マグレイン [2011] 2018)を参考にしてほしい。\nそして近年になって改めてベイズ統計学が注目されるようになったのには，2つの理由がある。第一は社会心理学における再現性の危機に対する対応として，従来の統計学ではない新しいアプローチとして期待されたこと。第二は計算機科学の発展により，非常にパワフルな推定方法が開発され，統計モデルを非常に柔軟に扱うことができるようになったこと，である。\n再現性の危機に対する対応としてのベイズ統計学は，従来の統計学を「頻度主義的」なものとして，ベイズ統計学を「ベイズ主義的」なものとして区別することで，その違いを強調するきらいがある。しかし，頻度主義的な方法に問題があるからという単純な理由でベイズ主義的な方法に乗り換えると，そもそもの問題であった統計法の悪用や誤用の種類が変わるだけに過ぎない。\n筆者は教育的観点から，ベイズ主義的な統計学の方が初学者には優しく，より理解しやすいのではないかと考えている。しかし，心理学のこの100年の歴史の中で積み重ねられてきた「頻度主義的」な研究のお作法は，誤用悪用を招く恐れもあるとは言え，非常にリッチな教育コンテンツ，分析ツールを提供してきた。またこれまでの心理学的文献が「頻度主義的」なものであったことを考えると，これらを捨て去って心機一転，一気にベイズに乗り換えようというのは現実的ではない。ベイズ統計学の弱点としては，教育コンテンツ，分析ツール，そしてベイズ統計学の教育者がそもそも少ないこと，が挙げられる。もちろんこれらの問題点が，近い将来のうちに解決されることを望むものである。\n第二の理由はベイズ統計学のポジティブな未来を予感させる。統計モデルが複雑になるにつれ，最尤推定法は実質的な限界を迎えるときでも，ベイズ推定の新しいアプローチは対応できる。これは，ベイズ統計学のパワーを具現化する計算機手法，すなわちMCMC法の発展によるところが大きい。統計モデルを非常に柔軟に，自らの研究データにカスタマイズした分析モデルを構築でき，その他のモデルともベイズファクターという一元的な指標で評価できることは，ベイズ統計学の大きな魅力である。しかし反面，細かく統計モデルをカスタマイズできるということは，研究者にプログラマとしての技量を要求することになる。心理学者はあくまでも統計をツールとして使いたいユーザなのだから，ソフトウェア的なエンジニアリングにあまりエフォートをかけていられないということもあるだろう。しかし，平均値の差の検定や要因計画にとらわれない，自由なモデリングができる魅力はおおきく，心理学の中でも統計モデリングによるアプローチをとる人も年々増加傾向にある。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter12.html#mcmc法",
    "href": "chapter12.html#mcmc法",
    "title": "12  ベイズ統計の活用",
    "section": "12.2 MCMC法",
    "text": "12.2 MCMC法\nベイズ統計を広く応用できるようにした，革新的な技術であるMCMC法についてみていこう。その前に，ベイズの定理を使った考え方について基本的な説明をしておく。\n\n12.2.1 ベイズの定理\nベイズの定理は，条件付き確率の定理である。ある事象\\(A\\)が起きたときに，事象\\(B\\)が起きる確率を\\(P(B|A)\\)と表すと，ベイズの定理は以下のように表される。\n\\[\nP(B|A) = \\frac{P(A|B)P(B)}{P(A)}\n\\]\nここで，\\(P(A)\\)は事象\\(A\\)が起きる確率，\\(P(B)\\)は事象\\(B\\)が起きる確率，\\(P(A|B)\\)は事象\\(B\\)が起きたときに事象\\(A\\)が起きる確率，\\(P(B|A)\\)は事象\\(A\\)が起きたときに事象\\(B\\)が起きる確率を表す。ここでの重要な点は，この式の右辺と左辺とで，条件付き確率の位置が変わっていることである。\nこの式は，事象\\(A\\)が起きたときに事象\\(B\\)が起きる確率\\(P(B|A)\\)を，事象\\(B\\)が起きたときに事象\\(A\\)が起きる確率\\(P(A|B)\\)と，事象\\(B\\)が起きる確率\\(P(B)\\)と，事象\\(A\\)が起きる確率\\(P(A)\\)を用いて表している。ここで\\(A\\)をデータ，\\(B\\)をモデルのパラメータと考えると，ベイズの定理は以下のように表される。\n\\[\nP(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}\n\\]\nこの式の左辺にあるのは，データ\\(D\\)が与えられた時のパラメータ\\(\\theta\\)が得られる確率であり，データに基づいてパラメータを推論するという推測統計学の基本的な考え方である。これを構成する右辺は，\\(P(D|\\theta)\\)はパラメータ\\(\\theta\\)が与えられた時のデータ\\(D\\)が得られる確率，\\(P(\\theta)\\)はパラメータ\\(\\theta\\)が得られる確率と言える。パラメータ\\(\\theta\\)がある値になったときに，データ\\(D\\)が得られる確率\\(P(D|\\theta)\\)は尤度と考えることができる。それにかけられる\\(P(\\theta)\\)は，パラメータ\\(\\theta\\)の事前確率と呼ばれるが，これがあることによってベイズ統計学の特徴がより鮮明になる。\nデータを取る前にパラメータがどのようにあるのか，この不確実さをベイズ統計学では事前確率，あるいは一般に事前分布として表現する。それと尤度を掛け合わせたものが(周辺尤度\\(P(D)\\)で除されるとは言え)統計的な推測の結果である事後確率，事後分布\\(P(\\theta|D)\\)となっている。\n古典的には，データを取る前にパラメータのあり方を想定するのは，科学的な態度として不適切であるとして，ベイズ統計学は批判されてきた。この点を認めたとして，改めて式を見てみると，確率分布とデータの関係を表す尤度に，事前分布と周辺尤度の比をかけた結果，事後分布が得られていることになる。尤度を計算してその最も高い値を持つパラメータを推定値としよう，というのが最尤推定法だったわけだが，ベイズ推定はその拡張で，尤度から得られる結果を分布として考えるために事前分布と周辺尤度の比を掛け合わせた，とも言える。尤度関数は確率関数ではないので，そのピークになるところを推定値として用いることしかできなかったが，ベイズの定理を経由すると結果的に得られるのが確率分布になるのだから，パラメータがどのあたりにありそうか，その分布として定量的に評価することも可能である。\n\n\n12.2.2 ベイズの定理の実践史\nベイズ統計学では，未知なるものを確率で表現する。心理統計では平均値パラメータや，複数の平均値パラメータ間の差をすることが目的であり，データを取っただけでは母数が未知なものだったわけだから，これを確率分布として表現することがベイズ統計の最初のステップである。\nベイズ統計は，「未知なるパラメータを確率で表現する（事前分布）」「データによって事前分布を事後分布にアップデートする」というたった2つのステップを繰り返すだけである。事前分布も，データの得られた状況に応じて適切に選択することができる。例えば男性と女性の平均身長の差を知りたい，という状況を考えてみよう。男性の身長の平均値，女性の身長の平均値が未知なるパラメータであるが，長さであること，人間であることを考えると，どれほど低く見積もっても100cm，どれほど高く見積もっても300cmという幅で考えていれば十分であろう。ならば，この区間のどこかにピークが来るような広い正規分布(例えば平均100，標準偏差10)を事前分布として用いれば十分である。\n尤度，すなわちデータが得られるメカニズムとして，確率分布としての正規分布を考えたとしよう。このとき，ベイズの定理の分子に乗るのはいずれも正規分布であるから，正規分布と正規分布の組み合わせから得られる事後分布の形は正規分布になる。\nこのように，事後分布の形が明確であれば，ベイズ統計学は従来の統計学と同じように，パラメータの推定値を求めることができる。しかし問題は，このように事後分布の形が明確でないことが少なくない，ということである。複雑なモデルになればなるほど，複数の確率分布がいろいろ組み合わさった形になり，結果的に事後分布がどのような確率分布になるのか，その形がわからないということになる。\nこれまでのベイズ統計学は，事後分布の形がわかるような，あるいは計算しやすい形になるような(分子の)確率分布の組み合わせを見出す，というのが中心的な問題であった。非常に限定的なものに感じられるかもしれない。まさにその理由で，ベイズ統計学は絵に描いた餅だったのである。\n計算機科学が発達するにつれて，複雑怪奇な事後分布の形であっても，そのピークを探索する方法が考えられた。これは確率分布のパラメータを少しずつ変えていくことで，事後分布の確率密度がより高い位置に動くように変化させていく，つまりあらゆる組み合わせを考えていくグリッドサーチの方法である。グリッドサーチは計算量が膨大になるが，頑張ればなんとかなる。その意味で，ベイズ統計学が少しは実用的になってきた。\nしかしもちろん，まだまだ普段使いできるほどのレベルではない。そのレベルにまで達したのがMCMCという技術である。\n\n\n12.2.3 Markov Chain Monte Carlo法\nMCMC法は，マルコフ連鎖モンテカルロ法の略で，マルコフ連鎖とモンテカルロ法という2つの技術を合わせたものである。前者は確率過程のモデル，後者はシミュレーションによる確率分布のサンプリングを行う方法である。一言で言ってしまえば，マルコフ連鎖によってどんな形の確率分布でも機械の中に作り出すことができるようになり，モンテカルロ法によってその確率分布から乱数を取り出すことができるようになったのである。\n確率分布と乱数の関係はこれまでに見てきた通りで，乱数ひとつひとつは確率分布の実現値であり，ありえる状態の一つでしかないが，これが大量になってくると全体的な傾向，すなわち確率分布の形状を目に見える形にしてくれるのである。数式では表現できない，複雑な事後分布の形であっても，そこから乱数を生成することはできる。そしてその乱数が大量に集まり、ヒストグラムをかいて稜線を眺めてみれば，それが事後分布の形である，と考えることができる。\nこの方法の第1の利点は，確率の計算を集計の問題に置き換えるところである。計算機能力の発達によって，数千，数万程度のデータの集計は一瞬でできるようになった昨今，パラメータについて数千の代表値を得て，その平均値を計算するのは容易いことである。確率分布による平均値，すなわち期待値の近似値として，この平均値を用いることができる。その近似値の精度はMCMCのサンプルサイズに依存するが，サンプルを一桁上げるとその精度も一桁上がるわけだから，乱数を大量に作ることでその精度は一気に向上させられる。\nこの方法の第2の利点は，積分計算が容易になることである。複数のパラメータを持つ確率分布は，多次元空間における確率分布である。そのなかで特定のパラメータに注目する場合，それパラメータ以外のパラメータは不要なので積分によって周辺化する必要がある。数式でこれを行うと，多重積分になって非常に面倒が生じるが，乱数生成アプローチの場合は当該パラメータについてのみ代表値を計上すれば良いので，非常に簡単である。\nあくまでも近似に過ぎないと言われればそうだが，この方法はかなり強力で，事前分布や尤度など考えるべき確率分布とその組み合わせ方を適切に設定すれば，事後分布の形を計算できなくても事後分布からの乱数は得られるのである。この組み合わせの設定は，確率型プログラミング言語によって実装される。確率型プログラミング言語では，事前分布と尤度(データがどの確率分布から出てきているか)を記述するだけで，そのまま事後分布からの乱数を生成することができる。ユーザは自分の好きな確率分布を好きな形で組み合わせることができるから，モデルの表現力が飛躍的に上がったのである。\n確率型プログラミング言語として，代表的なものはJAGSとStanである。現在はStanが主流である。StanはRだけでなく，Pythonなど他の言語からも利用可能である。RでStanを利用するには，パッケージとしてcmdstanrを用いるのが一般的である。このパッケージを導入するにあたっては，cmdstanrのホームページを参照してほしい。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter12.html#ツールの導入stanとbrms",
    "href": "chapter12.html#ツールの導入stanとbrms",
    "title": "12  ベイズ統計の活用",
    "section": "12.3 ツールの導入；Stanとbrms",
    "text": "12.3 ツールの導入；Stanとbrms\nStanの導入には，環境によって若干の違いがあるので，公式のホームページを参照してほしい。2025年3月現在公式ページからGet Startedへと進むと，OSとインターフェイス，インストーラをどこにするかを選択すると，導入に必要なもの(Requirements)と導入方法が表示される。\n\n画面ではOSとしてMacOSを選んでいるが，ここは各自の環境に合わせてもらいたい。インターフェイスはCmdStanRを選択して欲しい。Stanを実行するにはCコンパイラが必要であり，またCmdStanRはコマンドラインからStanを実行してRに繋げるという代物で，cmdstanr::install_cmdstan関数を実行した後，インストール先のパスを設定する必要がある。\nCmdStanRはstan言語で書いた確率モデルを実行し，計算機内部で事後分布を作ってその代表値(MCMCサンプル)を出力させることができる。自ら確率モデルを書くことができるので自由度が高いが，線形モデルに限定して実行するのであれば，Stanを開発しているのと同じチームが提供するbrmsパッケージが便利である。\nbrmsパッケージを用いれば，Rのformulaの指定の仕方で一般化混合線形モデル，階層線形モデルなどが表現できる。これらの非ベイズ推定版であるlmerパッケージとその書式が同じなので，非常に使いやすい。このパッケージの導入は，一般的なCRANからのインストールでも可能である。詳しくはbrmsのサイトを参照してほしい。\n\ninstall.packages(\"brms\")\n\nこれらの環境の準備ができたものとして，使い方を見ていこう。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter12.html#ベイズ法による推定の例",
    "href": "chapter12.html#ベイズ法による推定の例",
    "title": "12  ベイズ統計の活用",
    "section": "12.4 ベイズ法による推定の例",
    "text": "12.4 ベイズ法による推定の例\n\n12.4.1 パラメータリカバリによる確認と結果の読み取り\n前回に倣って，回帰分析のモデル式にそってデータを生成し，分析によってパラメータリカバリを行ってみよう。\n説明変数については制約がないので一様乱数から生成し，平均0，標準偏差\\(\\sigma\\)の誤差とともに被説明変数を作り，従来のやり方で推定してみよう。\n\npacman::p_load(tidyverse)\nset.seed(123)\nn &lt;- 500\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 1\n# データの生成\nx &lt;- runif(n, -10, 10)\ne &lt;- rnorm(n, 0, sigma)\ny &lt;- beta0 + beta1 * x + e\n\ndat &lt;- data.frame(x, y)\nresult.lm &lt;- lm(y ~ x, data = dat)\nsummary(result.lm)\n\n\nCall:\nlm(formula = y ~ x, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82796 -0.61831  0.03553  0.69367  2.68062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.021928   0.045010   44.92   &lt;2e-16 ***\nx           3.002194   0.007919  379.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.006 on 498 degrees of freedom\nMultiple R-squared:  0.9965,    Adjusted R-squared:  0.9965 \nF-statistic: 1.437e+05 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\n結果は切片2.0219277，傾き3.0021943であるから，設定した\\(\\beta_0 = 2,\\beta_1 = 3\\)が正しく復元できた，という話であった。\nこれは最尤法による推定であったが，brmsパッケージを使ってベイズ推定に変えてみよう。 方法は次のとおりである。\n\npacman::p_load(brms)\nresult.bayes &lt;- brm(y ~ x, data = dat)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 1:                0.01 seconds (Sampling)\nChain 1:                0.022 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 2:                0.01 seconds (Sampling)\nChain 2:                0.022 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 3:                0.009 seconds (Sampling)\nChain 3:                0.02 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 4:                0.01 seconds (Sampling)\nChain 4:                0.021 seconds (Total)\nChain 4: \n\n\n実行に際して，Compiling Stan program...との文字が表示されるが，これはbrmsパッケージが内部でstan言語を書き，それをC言語に書き換えてコンパイルしていることを意味する。他にもいろいろ出力されているが解説は後述する。簡単なモデルなので，すぐにプロンプトが待機状態に戻るはずである。\nさて，summary関数で結果の要約を見てみよう。\n\nsummary(result.bayes)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: dat (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.02      0.05     1.93     2.11 1.00     3576     2877\nx             3.00      0.01     2.99     3.02 1.00     4181     2652\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.01      0.03     0.95     1.07 1.00     3602     2759\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nまずRegression Coefficientsのところを見てみよう。とりあえずリカバリーが上手くいってるか見てみたいからだ。推定値Estimateのところに，2.0214066と3.0022318とあるから，なるほど正しく推定できているようである。その後に標準誤差(SE)があるのはいいとして，その次にあるのがl-95%CIとu-95%CI，すなわちlower-upperで表される区間である。\nベイズ統計学ではわからないものを確率分布として表現する。今回わからなかったものは回帰係数だから，切片\\(\\beta_0\\)と傾き\\(\\beta_1\\)はそれぞれ確率として表現され，結果も確率分布(事後分布)として得られている。つまり，ここに表されているのは事後分布の95%区間であり，\\(\\beta_0\\)は1.9328496, 2.1114449の間に分布しているもの，という結果になっている。\n最尤推定が点推定であったのに対し，ベイズ推定はこのように分布で表現されるから，表示されている推定値もその代表値である。代表値の取り方はご存知の通り，平均値，最頻値，中央値などが考えられ，そのいずれもが推定値として用いられる。平均値による代表値はExpectation A Posteriori，EAP推定値と呼ばれる。中央値による代表値はMEDian A Posteriori，そして最頻値というより分布のピークを取る推定値のことをMaximum A Posteriori，MAP推定値という。ここで得られた事後分布は分布の関数形によるものではなく，事後分布の代表値の集積による稜線で形を見ているに過ぎないから，MAP推定値を得る方法は1.ヒストグラムを書いてその最頻値である級数の平均値をとる，2.ヒストグラムにフィットする関数を近似し，そのピークを算出する，といった方法が考えられる。計算そのものはパッケージに含まれる関数を利用すれば良い。ここで大事なことは，分布の形状に応じてその代表値を選ぶことである。すなわち，正規分布のような左右対称の分布であれば，平均値，中央値，最頻値は同じ値になるが，ここが異なるようであれば歪んだ分布をしていることが考えられるから，事後分布のヒストグラムを描いて確認した上で，中央値やMAP推定値などを用いるといいだろう。\n\n\n12.4.2 MCMCを評価する\n結果はその他にもいろいろな情報を提供してくれているので，見ていこう。summary関数によって表示された出力の5行目，DrawsのところにMCMCサンプリングの情報が表示されている。これによると，4つのチェインがあり，そのそれぞれが2000回反復(iter)されたこと，そのうちの最初の1000回はwarmupと呼ばれるステップであったこと，最後にthinというのがあるが，これはMCMCサンプルを間引きするためのもので，ここでは1回毎にサンプルを取っている，すなわち毎回のサンプルを用いたことがわかる。\nMCMCは事後分布を作り，そこから乱数を取り出すステップであったこをと思い出そう。乱数を取り出すステップは，まず適当な初期値から始め，次に高次元同時確率空間の中である方向に移動する。その場所からまた次の場所を選ぶ，と次々とステップを進めていく形で事後分布の代表値を拾い集めてゆく。今回の例で言うと，\\(\\beta_0,\\beta_1,\\sigma\\)という3つのパラメータを推定しているので3次元空間を探索する。この空間のある座標は，この3つのパラメータが取りうる可能性のある値の組み合わせである。この空間で，初期値\\(t_0\\)の座標に立ち，近傍の別の座標\\(t_1\\)に移動する。この\\(t_1\\)の座標も，この3つのパラメータが取りうる別の可能性の組み合わせである。同様に\\(t_1\\)の近傍\\(t_2\\),\\(t_2\\)の近傍\\(t_3\\)，とステップを踏むことで，それぞれのステップがMCMCサンプルの1つとして記録されていく。この記録の集約が事後分布の近似になる。\nこのような形でサンプリングが進むことを考えると，まず懸念されるのが「初期値によって結果が変わるのではないか」ということである。実際，初期値によっては，サンプリングがうまくいかないことはある。うまく推定できるときは，どんな初期値から発生しても，事後分布が作る空間の密度の濃いところからサンプリングが進むので，同じような値を集めてくることができるだろう。\nMCMCがうまくいってるかどうかを評価するために，MCMCでは一般的に複数の初期値から始め，別々のステップを踏んでログを取る。このステップのログをチェインと呼ぶ。つまり，それぞれのチェインは異なる初期値から始まる一連の代表値の連なりなのである。今回の結果では，4つのチェイン，つまり4つの初期値から始まったことがわかる。\nさて，もう一度サンプリングの進み方を思い返してみよう。初期値から始めてステップしている最初のうちは，最終的に目的としている事後分布の代表値からは大きく外れているかもしれない。ステップを繰り返すことで，より事後分布の密度の濃いところに近づいていくのだから，最初のうちはうまくいってなくても当然である。この最初のうちのステップは代表値として信用できないので，「バーンインburn in期間だった」ということで切り捨てることが一般的である。brms が採用しているMCMCアルゴリズムのstanは，この初期の探索時に，効率よくサンプリングできるようにステップサイズ(どれぐらい遠くの近傍まで考えるか)等アルゴリズムを自動で調整する期間を設定している。これを特にウォームアップ期間という。\n結果に戻ってみてみると，4つのチェインで2000ステップ(iter)を踏んでいるが，ウォームアップ期間が1000あるので，実際にサンプリングが行われたのは1000ステップ以降である。なのでtotal post-warmup draws =4000となっている。\nthinは，サンプリングの間引き間隔を指定するものであった。ステップ毎に代表値のログを取っていくとしたが，原理的にはそれぞれのステップ・代表値は事後分布から独立にサンプリングされたものであるはずだ。MCMCの結果を見て，ステップ毎の自己相関を確認し，もし\\(t-1\\)時点の代表値が\\(t\\)時点の代表値に影響を与えているようであればよろしくない。そこで間引きをすることで，そのような影響を与えるようなステップを省略することで，より独立性の高いサンプルを得ることができる，と考えるのである。間引きをすると事後分布からのサンプルの数が減ることになる。また，自己相関が高いようなMCMCサンプルしか得られないのは，モデルやパラメタライゼーションが不適切である場合が多く，thinオプションを使うのは結果オブジェクトのサイズを減らす目的である，と考えた方がいいだろう。\n推定値の出力結果に，RhatとBulk-ESSという指標がある。Rhatは，チェイン間の自己相関を表す指標で，1に近いほどよいとされる。基準として，全てのパラメータにおけるRhatが1.1未満であれば，複数のチェインが絡み合った，良いサンプリングであったと評価される。逆に，Rhatが1.1以上であれば，初期値によって異なるパラメータ空間を探索していたことになり，事後分布からの適切な代表値ではない可能性があるので，モデルの見直しなどが必要になる。Bulk-ESSは，有効サンプルサイズEffective Sample Sizeの略であり，実際に独立したサンプルが何個分の情報を持っているかを推定する指標である。これの量的目安は大まかにいって，3桁以上の数字があれば良いと言われる。逆に1，2桁の数字しか示されないのであれば，有効なサンプリングができていないと考えて，モデルの見直しなどが必要になる。\n\n\n12.4.3 可視化してモデルの評価をみる\n結果を可視化してみるとわかりやすい。次のコードは，推定されたパラメータの事後分布のヒストグラムと，トレースプロットと呼ばれるものを描画する。\n\nplot(result.bayes)\n\n\n\n\n\n\n\n\nトレースプロットとは，チェイン毎のステップログを描画したものである。チェインがうまく混合しているかどうかを確認するためのものであり，今回は4つのチェインが混じり合った状態であるから，事後分布からのサンプリングとしてはうまくいっていると考えられる。\n\n\n12.4.4 MCMCサンプルを確認する\nこれら今回の出力はすでに要約されたものになっているが，具体的にどのようなMCMCサンプルが得られているか確認してみよう。結果オブジェクトから，brmsパッケージに含まれるas_draws_df関数を用いて，MCMCサンプルをデータフレームとして取り出すことができる。\n\nmcmc_samples &lt;- brms::as_draws_df(result.bayes)\nmcmc_samples %&gt;%\n  as.data.frame() %&gt;%\n  head()\n\n  b_Intercept      b_x     sigma Intercept    lprior      lp__ .chain\n1    2.005643 3.007028 0.9670094  1.722002 -7.430387 -720.0947      1\n2    2.034874 2.998960 0.9822221  1.751993 -7.430557 -719.4520      1\n3    2.139143 2.994792 0.9970785  1.856656 -7.431063 -723.0209      1\n4    1.997631 3.008462 1.0296125  1.713854 -7.430537 -719.7706      1\n5    2.075360 3.003420 1.0175833  1.792059 -7.430834 -719.8164      1\n6    2.039586 2.998572 0.9869394  1.756742 -7.430591 -719.4091      1\n  .iteration .draw\n1          1     1\n2          2     2\n3          3     3\n4          4     4\n5          5     5\n6          6     6\n\n\nいま取り出したmcmc_samplesは，as_draws_df関数の出力によって，data.frame型の特殊系になっているので，改めてas.data.frame関数を用いてデータフレームに変換し，最初の数行を表示させている。 これの後ろの3列に，.chain,.iteration,.drawという列があるが，これはそれぞれMCMCサンプルのchain番号，サンプルの通し番号，サンプルの通し番号である。\nすでに述べたように，デフォルトでは4つのチェインからサンプリングを行う。画面に表示されているのは，第1チェインの1，2，…,6番目のステップ＝サンプルである。つまり，各行が3次元同時確立空間の代表値であることを指している。\nb_Interceptは切片\\(\\beta_0\\)のサンプル，b_xは傾き\\(\\beta_1\\)のサンプルである。また今回は単回帰分析であるから，\\(Y \\sim N(\\beta_0 + \\beta_1 x, \\sigma)\\)というモデルを推定していたわけで，sigmaはこの\\(\\sigma\\)のサンプルである。Interceptは\\(\\beta_0 + \\beta_1 x\\)，すなわち正規分布の位置を表している。\nlpriorはLog Priorの略で，パラメータの事前分布の対数を表している。lp__はLog Posteriorの略で，パラメータの事後分布の対数を表している。いずれも，モデルの推定に関係する情報として提供されているが，今ここは気にしなくてもいいだろう。\n次のコードは，MCMCサンプルの要約統計量を使って事後分布を記述したものである。MAP推定値については，Rのdensity関数を使って，観測データからカーネル密度推定（Kernel Density Estimation, KDE）を計算し，そのピークの度数を取ることで，MAP推定値を算出している。\n\n# MAP推定用の関数を定義\nfind_map &lt;- function(x) {\n  density_obj &lt;- density(x)\n  return(density_obj$x[which.max(density_obj$y)])\n}\n\nmcmc_samples %&gt;%\n  as.data.frame() %&gt;%\n  select(b_Intercept, b_x, sigma) %&gt;%\n  rowid_to_column(\"iter\") %&gt;%\n  pivot_longer(-iter) %&gt;%\n  group_by(name) %&gt;%\n  summarise(\n    EAP = mean(value),\n    MAD = median(value),\n    MAP = find_map(value),\n    SD = sd(value),\n    L95 = quantile(value, probs = 0.025),\n    U95 = quantile(value, probs = 0.975),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 7\n  name          EAP   MAD   MAP      SD   L95   U95\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 b_Intercept  2.02  2.02  2.02 0.0458  1.93   2.11\n2 b_x          3.00  3.00  3.00 0.00786 2.99   3.02\n3 sigma        1.01  1.01  1.00 0.0318  0.948  1.07\n\n\n今回は，ベイズ統計の位置付けとMCMC法によるベイズ推定の実際を，パッケージを用いて説明した。 brmsパッケージは線形モデルやその応用において非常に強力であり，ベイズ統計の実践においては，これを用いることが多いだろう。ただし，線形モデルでないモデルについては，自分でstanのコードを書いて推定することも多い。線形モデルの限界に囚われず，自由な統計モデリングの世界があることも視野に入れておいてほしい。\n\n\n12.4.5 brmsパッケージのオプション\nベイズ推定には事前分布が必要である。しかしbrm関数を実行した時に，事前分布は特段指定しなかった。これはパッケージがデフォルトで用意した事前分布を用いたからである。 どのような事前分布が用いられたかを確認するには，以下のようにする。\n\nbrms::get_prior(result.bayes)\n\n                   prior     class coef group resp dpar nlpar lb ub\n                  (flat)         b                                 \n                  (flat)         b    x                            \n student_t(3, 0.3, 21.3) Intercept                                 \n   student_t(3, 0, 21.3)     sigma                             0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nこれをみると，回帰係数にはflatな事前分布が用いられていることがわかる。これはデータから考えられた取りうる範囲が全て等確率な，一様分布を次全部ぷとしたことを表している。これは無情報時全部分布と呼ばれる。\n残差の分散\\(\\sigma\\)については，student_t(3, 0, 21.3)，すなわちt分布が使われていることがわかる。具体的にこの分布がどのような形状か，描いてみてみよう。\n\ndf &lt;- 3\nmu &lt;- 0\nsigma &lt;- 21.3\n\ncurve(2 * dt((x - mu) / sigma, df) / sigma,\n  from = 0, to = 100,\n  col = \"blue\", lwd = 2, main = \"Half-Student-t Distribution\",\n  xlab = \"sigma\", ylab = \"Density\"\n)\n\n\n\n\n\n\n\n\n分散は正の値しか取らないので，brms は student_t(3, 0, 21.3) を 半分に折りたたみ、半t分布（half-Student-t） に変換して利用している。t分布は正規分布より裾の重い分布であり，大きな値が出る可能性も考慮されている。分散の事前分布には他にも，cauchy分布やexponential分布などが用いられることもある。\n\n\n12.4.6 MCMCサンプリングの設定\nbrmsパッケージでは，MCMCサンプリングの設定もできる。例えばチェインの数を増やすとか，warmup期間を変えるとか，間引きを変えるとか，いろいろな設定が可能である。また，再現性を確保するために乱数のシードを固定することもできる。これらの設定をしたコードの例を示す。必要に応じて，いろいろな設定を試してみるといいだろう。特に事前分布については，いろいろ変えても結果が大きく変わらない方が，推定値の信頼性も高まると考えられる。事前分布の変更によって，事後分布がどの程度影響されるかを分析することを感度分析というが，このためにも事前分布をデフォルトに任せず設定したり，デフォルトを利用したとしてもどの設定にしているのかを確認できるようになっておこう。\n\n# 事前分布の設定\npriors &lt;- c(\n  set_prior(\"uniform(0, 100)\", class = \"Intercept\"), # 切片: 一様分布(0, 100)\n  set_prior(\"normal(0, 10)\", class = \"b\"), # 回帰係数: N(0, 10)\n  set_prior(\"cauchy(0, 5)\", class = \"sigma\") # 標準偏差: Cauchy(0, 5)\n)\n\nfit &lt;- brm(y ~ x,\n  data = dat,\n  prior = priors,\n  iter = 3000,\n  warmup = 2000,\n  chains = 3,\n  seed = 12345,\n)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter12.html#課題",
    "href": "chapter12.html#課題",
    "title": "12  ベイズ統計の活用",
    "section": "12.5 課題",
    "text": "12.5 課題\n以下のデータセットは被説明変数\\(y\\)，説明変数\\(x1,x2\\)からなる重回帰分析のサンプルデータです。データセット全体(\\(n=100\\))はex_regression3.csvからダウンロード可能です。このデータセットを用いて，brmsパッケージによる重回帰分析を実行してください。\n分析結果について，以下の項目を順に報告してください：\n\n回帰係数（切片，\\(x1\\)，\\(x2\\)）の推定値と95%信用区間\nMCMCの収束診断（RhatとBulk-ESSの値に基づいて判断）\n各パラメータの事後分布のヒストグラムとトレースプロット\nデフォルトで使用された事前分布の確認と説明\nMCMCサンプルからのMAP推定値の算出（本章で示したfind_map関数を利用）\nMCMCサンプルに基づく90%信用区間と75%信用区間の算出\n\nなお，レポートには使用したRコードと，各項目の結果に対する簡潔な解釈を含めてください。\n\n\n\n\nシャロン・バーチュ・マグレイン. (2011) 2018. 異端の統計学ベイズ. Translated by 冨永星. 草思社.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ベイズ統計の活用</span>"
    ]
  },
  {
    "objectID": "chapter14.html",
    "href": "chapter14.html",
    "title": "14  多変量解析(その1)",
    "section": "",
    "text": "14.1 因子分析\nここでは心理系で最もよく使われる分析法のひとつ，因子分析を中心に多変量解析の全体的な解説を行う。 多変量解析はその名のとおり，変数が多く含まれるデータの解析であり，その目的は情報の要約にある。多くの変数が含まれる時にひとつ一つの変数を解釈していくのは大変な労力であるから，要領よくまとめることができればそれに越したことはないからである。\nこの目的から派生して，多変量解析にはさまざまな意味解釈が付随する。以下にモデルの解釈と対応する多変量解析技術を列記してみた。\nこれらのモデルに共通する本質的な特徴は「変数間関係をデータから見出すこと」である。変数間関係をどのようなもので表現するかによって，多少モデルの扱い方は変化する。一般的には分散共分散(相関係数)を変数間関係とするが，この場合は間隔尺度水準以上のデータが得られていることが必要になる。もし順序尺度水準でしか得られていないのであれば，ピアソンの相関係数の代わりにポリコリック相関係数やポリシリアル相関係数と呼ばれる相関係数を用いることになる。0/1のバイナリデータの場合はさらに特殊で，ピアソンの相関係数の代わりにテトラコリック相関係数を用いることになる。\n変数間関係は必ずしも相関係数だけではない。カテゴリカル変数の場合は，あるカテゴリと他のカテゴリが同時に選択される・発生する頻度(共頻関係)を，その変数間関係の指標として捉えることができる。共頻関係を分析する手法としては双対尺度法(西里 2010) (対応分析や数量化III類と原理的に同じ)などが知られている。このようなカテゴリデータの分析は，自由記述などの自然言語を形態素解析し，多変量解析で分析するテキストマイニングなどで応用されている。\nまた，変数間関係を変数同士の類似度，すなわち距離であると考えることもできる。距離の公理を満たすデータがあれば，それを元に多次元尺度構成法(高根 1980; 岡太 and 今泉 1994)で可視化したり，クラスター分析で分類(新納 2007; 足立 2006)したりすることができる。\nさらに，相関係数は変数間の直線的な関係の強さを意味するが，隠れた変数による擬似相関の可能性も含まれるため，偏相関係数を使って周囲の変数からの影響を統制した変数間関係を考えることもある。この手法はグラフィカルモデリング(宮川 1997)やネットワーク分析(アデラ＝マリア et al. [2022] 2024)で用いられるものである。\nいずれにせよ，こうした変数間関係をもとに，外的な基準があればそこに対するフィッティングを目的として未知数を推定するし，外的な基準がなければモデル的仮定に基づいてデータから構造化していくことになる。またその目的の基本は情報の要約であるが，可視化に重点をおいたモデルや潜在得点の推定に重点を置いたモデルなど，各種モデルによって得意とする場所や理論的に強調されるところは異なる。\nやや本筋から外れるが，こうした多変量を同時に扱うための数学的基盤として，線形代数の知識が必要となることが少なくない。線形代数はベクトルや行列の演算体系であり，その利点は「計算」と「可視化」を統合する観点を得られるところにある。より深く理解したいものにとっては，これらの学習も合わせて行うことを期待する。\nここでは心理学で最もよく用いられる手法の一つである，因子分析法について概説する。 因子分析法は測定についての統計モデルである。類似の手法として主成分分析があげられるが，主成分分析は測定のモデルというより要約のモデルというべきである。\n因子分析のモデルは次の式で表される。 \\[ z_{ij} = a_{j1}f_{i1}+a_{j2}f_{i2}+cdots+a_{jm}f_{im}+d_jU_{ij} \\]\nここで\\(z_{ij}\\)は個人\\(i\\)の項目\\(j\\)に対するスコアを標準化したものである。\\(a_{j.}\\)は項目\\(j\\)の因子負荷量(factor loadings)，\\(f_{i.}\\)は個人\\(i\\)の因子得点(factor score)，\\(d_j\\)は項目\\(j\\)の独自因子負荷量，\\(U_{ij}\\)は項目\\(j\\)に伴う個人\\(i\\)の独自因子得点である。\\(a_{j.}\\)で表される\\(m\\)個の因子を共通因子と呼ぶ。一般に\\(m\\)は項目数\\(M\\)よりもかなり小さい。例えば性格検査のBIG-fiveは\\(M=25\\)で\\(m=5\\)である。YG正確検査は\\(M=120\\)で\\(m=12\\)である。この意味で多変量解析の目的の一つ，情報圧縮のモデルであるということもできる。\nこれに対して主成分分析は次のように表される。 \\[ P_{i} = w_1X_{i1} + w_2X_{i2} + \\cdots + w_MX_{iM} \\]\nここで\\(X_{i.}\\)は個人\\(i\\)の項目\\(j\\)に対する反応を表し，この重み付き線型結合で主成分\\(P\\)を形成する。ここでの未知数は\\(w_j\\)であるから，一つの合成変数に作るための最適な重みを見つけることが目的となる。その基準の一つが，生成される合成変数\\(P\\)が個人\\(i\\)の特徴を最大限際立たせるように，すなわち\\(P_i\\)の分散を最大にすることと考える。もちろん一つの合成変数で\\(M\\)個の変数が持つ情報をすべて反映させることは難しいので，第二，第三の合成得点(主成分)を作ることも可能である。\nそうすると，情報圧縮という観点から見た\\(m\\)個の共通因子，\\(m\\)個の主成分の違いは何だろうか。これはすでに述べたように，因子分析は測定のモデルなので，得られたデータ\\(z_{ij}=\\frac{X_{ij}-\\bar{X_j}}{\\sigma_j}\\)には誤差\\(d_jU_{ij}\\)が含まれていると考えているのに対し，主成分分析では\\(X_{ij}\\)にそれを仮定せず，得られた値をそのまま用いているところが異なる。\n実践的な面では，心理尺度のような反応に誤差が仮定されるものには因子分析を用い，公的な記録など値に誤差が想定されないものには主成分分析を用いることが相応しい。因子分析が心理学やテスト理論の領域で広まり，主成分分析が経済学，商学，社会学の領域で広まったのはそうした背景による。\n計算論的には，いずれも変数間関係を元に最大限説明できる要素を抽出するというところで，行列の固有値分解を用いるという点が同じであるから，統計パッケージによっては同じメニューで異なる出力になっているものも少なくない。しかし上で述べたように，モデルの設計上の違いがあることは知っておいて損はないだろう。また，因子分析は変数間関係として相関行列を，主成分分析は分散共分散行列を用いることが多い。これは因子分析を用いる心理学的な領域では，測定値に絶対的な意味がなく相対的な意味(ex.より外向的，より内向的)しかないことに対し，他の社会科学領域では絶対的な意味がある(ex.国家間の貿易黒字・赤字の額など)場合が多いからである。また，主成分分析は多くの変数を情報圧縮する目的で第一主成分のみに注目することが多いのに対し，因子分析は測定しているものの考え方から複数の因子を考えることが多い。因子分析において単因子で考えるか多因子で考えるかについては，知能検査において知能を一般的な単一の因子で考えるのか，各領域に対応する複数の因子があるのかといった，理論的な相違がその黎明期にみられたことを反映している。\n類似の手法ではあるが，こうした背景を知っておくことで適切な手法を用いることができるようになるだろう。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#因子分析",
    "href": "chapter14.html#因子分析",
    "title": "14  多変量解析(その1)",
    "section": "",
    "text": "14.1.1 探索的因子分析\n特に断りなく単に因子分析というとき，探索的因子分析(Exploratory Factor Analysis)を指すことが多い。探索的というのは，因子負荷量(因子から項目へのパス係数，因子と項目の関係の強さを反映したもの)はもちろん共通因子の数についても事前に定めず，データから因子構造を探ることを目的とするものだからである。\n探索的因子分析は次のステップで進められる。\n\n因子数の決定\n因子負荷量の推定(因子軸の回転)\n因子得点の推定\n\nもちろん分析に入る前に，分析対象となるデータの記述統計や可視化を通じて基本的な項目属性を把握していることが前提である。\n\n14.1.1.1 因子数の決定\n因子分析を数学的に語れば，\\(M\\)個の項目相互の相関係数を表した相関行列\\(\\mathbb{R}\\)を固有値分解することに尽きる。相関係数はピアソンの積率相関係数を用いることが一般的であるが，項目が順序尺度水準であるとかバイナリ変数であるとかいった場合は，それに応じた相応しい相関係数を用いる。\n固有値分解とは相関行列の次元性を見ることでもある。\\(M\\)個の項目もつ情報は\\(M\\)次元あると考える。例えば2変数\\(X,Y\\)があれば，変数\\(X\\)をx軸，変数\\(Y\\)をy軸に取った2次元空間に核反応をプロットすることでデータ全体の関係を表現できるだろう。しかしこれらの二変数が相関しているなら，変数\\(X,Y\\)を直交させた空間で表現する必要は必ずしもなく，より分散の大きくとれる二次元基底を見つけることができるに違いない。これが因子分析，主成分分析に共通する考え方であり，最大の分散を持つ次元に注目するのが主成分分析，多次元のなかで有用な次元を共通因子，それ以外を誤差因子と区別して多因子(多次元)で考えるのが因子分析である。\nここで共通因子の数を決めるのは分析者であり，「有用な次元」の決定は主観的な側面を含むことに注意しよう。もちろんデータの構造から適した次元数を考える手法はいろいろ提案されており，昨今はより客観的基準で因子数を決定するのが一般的であるが，数学的な特徴から実践的な意味合いをもつ共通次元とみなすのは，あくまでも分析者の責任において行われるものである。\n因子数を決定する手法として，スクリープロットをつかった平行分析がある。 次のコードを見ながら具体的に見ていこう。分析にはpsychパッケージを用い，データはpsychパッケージの持つサンプルデータ，bfiを用いる。 これは性格テストのビッグファイブ因子それぞれについて5項目で測定したデータである。\n\nlibrary(tidyverse)\nlibrary(psych)\ndat &lt;- psych::bfi |&gt; select(-gender, -education, -age)\n# 並行分析\nfa.parallel(dat)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  6  and the number of components =  6 \n\n\nデータ行列から得られる固有値の大きい順に折れ線グラフを描いたものを，スクリープロットという。 デフォルトではPCすなわち主成分分析Principle Component Analysisのスクリープロットと，FAすなわち因子分析Factor Analysisのスクリープロットが表示されている。この違いは上で述べたように，データに誤差を仮定するかどうかの違いにある。因子分析はこれを仮定するため1つの項目のもつ情報量が1単位以下になる(相関係数\\(r_{jj}\\)が\\(1.0\\)より小さくなる。正確には，\\(r_{jj} = 1-h_j^2 = u^2 &lt; 0\\)であり，ここで\\(h_j^2\\)は共通性と呼ばれる共通因子負荷量の二乗和，\\(u_j^2\\)は独自性因子負荷量の二乗和)ため，主成分分析のそれより必ず低くなる。\nプロットされているのはActual Data, Simulated Data, Resmapled Dataとなっているのがわかるだろう。実際のデータは何らかの意味構造を有しているだろうから，その相関関係にも偏りが生じ，よく説明できる次元とそうでない次元とが生まれるため，徐々に減衰するカーブで表示される。これに対してSimulated Dataは同じサイズの乱数データから，Resampled Dataは実際のデータをごちゃ混ぜにした行列を作って得られた固有値構造を表している。乱数や撹拌したデータは実際の意味構造を持たず，どの次元も均等に無意味になるため，フラットな線で表示されるだろう。このフラットな線と実データの線を比べ，フラットなラインよりも大きな意味がある次元は無意味ではない，と考えて因子数を決めるのが平行分析の考え方である。この考え方に基づくと，因子分析解も主成分分析解も6因子(6成分)がてきせつであるということになる。\nなお図中には固有値が1.0のところにもラインが引かれている。これはかつて使われていたガットマン基準というもので，項目1つ分の分散も持たないような因子は共通因子たり得ない，という考え方である。この考え方によると3因子が妥当ということになる。ただし判断の基準として，共通因子で分散全体の何%を説明したか，というのもあり，たとえば3因子までで50%も説明しないようであれば半分以上の情報を捨てることになるから，4，5因子まで採用するという考え方もあり得る。\n\n\n14.1.1.2 因子負荷量の推定\n因子の数が決まると，その過程のもとで因子負荷量の推定に入る。例えば次のようにして結果を得る。\n\nresult.fa &lt;- fa(dat, nfactors = 6, fm = \"ML\", rotate = \"geominQ\")\n\n要求されたパッケージ GPArotation をロード中です\n\n\npsychパッケージのfa関数は実に多くのオプションを持っているが，ここでは因子数(nfactors)，推定法(fm)，回転法(rotate)の3つを指定した。 因子数はすでに述べたので，推定法と回転法について解説する。\n推定法は，ここでは最尤法(ML)を指定した。サンプルサイズが200を超えるような大きなデータであれば，多変量正規分布のもとからデータが得られたと仮定して因子負荷量を推定するのが最も適切だろう。サンプルサイズが小さい場合は，最小二乗法系列(ULS,OLS,WLS,GLSなど)の推定法を指定し，データとモデルのずれを最も小さくするような手法にするのが良い。特段の指定がなければ最小残差法(minres)が選ばれる。これは最小二乗法と同じだが，アルゴリズムが改善されていて収束しやすいという特徴がある。推定法として主成分解(pa)を選べば，残差を推定しないモデルとなる。アルゴリズムの違い，仮定の違いなどでいろいろ変えうるが，基本的にこれで大きく変化が出るようなものではない。\n回転法は因子負荷量を推定した後で，さらに解釈をしやすくするためのものである。因子分析や主成分分析は，データの持ってる空間的特徴の軸を見つけ直すという説明はすでにした通りだが，この軸は原点こそ決まっているが，線形代数的変換によって軸を任意の方向に回転させることができる。であれば最も解釈がやりやすい方向に回転させるのが実践上便利である。この解釈がやりやすい方向というのを数学的に言い換えるならば，一つは項目と因子の関係が単純構造にあることだろう。単純構造とは，ある項目が特定の因子に寄与しているのなら，そのほかの因子には寄与していないということである。例えば，外向性を測定する項目が第1因子に重く負荷しているのであれば，第2,3,4,5因子には負荷していないほうが解釈しやすい。因子はデータの空間的特徴を表す軸(次元)なのだから，事後的にその軸がの意味であったかを考察する必要があるので，「この因子はこの項目にもあの項目にも影響している」という状況は悩みの種だからである。\nこの基本方針のもと，いくつかの計算法が考えられている。もっとも古典的なバリマックス回転は，因子負荷量の二乗和の分散が最大になるように回転角を定める。ほかにも，オブリミン回転やジオミン回転などさまざまな回転方法が考えられており，これらについて詳しくは 小杉 (2018) などを参照されたい。 また，回転方法は大きく分けて斜交回転と直交回転とに分けられる。直交回転は回転後の軸が直交する，すなわち因子間相関を仮定しない方法であり，斜交回転は因子間相関を仮定する回転方法である。後者の方が数学的な仮定が緩いため，分析の手順としてはまず斜交回転を行い，因子間相関が十分にひくく直交をかていできるなら直交回転をやり直す，という方法をとるべきである。ちなみにここではgeominQというジオミン回転の斜交版を適用して結果を出力している。\nBernaards and Jennrich (2005) パッケージには多くの回転法が含まれており，回転法をrotateオプションで選択することができるので，ヘルプなどを見て理解を深めて欲しい。\n因子軸の回転についても，推定法と同じように絶対的な基準はなく，それぞれの考え方や仮定に基づくアルゴリズムの違いがあるだけである。推定法と違って，因子負荷量は異なる回転法を施すと大きく変わることがある。因子軸の回転は解釈を容易にするためのものであるから，分析者にとって都合の良い回転方法を指定していいが，その回転方法が何で，どういう仮定があるのかについては，自身の言葉で説明できるようになっていた方がいい。\n\n\n14.1.1.3 出力結果を確認する\n推定法，回転法についての概略を踏まえた上で，結果を見てみよう。\n\nprint(result.fa, sort = T, cut = 0.3)\n\nFactor Analysis using method =  ml\nCall: fa(r = dat, nfactors = 6, rotate = \"geominQ\", fm = \"ML\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n   item   ML1   ML2   ML5   ML3   ML4   ML6   h2   u2 com\nE2   12  0.70                               0.55 0.45 1.0\nE1   11  0.58                               0.39 0.61 1.4\nN4   19  0.51  0.35                         0.48 0.52 2.0\nE4   14 -0.50                          0.33 0.56 0.44 2.2\nE5   15 -0.41                               0.40 0.60 2.8\nN2   17        0.84                         0.69 0.31 1.1\nN1   16        0.83                         0.71 0.29 1.1\nN3   18        0.61                         0.52 0.48 1.3\nN5   20  0.33  0.37                         0.34 0.66 2.8\nA2    2              0.70                   0.50 0.50 1.2\nA3    3              0.65                   0.51 0.49 1.1\nA1    1             -0.57              0.37 0.33 0.67 1.8\nA5    5              0.50                   0.48 0.52 1.7\nA4    4              0.42                   0.28 0.72 1.7\nC2    7                    0.67             0.50 0.50 1.2\nC4    9                   -0.60        0.35 0.55 0.45 1.9\nC3    8                    0.54             0.31 0.69 1.1\nC1    6                    0.53             0.35 0.65 1.4\nC5   10                   -0.51             0.43 0.57 1.8\nO3   23                          0.67       0.48 0.52 1.0\nO1   21                          0.58       0.34 0.66 1.1\nO5   25                         -0.49  0.41 0.37 0.63 2.0\nO2   22                         -0.40  0.34 0.29 0.71 2.3\nO4   24  0.40                    0.40       0.25 0.75 2.4\nE3   13                          0.38       0.48 0.52 3.0\n\n                       ML1  ML2  ML5  ML3  ML4  ML6\nSS loadings           2.34 2.25 2.00 1.89 1.77 0.82\nProportion Var        0.09 0.09 0.08 0.08 0.07 0.03\nCumulative Var        0.09 0.18 0.26 0.34 0.41 0.44\nProportion Explained  0.21 0.20 0.18 0.17 0.16 0.07\nCumulative Proportion 0.21 0.41 0.59 0.77 0.93 1.00\n\n With factor correlations of \n      ML1   ML2   ML5   ML3   ML4   ML6\nML1  1.00  0.24 -0.36 -0.20 -0.28 -0.08\nML2  0.24  1.00 -0.01 -0.12  0.05  0.25\nML5 -0.36 -0.01  1.00  0.19  0.28  0.26\nML3 -0.20 -0.12  0.19  1.00  0.14  0.04\nML4 -0.28  0.05  0.28  0.14  1.00  0.11\nML6 -0.08  0.25  0.26  0.04  0.11  1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 6 factors are sufficient.\n\ndf null model =  300  with the objective function =  7.23 with Chi Square =  20163.79\ndf of  the model are 165  and the objective function was  0.36 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  2762 with the empirical chi square  661.28  with prob &lt;  1.4e-60 \nThe total n.obs was  2800  with Likelihood Chi Square =  1013.79  with prob &lt;  4.6e-122 \n\nTucker Lewis Index of factoring reliability =  0.922\nRMSEA index =  0.043  and the 90 % confidence intervals are  0.04 0.045\nBIC =  -295.88\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   ML1  ML2  ML5  ML3  ML4  ML6\nCorrelation of (regression) scores with factors   0.90 0.93 0.89 0.87 0.86 0.78\nMultiple R square of scores with factors          0.81 0.86 0.78 0.76 0.75 0.61\nMinimum correlation of possible factor scores     0.61 0.72 0.57 0.53 0.49 0.22\n\n\nこの出力ではsortオプションとcutオプションを指定した。sortオプションは因子負荷量の大きい順に並べ替えてくれるものであり，cutオプションは因子負荷量の表示を抑制するものである。あくまで表示上のオプションであり，実際は各因子から各項目へのパス(\\(5 \\times 25\\)本)が計算されている。\nまず表示されているのが因子負荷行列であり，項目の因子ごとの負荷量に加え，共通性\\(h_j^2\\)と独自性\\(u_j^2=1-h_j^2\\)，複雑度complexityが示されている1。なお，ここで表示されている因子負荷量などは回転後のパターン行列であり，斜交回転の場合は，因子軸の負荷量をどう考えるかによって因子パターンと因子構造とに分かれる。因子パターンは変数を斜交座標系に直交に投影した影のようなものであり，変数から因子への直接的な効果を表すと考えられる。因子構造は因子構造は変数を各因子軸に平行に投影した影のようなものであり，変数と因子の間の単純相関を表している。\nその下には負荷量の平方和SS loagingsがあり，これが説明する分散の大きさである。それを比率にしたもの(Proportion Var)，累積比率にしたもの(Cumulative Var)がある。今回は累積して44%の説明しかしていないことになるから，56%もの情報をカットしているので，情報圧縮の観点から言えば少し捨てすぎている危険性もある。\n続いて，回転行列に斜交回転を指定しているから，因子間相関が出力されている。これを見ると絶対値最大で-0.36がみられる。全ての因子間相関が\\(\\pm 0.3\\)に収まるようであれば，直交回転を考えても良い。\nその後に出力されているのは適合度に関する指標である。各指標に関する解説は割愛する。\n\n\n14.1.1.4 因子得点の推定\nここまでで因子と項目の関係を探索的に求める方法について見てきたが，心理学的な研究としては因子と回答者の関係についても興味関心をもつだろう。すなわち，「外向性が高い人は誰か」「情緒不安定性が低い人はどういう特徴を持つか」といった，人に対する理解を深めることである。\n数学的には，行列の固有値分解のときに固有値と同時にえらえる固有ベクトルが因子負荷量になる。この相関行列などを構成する時点で，すでに個人の相がもつ情報は要約されて欠落している。なので，因子の構造が明らかになってから，逆算的に個人の得点を考えることになる。因子分析のモデル式で見たように，因子得点は\\(f_{i.}\\)であるが，右辺の因子負荷量がすでに定まっているのなら，左辺も実測値から与えられているので，方程式を解くように答えを求めることができるのである。\npsych::fa関数はデフォルトで因子得点を返すようになっており，以下のコードで確認できる。\n\nhead(result.fa$scores, 10)\n\n              ML1         ML2         ML5         ML3        ML4         ML6\n61617 -0.04010511 -0.13419381 -0.69145376 -1.29299334 -1.6430660 -0.12400705\n61618 -0.35267122  0.09080773 -0.04506789 -0.60267233 -0.0566277  0.39616337\n61620 -0.05290788  0.69698270 -0.68175428 -0.03704608  0.2334657  0.01280638\n61621  0.22194371 -0.07729865 -0.15482554 -0.90538802 -0.9125738  0.95278898\n61622 -0.39695952 -0.28825362 -0.61037363 -0.12382926 -0.5814368  0.21976607\n61623 -1.05223173  0.41585860  0.29450029  1.35906961  0.8457775  0.45985066\n61624 -0.43875292 -1.21974967  0.06875832  0.03201599  0.6001998 -0.69274350\n61629  1.42563085  0.35294078 -2.23058559 -0.99435391 -1.2166723 -1.00627947\n61630          NA          NA          NA          NA         NA          NA\n61633 -0.62399161  1.11010731  0.53805411  1.05139495  0.4870489  0.04325653\n\n\nここで一部NAが返されているところがある(例えばID 61630)，これは回答の中に欠測値が含まれていた場合におこる。\n\nhead(dat, 10)\n\n      A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4\n61617  2  4  3  4  4  2  3  3  4  4  3  3  3  4  4  3  4  2  2  3  3  6  3  4\n61618  2  4  5  2  5  5  4  4  3  4  1  1  6  4  3  3  3  3  5  5  4  2  4  3\n61620  5  4  5  4  4  4  5  4  2  5  2  4  4  4  5  4  5  4  2  3  4  2  5  5\n61621  4  4  6  5  5  4  4  3  5  5  5  3  4  4  4  2  5  2  4  1  3  3  4  3\n61622  2  3  3  4  5  4  4  5  3  2  2  2  5  4  5  2  3  4  4  3  3  3  4  3\n61623  6  6  5  6  5  6  6  6  1  3  2  1  6  5  6  3  5  2  2  3  4  3  5  6\n61624  2  5  5  3  5  5  4  4  2  3  4  3  4  5  5  1  2  2  1  1  5  2  5  6\n61629  4  3  1  5  1  3  2  4  2  4  3  6  4  2  1  6  3  2  6  4  3  2  4  5\n61630  4  3  6  3  3  6  6  3  4  5  5  3 NA  4  3  5  5  2  3  3  6  6  6  6\n61633  2  5  6  6  5  6  5  6  2  1  2  2  4  5  5  5  5  5  2  4  5  1  5  5\n      O5\n61617  3\n61618  3\n61620  2\n61621  5\n61622  3\n61623  1\n61624  1\n61629  3\n61630  1\n61633  2\n\n\n因子得点はモデル式から逆算的に推定するので，一箇所でも値が見つからなければ答えが出ないのである。 また，この推定法による因子得点は標準化されたスコアなので単位がなく，相対的に比較することしかできない。 加えて，推定された相対的なスコアであるから，以下の研究プロセスにおいて差の検定などをすることは不適切である，という考え方もある。\n実践的には簡便的因子得点と呼ばれる因子得点の計算方法がある。これは因子分析の結果，当該因子に関係する項目に着目し，その評定値を平均することで算出するものである。 先の具体的にみていこう。第一因子はE2,E1,N4,E4,E5から構成されていると考えたとする。ここでE4,E5は因子負荷量が負であるから，評定値を逆転して考える必要がある。これを踏まえて，たとえば以下のように計算する。\n\nFscore1.raw &lt;- dat |&gt;\n  # 第一因子に該当しそうな項目だけ抜き出す\n  select(E2, E1, N4, E4, E5) |&gt;\n  # 逆転項目の評定値を反転する\n  mutate(\n    E4 = 7 - E4,\n    E5 = 7 - E5\n  )\n\n# 行ごとに欠損値を除いた平均値を計算する\nFscore1 &lt;- apply(Fscore1.raw, 1, function(x) mean(x, na.rm = TRUE))\nsummary(Fscore1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   2.800   2.892   3.600   6.000 \n\n\nここでは6件法の評定値を逆転させるために，7から引くという操作をして，欠測を除いて平均するようにしている。 こうすることで，尺度値の持つ意味(中点以上が賛成，未満が反対といったような)を踏まえて考えることができるし，全てが欠測でない限りスコアの算出ができるという利点がある。\nただしこの方法は，因子負荷量による項目ごとの重みづけを考えないこと，因子分析法によって除外したはずの誤差分散の情報を含んだスコアにしていること，といった短所をもつ。また，そもそも評定値が尺度構成法で正しくスコアリングされたものであるべきだが，実践的にそのような工夫をしている例はほとんど見られないため，非常に精度の低い，荒い推定値になっていると言わざるを得ない。\nとはいえ，推定法でもとめたものと簡便法で求めたものは，非常に高く相関するので，心理学のデータがそれほどの精度を持つものでないと割り切れるのであれば，勘弁法でも十分だろう。\n\ncor(result.fa$scores[, 1], Fscore1, use = \"pairwise\")\n\n[1] 0.9595003\n\nplot(result.fa$scores[, 1], Fscore1)\n\n\n\n\n\n\n\n\n\n\n\n14.1.2 確認的因子分析\nここまで探索的因子分析について詳しく解説してきた。 そこでは因子数や因子負荷量は事前の情報がなく，データによって語らせる方法で後付け的に解釈を行なっていくのであった。数値例にもあるように，第一因子は主にE因子の項目(外向性,Extraversion)から構成されているが，中にはN因子(情緒不安定性,Neuroticism)の項目も一部含まれており(N4)，解釈に頭を悩ませることも少なくない。そもそもBig5の名前にあるように理論的には5因子なのだが，データは6因子を示す，ということもある。\nこのように探索的因子分析はデータに沿った解釈をするしかないのだが，性格検査のような理論的背景や仮定があるのなら，そちらを重視したいということもあるだろう。 このような場合は，因子の構造や仮定を盛り込んだモデルをデータに当てはめるという，確認的因子分析(Confirmatory Factor Analysis, CFA)を用いる。これは構造方程式モデリング(Structural Equation Modeling)の枠組みで因子分析をとらえたものであり，項目と潜在変数の関係を方程式で表して推定する。\n構造方程式モデリングは，分散共分散構造/相関構造の要素に潜在変数を含んだ方程式を当てはめた時の係数を推定するモデルである。方程式はパス図と呼ばれる表現方法で図示されることが多い。パス図では相関的関係を双方向の矢印で，回帰的関係を単方向の矢印で表現し，観測変数を矩形で，潜在変数を楕円で表す。パス図の表現を使うと，因子分析と主成分分析の違いは一目瞭然である。\n\n\n\n主成分分析(左)と因子分析(右)のパス図表現\n\n\nまた，探索的因子分析と確認的因子分析の違いも明白である。\n\n\n\nEFA(左)とCFA(右)のパス図表現。簡略化するために誤差因子は描画しない。\n\n\n確認的因子分析では，どの因子がどの項目に影響しているかを個別に指定している。言い換えるなら，影響がないと仮定するパスの係数を\\(0\\)に固定しているともいえる。この図では確認的因子分析モデルの因子間相関が\\(0\\)であると仮定しているため，パスを引いていない(引くことももちろん可能である)。\nこの方法ではモデルの方が先にあり，このモデルから考えられる分散共分散行列の式を実際の分さ共分散行列に当てはめることになる。当てはめる，すなわち係数を推定する方法は，大きく分けて最小二乗法，最尤法，ベイズ法であるが，実際は各種推定法のアルゴリズム名まで把握しておくといいだろう。さらに推定後，モデルと実際の分散共分散行列がどれほど一致しているか，即ち適合度Model Fit Indicesをみてその評価を行うことになる。適合度指標も複数あるため，それらを見ながら総合的に評価することになる。\nRでの具体例を見ておこう。パッケージlavvaanを用いて2以下のようにモデルを与える3。\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\n\n\n次のパッケージを付け加えます: 'lavaan'\n\n\n以下のオブジェクトは 'package:psych' からマスクされています:\n\n    cor2cov\n\n# モデル指定\nmodel &lt;- \"\nNeuroticism =~ N1 + N2 + N3 + N4 + N5\nAgreeableness =~ A1 + A2 + A3 + A4 + A5\nExtraversion =~ E1 + E2 + E3 + E4 + E5\nOpenness =~ O1 + O2 + O3 + O4 + O5\nConscientiousness =~ C1 + C2 + C3 + C4 + C5\n\"\n\nここでmodelオブジェクトは文字列として入力されていることに注意しよう。シングル，あるいはダブルクォーテーションでモデル記述を囲むのである。また，潜在変数名を左辺に置き，それを構成する観測変数を右辺に置く測定方程式は，=~という演算子で繋ぐ。変数同士の相関的関係は~~という演算子を，回帰的関係は~という演算子を用いる。特に潜在変数同士の関係を記述する方程式は構造方程式と呼ばれる。\nここでは因子間相関に関する記述はないが，デフォルトで\\(0\\)と指定しないところには相関のパスが仮定される。係数をゼロに指定したい場合は，Neuroticism ~~ 0 * Opennessのように記述するといいだろう。\nさて，モデルの指定が終われば，データと推定法を指定して推定させよう。 ここでは推定法(estimator)オプションを最尤法(ML)とした。また，要約を出力するときに，適合度指標(fit.meassures)と標準化係数(standardized)も表示するよう指定してある。\n\n# モデル推定\nmodel.fit &lt;- sem(model, estimator = \"ML\", data = dat)\nsummary(model.fit, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 55 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        60\n\n                                                  Used       Total\n  Number of observations                          2436        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                              4165.467\n  Degrees of freedom                               265\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             18222.116\n  Degrees of freedom                               300\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.782\n  Tucker-Lewis Index (TLI)                       0.754\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -99840.238\n  Loglikelihood unrestricted model (H1)     -97757.504\n                                                      \n  Akaike (AIC)                              199800.476\n  Bayesian (BIC)                            200148.363\n  Sample-size adjusted Bayesian (SABIC)     199957.729\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.078\n  90 Percent confidence interval - lower         0.076\n  90 Percent confidence interval - upper         0.080\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.037\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.075\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                       Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism =~                                                            \n    N1                    1.000                               1.300    0.825\n    N2                    0.947    0.024   39.899    0.000    1.230    0.803\n    N3                    0.884    0.025   35.919    0.000    1.149    0.721\n    N4                    0.692    0.025   27.753    0.000    0.899    0.573\n    N5                    0.628    0.026   24.027    0.000    0.816    0.503\n  Agreeableness =~                                                          \n    A1                    1.000                               0.484    0.344\n    A2                   -1.579    0.108  -14.650    0.000   -0.764   -0.648\n    A3                   -2.030    0.134  -15.093    0.000   -0.983   -0.749\n    A4                   -1.564    0.115  -13.616    0.000   -0.757   -0.510\n    A5                   -1.804    0.121  -14.852    0.000   -0.873   -0.687\n  Extraversion =~                                                           \n    E1                    1.000                               0.920    0.564\n    E2                    1.226    0.051   23.899    0.000    1.128    0.699\n    E3                   -0.921    0.041  -22.431    0.000   -0.847   -0.627\n    E4                   -1.121    0.047  -23.977    0.000   -1.031   -0.703\n    E5                   -0.808    0.039  -20.648    0.000   -0.743   -0.553\n  Openness =~                                                               \n    O1                    1.000                               0.635    0.564\n    O2                   -1.020    0.068  -14.962    0.000   -0.648   -0.418\n    O3                    1.373    0.072   18.942    0.000    0.872    0.724\n    O4                    0.437    0.048    9.160    0.000    0.277    0.233\n    O5                   -0.960    0.060  -16.056    0.000   -0.610   -0.461\n  Conscientiousness =~                                                      \n    C1                    1.000                               0.680    0.551\n    C2                    1.148    0.057   20.152    0.000    0.781    0.592\n    C3                    1.036    0.054   19.172    0.000    0.705    0.546\n    C4                   -1.421    0.065  -21.924    0.000   -0.967   -0.702\n    C5                   -1.489    0.072  -20.694    0.000   -1.012   -0.620\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Neuroticism ~~                                                        \n    Agreeableness     0.141    0.018    7.712    0.000    0.223    0.223\n    Extraversion      0.292    0.032    9.131    0.000    0.244    0.244\n    Openness         -0.093    0.022   -4.138    0.000   -0.112   -0.112\n    Conscientisnss   -0.250    0.025  -10.117    0.000   -0.283   -0.283\n  Agreeableness ~~                                                      \n    Extraversion      0.304    0.025   12.293    0.000    0.683    0.683\n    Openness         -0.093    0.011   -8.446    0.000   -0.303   -0.303\n    Conscientisnss   -0.110    0.012   -9.254    0.000   -0.334   -0.334\n  Extraversion ~~                                                       \n    Openness         -0.265    0.021  -12.347    0.000   -0.453   -0.453\n    Conscientisnss   -0.224    0.020  -11.121    0.000   -0.357   -0.357\n  Openness ~~                                                           \n    Conscientisnss    0.130    0.014    9.190    0.000    0.301    0.301\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.793    0.037   21.575    0.000    0.793    0.320\n   .N2                0.836    0.036   23.458    0.000    0.836    0.356\n   .N3                1.222    0.043   28.271    0.000    1.222    0.481\n   .N4                1.654    0.052   31.977    0.000    1.654    0.672\n   .N5                1.969    0.060   32.889    0.000    1.969    0.747\n   .A1                1.745    0.052   33.725    0.000    1.745    0.882\n   .A2                0.807    0.028   28.396    0.000    0.807    0.580\n   .A3                0.754    0.032   23.339    0.000    0.754    0.438\n   .A4                1.632    0.051   31.796    0.000    1.632    0.740\n   .A5                0.852    0.032   26.800    0.000    0.852    0.528\n   .E1                1.814    0.058   31.047    0.000    1.814    0.682\n   .E2                1.332    0.049   26.928    0.000    1.332    0.512\n   .E3                1.108    0.038   29.522    0.000    1.108    0.607\n   .E4                1.088    0.041   26.732    0.000    1.088    0.506\n   .E5                1.251    0.040   31.258    0.000    1.251    0.694\n   .O1                0.865    0.032   27.216    0.000    0.865    0.682\n   .O2                1.990    0.063   31.618    0.000    1.990    0.826\n   .O3                0.691    0.039   17.717    0.000    0.691    0.476\n   .O4                1.346    0.040   34.036    0.000    1.346    0.946\n   .O5                1.380    0.045   30.662    0.000    1.380    0.788\n   .C1                1.063    0.035   30.073    0.000    1.063    0.697\n   .C2                1.130    0.039   28.890    0.000    1.130    0.650\n   .C3                1.170    0.039   30.194    0.000    1.170    0.702\n   .C4                0.960    0.040   24.016    0.000    0.960    0.507\n   .C5                1.640    0.059   27.907    0.000    1.640    0.615\n    Neuroticism       1.689    0.073   23.034    0.000    1.000    1.000\n    Agreeableness     0.234    0.030    7.839    0.000    1.000    1.000\n    Extraversion      0.846    0.062   13.693    0.000    1.000    1.000\n    Openness          0.404    0.033   12.156    0.000    1.000    1.000\n    Conscientisnss    0.463    0.036   12.810    0.000    1.000    1.000\n\n\n出力として，まずモデルの要約(推定法など)が示され，続いて適合度指標(CFI, TFI, AIC, BIC, RMSEA, SRMR)などが表示される。これらの適合度指標は大きく3つのカテゴリーに分類できる：\nまずは，飽和モデルとヌルモデルの間の比較指標である。CFIやTFIがこれに該当する。ヌルモデルを0，飽和モデルを1としたとき，今回のモデルがどこに位置づくかを示す。ここで飽和モデルとはデータに完全に適合するモデルであり，すべての観測変数間にすべての可能なパスが引かれたものを指す。逆にヌルモデル(独立モデル)は観測変数間に関連性がまったくないと仮定する最も制約の強いモデルである。\n次に，尤度に基づく相対指標である。AIC，BIC，SABICがこれに該当する。尤度はデータが確率モデルにどれほど近いかを表したものであるから，データやモデルが異なれば比較はできない。当該データに対する相対比較として用いる。AIC(赤池情報量規準，Akaike Information Criterion)は対数尤度とパラメータの数で計算されており，対数尤度が大きくパラメータ数が少ない方が良いモデルだと考える規準である。-2LL + 2p(LLは対数尤度，pはパラメータ数)で算出され，小さければ小さいほど当てはまりが良いと判断する。BIC(ベイジアン情報量規準)は，AICよりもサンプルサイズに対して強いペナルティを与えている。SABICはサンプルサイズを調整したBICの亜形である。\n最後に，実データの分散説明量の残量に関する指標がある。RMSEAとSRMRがこれに該当する。RMSEA(Root Mean Square Error of Approximation)はモデルの近似誤差を評価しており，0.05未満が良好とされる。SRMR(Standardized Root Mean Square Residual)は実データと予測データの残差の標準化された平方根で， 0.08未満が良好とされる。\nモデル評価の際は、これらの指標を総合的に検討することが推奨される。単一の指標だけではなく、複数の指標を参照し、それらが一貫して良好な適合を示すかを確認することが重要である。\n続く出力で，推定値Estimator や検定統計量が表示される。心理学の場合は，全ての変数を標準化した推定値Std.allを参照することが多い4。ここでパス係数が小さかったり，統計的に有意にならないものを削除することでモデルの適合度は上げることができる。 ただし，適合度を上げることが研究の目的になってはならない。仮定を当てはめるのだから，適当とされる目安を達成できない場合は仮定を省みて改良することはあるだろうが，適合度を上げるために仮定に合わないパスを引くようなことは，「頑張って有意にする」のと同じQRPsである。\nところで，パッケージを使えば，パス図も自動で描いてくれる。他にも，lavaanExtra，tidySEM，lavaanPlot,lavaanPlot2など開発中のものも含めて様々なものがあるが，ここでは古典的なsemPlotパッケージによる出力例を示す。\n\nlibrary(semPlot)\nsemPaths(model.fit, what = \"stand\", style = \"lisrel\")\n\n\n\n\n\n\n\n\n以上が因子分析法の概略である。\n因子分析法は測定に関するモデルであり，心理尺度を作成する場合は非常によく用いられるものである。しかしあくまでもデータや項目間の相関関係から共通次元を見出すものであるから，構成概念を直接測定したとか，構成概念の存在が証明されたかのような利用・解釈は適切ではない。例えば，何らかの話題についての文言，極端な話「ラーメンに関する記述」を用意して，そこに量的な評価を加えれば，ラーメン因子だろうが豚骨因子だろうが，何らかの解釈ができる因子を抽出することはできる。そのことと，人が心理的に豚骨因子を内在化しているということにはならない。\nまた構造方程式モデリングによって潜在変数間に回帰や相関のパスを仮定することはできるが，そのことが実際どのような形で顕現化するかについて考えておく必要がある。モデルが非常に適合していて，潜在変数間に強い影響関係があったとしても，測定方程式のパス係数が低かったりすると，結局一方の因子得点が1単位増えたことで，従属する潜在変数がどのように変化し，それがどのように行動・測定値に反映されるかを意識しよう。そのような実態的な影響がない，つまり妥当性のない統計モデルは机上の空論に過ぎないからである。\n測定とその実際的影響については，因子分析モデルの一種とも言える項目反応理論と問題意識を軌を一にする。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#項目反応理論",
    "href": "chapter14.html#項目反応理論",
    "title": "14  多変量解析(その1)",
    "section": "14.2 項目反応理論",
    "text": "14.2 項目反応理論\nつづいて項目反応理論(Item Response Theory, IRT)を取り上げる。項目反応理論は古典的テスト理論(Classical Test Theory, CTT)との対比で，現代テスト理論と呼ばれることもある。テスト理論を背景に持つものであるから，従属変数としてバイナリ変数を前提としている(0が誤答，1が正答を表す)。これは言い換えれば因子分析において従属変数がバイナリであるものといってもよく，実際にカテゴリカル因子分析との数学的等価性が明らかになっている。\nもう一つ因子分析と異なる側面としては，因子分析が性格心理学を背景に因子構造を探索することに重点が置かれていることに対して，項目反応理論は因子得点をより精緻にすることに重点が置かれている。また性格心理学の場合は何因子構造であるかということがすでに学術的な問いであるが，項目反応理論を一とするテスト理論においては「学力」の一因子構造であることが望ましいとされる。このことから，因子分析を使った心理尺度の構成は「単純構造」が良いものであると考え項目を洗練(取捨選択)することが多いのに対し，項目反応理論は第一因子の負荷量が十分大きければ(一般に30%程度の分散説明率があれば良い)一因子構造であると考えるし，いかなる項目であっても何らかの情報をもたらすものと考えて項目をプールする(捨てない)という方針で進められることが多い点である。\n項目反応理論の各種モデルを用いた，コンピュータ適応型テスト(Computer Adaptive Test, CAT)が現在のテスト理論の主流である。これは受験者の回答パターンに応じて項目プールから動的に次々問題を提供し，効率よく受験者の能力を推定していくものである。CATに必要なのはIRTを背景にしたモデルはもちろんのこと，各能力水準を測定するのに適した項目プールであり，また項目プールというデータベースとの連携システムである5。\n\n14.2.1 ロジスティックモデル\nIRTはバイナリデータに対する単因子モデルである。バイナリデータであるから，連続値を前提とするピアソンの積率相関係数ではなく，テトラコリック相関係数を用いてその次元性を解析する。また因子に該当する被験者母数\\(\\theta\\)によって項目反応が回帰されるモデルでもあるから，ロジスティック回帰分析のようなモデル化をすることになる。\n被験者母数は標準正規分布が仮定されるが，これを累積正規分布の形で表現するとロジスティックカーブがよくあてまるし，項目の特徴を表現するための項目母数を線形モデルの中に組み込むためには，ロジスティックモデルで表現する方がわかりやすいという側面もある6。以下に標準正規分布と累積正規分布，並びにロジスティック関数による正規累積分布の近似を示す。なお，ロジスティックモデルで扱われる関数は，以下のように係数(1.702)を用いると，よりよく近似することが知られている。\n\\[ f(x) = \\frac{1}{1 + exp(-1.702x)} \\]\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# データの準備\nx &lt;- seq(-4, 4, length.out = 1000)\nnormal_df &lt;- data.frame(\n  x = x,\n  density = dnorm(x),\n  cdf = pnorm(x),\n  logistic = 1 / (1 + exp(-1.702 * x))\n)\n\n# 1. 標準正規分布\np1 &lt;- ggplot(normal_df, aes(x = x, y = density)) +\n  geom_line() +\n  labs(title = \"標準正規分布\",\n       x = \"x\",\n       y = \"確率密度\") +\n  theme_minimal()\n\n# 2. 累積正規分布\np2 &lt;- ggplot(normal_df, aes(x = x, y = cdf)) +\n  geom_line() +\n  labs(title = \"累積正規分布\",\n       x = \"x\",\n       y = \"累積確率\") +\n  theme_minimal()\n\n# 3. ロジスティック曲線\np3 &lt;- ggplot(normal_df, aes(x = x, y = logistic)) +\n  geom_line() +\n  labs(title = \"ロジスティック曲線による近似\",\n       x = \"x\",\n       y = \"確率\") +\n  theme_minimal()\n\n# 3つのプロットを横に並べて表示\np1 + p2 + p3\n\n\n\n\n\n\n\n\nこのロジスティック関数を用いて，項目母数を使って項目の特徴を描画することを考えよう。IRTのロジスティックモデルには，パラメータが1つのもの，2つのもの…とさまざまなものが考えられているが，パラメータ数が多いモデルはパラメータ数が少ないモデルに含まれる(特殊形)である。\n\n14.2.1.1 1PLモデル\nまずは1パラメータロジスティックモデル(1PLモデル)を考えよう。このモデルは，項目母数\\(b\\)を用いて，以下のように表現される。\n\\[ P(Y_{ij} = 1 | \\theta_i, b_j) = \\frac{1}{1 + exp(-1.702(\\theta_i - b_j))} \\]\nここで，\\(Y_{ij}\\)は被験者\\(i\\)が項目\\(j\\)に正答したかどうかを表すバイナリ変数であり，\\(\\theta_i\\)は被験者\\(i\\)の能力を表す被験者母数である。また，\\(b_j\\)は項目\\(j\\)の困難度(difficulty)を表す項目母数である。というのも，この\\(b_j\\)が大きくなるとロジスティック曲線は右に寄る，また小さくなると左に寄るからである。横軸は被験者母数\\(\\theta\\)であり，縦軸は通過率であるから，曲線が右にシフトすることはより能力が高くなければ通過率が上昇しないことを表すからである。\n\nlogistic_1pl &lt;- function(theta, b) {\n  1 / (1 + exp(-1.702 * (theta - b)))\n}\n\nx &lt;- seq(-4, 4, length.out = 1000)\nnormal_df &lt;- data.frame(\n  x = x,\n  default = logistic_1pl(x, 0), #deafult\n  easy = logistic_1pl(x, -1),\n  hard = logistic_1pl(x, 1)\n)\n\nggplot(normal_df) +\n  geom_line(aes(x = x, y = default, color = \"デフォルト(b=0)\")) +\n  geom_line(aes(x = x, y = easy, color = \"易しい(b=-1)\")) +\n  geom_line(aes(x = x, y = hard, color = \"難しい(b=1)\")) +\n  scale_color_brewer(palette = \"Set2\") + \n  labs(title = \"1pl logistic model\",\n       x = \"theta\",\n       y = \"通過率\",\n       color = \"難易度\") +  # 凡例のタイトルを追加\n  theme_minimal() +\n  theme(legend.position = \"bottom\")  # 凡例を下部に配置\n\n\n\n\n\n\n\n\n\n\n14.2.1.2 2PLモデル\n2パラメータロジスティックモデル(2PLモデル)は，1PLモデルに加えて項目母数\\(a\\)を含める。この母数は識別力と呼ばれる。\n\\[ P(Y_{ij}=1|\\theta_i,a_j,b_j) = \\frac{1}{1+exp(-1.702a_j(\\theta_i-b_j))} \\]\nこれが識別力と呼ばれるのは，ロジスティック曲線の傾きを変えるからである。傾きが強くなって急激に上昇することは，ある\\(\\theta\\)の値で急に正誤の確率が変わることを意味し，逆に傾きが緩くなることは特定の\\(\\theta\\)の値でも正誤の違いが大きくないことを意味するからである。ちなみにカテゴリカル因子分析の文脈では，困難度\\(b_j\\)が閾値に，識別力\\(a_j\\)が因子負荷量に相当する。\n\nlogistic_2pl &lt;- function(theta, a, b) {\n  1 / (1 + exp(-1.702 * a * (theta - b)))\n}\n\nx &lt;- seq(-4, 4, length.out = 1000)\nnormal_df &lt;- data.frame(\n  x = x,\n  default = logistic_2pl(x, 1, 0), #deafult\n  easy = logistic_2pl(x,1.5, -1),\n  hard = logistic_2pl(x,0.5, 1)\n)\n\nggplot(normal_df) +\n  geom_line(aes(x = x, y = default, color = \"デフォルト(b=0,a=1)\")) +\n  geom_line(aes(x = x, y = easy, color = \"b=-1, a=1.5\")) +\n  geom_line(aes(x = x, y = hard, color = \"b=1, a=0.5\")) +\n  scale_color_brewer(palette = \"Set2\") + \n  labs(title = \"1pl logistic model\",\n       x = \"theta\",\n       y = \"通過率\",\n       color = \"モデルと設定\") +  # 凡例のタイトルを追加\n  theme_minimal() +\n  theme(legend.position = \"bottom\")  # 凡例を下部に配置\n\n\n\n\n\n\n\n\n\n\n14.2.1.3 3PLモデル\n実践的には2PLモデルが最もよく用いられるが，理論的には3，4，5PLモデルまで提案されており， それぞれ次のように表現される。\n\\[ P(Y_{ij}=1|\\theta_i,a_j,b_j,c_j) = c_j + \\frac{1-c_j}{1+exp(-1.702a_j(\\theta_i-b_j))} \\]\n\\[ P(Y_{ij}=1|\\theta_i,a_j,b_j,c_j,d_j) = c_j\\frac{d_j-c_j}{1+exp(-1.702a_j(\\theta_i-b_j))} \\]\n\\[ P(Y_{ij}=1|\\theta_i,a_j,b_j,c_j,d_j,e_j) = c_j + \\frac{d_j-c_j}{\\{1+exp(-1.702a_j(\\theta_i-b_j))\\}^{e_j}} \\]\nここで\\(c_j\\)は下方漸近線母数，\\(d_j\\)は上方漸近線母数，\\(e_j\\)は非対称性母数と呼ばれている。このように，モデルとしては徐々にパラメータ数を増やして表現しているが，推定すべきパラメータの数が増えるとより大きい標本サイズ(受験者数)が必要となるし，等価など運用シーンでも複雑になることから，あまり用いられるものではない。\n\n\n14.2.1.4 ロジスティックモデルと分析の実際\n項目の特徴を表現するロジスティック曲線は，項目反応関数(Item Responose Function)あるいは項目特性曲線(Item Characteristic Curve)と呼ばれる。ltmパッケージやexametrika パッケージなど，IRTモデルを実行するためのRパッケージは複数あり，これを使って実践例を見てみよう。\nここでは著者が開発したexametrikaパッケージとそのサンプルデータを用いて，実際にIRTを実行してみよう。exametrikaパッケージにはサンプルデータが複数含まれている。今回用いるJ15S500は，500人の被験者が15問の項目に回答したサンプルデータである。\n\n## なければインストールしておく\n# install.packages(\"exametrika\")\nlibrary(exametrika)\nresult.2pl &lt;- IRT(J15S500, model = 2, verbose = FALSE)\nprint(result.2pl)\n\nItem Parameters\n       slope location PSD(slope) PSD(location)\nItem01 0.698   -1.683     0.1093         0.266\nItem02 0.810   -1.552     0.1166         0.221\nItem03 0.559   -1.838     0.0988         0.338\nItem04 1.416   -1.178     0.1569         0.113\nItem05 0.681   -2.242     0.1152         0.360\nItem06 0.997   -2.162     0.1499         0.273\nItem07 1.084   -1.039     0.1281         0.130\nItem08 0.694   -0.558     0.1002         0.153\nItem09 0.347    1.630     0.0766         0.427\nItem10 0.492   -1.421     0.0907         0.306\nItem11 1.122    1.020     0.1314         0.124\nItem12 1.216    1.031     0.1385         0.117\nItem13 0.875   -0.720     0.1111         0.133\nItem14 1.200   -1.232     0.1407         0.134\nItem15 0.823   -1.203     0.1127         0.180\n\nItem Fit Indices\n       model_log_like bench_log_like null_log_like model_Chi_sq null_Chi_sq\nItem01       -263.524       -240.190      -283.343       46.669      86.307\nItem02       -252.914       -235.436      -278.949       34.954      87.025\nItem03       -281.083       -260.906      -293.598       40.353      65.383\nItem04       -205.851       -192.072      -265.962       27.558     147.780\nItem05       -232.072       -206.537      -247.403       51.070      81.732\nItem06       -173.930       -153.940      -198.817       39.981      89.755\nItem07       -252.039       -228.379      -298.345       47.320     139.933\nItem08       -313.754       -293.225      -338.789       41.057      91.127\nItem09       -325.692       -300.492      -327.842       50.399      54.700\nItem10       -309.448       -288.198      -319.850       42.500      63.303\nItem11       -250.836       -224.085      -299.265       53.501     150.360\nItem12       -240.247       -214.797      -293.598       50.900     157.603\nItem13       -291.816       -262.031      -328.396       59.571     132.730\nItem14       -224.330       -204.953      -273.212       38.754     136.519\nItem15       -273.120       -254.764      -302.847       36.713      96.166\n       model_df null_df   NFI   RFI   IFI   TLI   CFI RMSEA    AIC    CAIC\nItem01       12      13 0.459 0.414 0.533 0.488 0.527 0.076 22.669 -27.930\nItem02       12      13 0.598 0.565 0.694 0.664 0.690 0.062 10.954 -39.645\nItem03       12      13 0.383 0.331 0.469 0.414 0.459 0.069 16.353 -34.246\nItem04       12      13 0.814 0.798 0.885 0.875 0.885 0.051  3.558 -47.041\nItem05       12      13 0.375 0.323 0.440 0.384 0.432 0.081 27.070 -23.529\nItem06       12      13 0.555 0.517 0.640 0.605 0.635 0.068 15.981 -34.619\nItem07       12      13 0.662 0.634 0.724 0.699 0.722 0.077 23.320 -27.279\nItem08       12      13 0.549 0.512 0.633 0.597 0.628 0.070 17.057 -33.542\nItem09       12      13 0.079 0.002 0.101 0.002 0.079 0.080 26.399 -24.201\nItem10       12      13 0.329 0.273 0.405 0.343 0.394 0.071 18.500 -32.099\nItem11       12      13 0.644 0.615 0.700 0.673 0.698 0.083 29.501 -21.099\nItem12       12      13 0.677 0.650 0.733 0.709 0.731 0.081 26.900 -23.699\nItem13       12      13 0.551 0.514 0.606 0.570 0.603 0.089 35.571 -15.028\nItem14       12      13 0.716 0.692 0.785 0.765 0.783 0.067 14.754 -35.846\nItem15       12      13 0.618 0.586 0.706 0.678 0.703 0.064 12.713 -37.886\n           BIC\nItem01 -27.906\nItem02 -39.621\nItem03 -34.222\nItem04 -47.017\nItem05 -23.505\nItem06 -34.595\nItem07 -27.255\nItem08 -33.518\nItem09 -24.177\nItem10 -32.076\nItem11 -21.075\nItem12 -23.675\nItem13 -15.004\nItem14 -35.822\nItem15 -37.862\n\nModel Fit Indices\n                   value\nmodel_log_like -3890.655\nbench_log_like -3560.005\nnull_log_like  -4350.217\nmodel_Chi_sq     661.300\nnull_Chi_sq     1580.424\nmodel_df         180.000\nnull_df          195.000\nNFI                0.582\nRFI                0.547\nIFI                0.656\nTLI                0.624\nCFI                0.653\nRMSEA              0.073\nAIC              301.300\nCAIC            -457.689\nBIC             -457.330\n\n\n数値的な特徴としては，Item Parametersのところにslope(識別力)，location(困難度)が示されており，またそれぞれの標準誤差が示されている。 続くItem Fit Indicesは項目ごとの適合度，Model Fit Indicesはテスト全体のモデル適合度であるが，IRTモデルはSEMの適合度の観点から言えば非常に当てはまりは悪い。これはバイナリデータに対するモデリングであることなどを考えると，ある程度は仕方がないことであるとも言える。\nIRTの良さは，こうした数値的特徴というよりも，項目分析の時におけるIRTの可視化のしやすさにあると言えるだろう。exametrikaパッケージでは，plot関数を用いて項目特性曲線を描画することができる。\n\nplot(result.2pl, item = 1:5, type = \"IRF\", overlay = TRUE)\n\n\n\n\n\n\n\n\nまた，IRF関数をテストの全項目に対して加算したテスト反応関数(Test Response Function)を描画することもできる。\n\nplot(result.2pl, type = \"TRF\")\n\n\n\n\n\n\n\n\nさらに項目反応関数を変換した項目情報関数(Item Information Function)を描画することもできる。項目情報関数は，その項目において最も分散が大きくなるところ，すなわち\\(\\theta =0.5\\)をピークにする関数であり，以下のように定義される。 \\[ I_j(\\theta) = \\frac{P_j^{\\prime}(\\theta)^2}{P_j(\\theta)(1-P_j(\\theta))} \\]\n要するに，正答と誤答の確率の差が大きいほど，その項目の情報量が大きくなるということである。 この関数をexametrikaパッケージでプロットするには次のようにtype = \"IIF\"と指定する。\n\nplot(result.2pl, item = 4, type = \"IRF\")\n\n\n\n\n\n\n\nplot(result.2pl, item = 4, type = \"IIF\")\n\n\n\n\n\n\n\n\nIIFが示すのは，項目反応理論における信頼性の概念であるとも言える。すなわち，IRTにおいては信頼性が\\(\\theta\\)の関数として表現され，どの領域匂いってその項目が最も効率的に機能するかを評価するのである。\n確認しておくと，古典的テスト理論においては，テストの全体に対する真分散の割合で信頼性をとらえていたのであった。また因子分析においては項目における共通性\\(h_j^2\\)で信頼性をとらえていた。つまりテスト全体から各項目へと進んで行ったのだが，現代テスト理論においては関数・項目の機能性を評価するようにと発展してきたのである。\nすでに述べたように，IRTの観点からは，難易度が高すぎる・低すぎる項目であっても，削除するようなことはない。そうした項目は，高い能力・低い能力を査定する時に必要なのである。実戦に際してそうした項目は大きな分散を持ち得ないから，共通性も低くなりがちであるが，だからと言ってそうした項目を削除するようなことはしない。この辺りに，テスト理論と因子分析との思想的な違いがあると言える。\nテスト全体の情報関数は，テストに含まれる項目情報関数の総和で表現される。この関数をexametrikaパッケージでプロットするには次のようにtype = \"TIF\"と指定する。\n\nplot(result.2pl, type = \"TIF\")\n\n\n\n\n\n\n\n\nこれを見ると，この15項目からなるテストは全体として\\(\\theta=-1\\)のあたりをピークとしており，相対的にやや\\(\\theta\\)が低い受験者に対して精緻な情報を提供するようになっていることがわかる。事前に項目母数がわかっている多くの項目プールがあり，それらを組み合わせてテストを作成する場合には，このような情報関数を用いて事前にテストの精度をデザインして適用することができる。\n被験者母数\\(\\theta\\)の推定については，受験者の回答パターンから推定される。一般に標準正規分布を事前分布としたベイズ推定が用いられる7。exametrikaパッケージでは，分析と同時に被験者母数の推定も行われている。\n\nhead(result.2pl$ability)\n\n          ID         EAP       PSD\n1 Student001 -0.66456780 0.5457047\n2 Student002 -0.14853704 0.5626979\n3 Student003  0.01362525 0.5699764\n4 Student004  0.58775674 0.6012839\n5 Student005 -0.97796871 0.5415528\n6 Student006  0.85892502 0.6187224\n\n\n\n\n\n14.2.2 IRTモデルの展開\nIRTモデルは基本的にバイナリデータに対するモデルであるが，多段階反応，多値反応に対するモデルも提案されている。たとえばリッカート尺度のような多段階反応に対しては，段階反応モデル(Graded Response Model)や部分採点モデル(Partial Credit Model)が提案されている。これらを用いることの利点は，心理尺度データに対して順序尺度水準を仮定できるところにある。\n心理学では基本的に，段階評定はせいぜい順序尺度水準の精度しか持ち得ないと考えられていながら，その数学的な利便性から(あるいはそこまでの精度がないとそもそも信用されていなかったから)，間隔尺度水準とみなしてピアソンの積率相関係数を算出し，一般的な因子分析を行ってきた。こうした「みなし」が必要だった理由のひとつが，統計パッケージにGRMやPCMが実装されていなかったからである。昨今では，GRMと2PLモデルとの数学的等価性から同じ潜在変数モデルとして推定する統計ソフトウェア(Mplusなど)もあるし，RにはltmパッケージなどGRM，PCMを提供するものもある。つまり，ツールがないからという言い訳はもう通用しない時代である。\n多段階モデルで分析できるさらなる利点は，適切な反応段階を考えられる点である。リッカート法といえば5件法，7件法であるというのが一般的に考えられているが，このことに特段の理論的根拠はない。それよりも，回答者が5，7段階のカテゴリ反応をしっかりと弁別できるのかどうかを考えるべきである8。\n具体例で見てみよう。ltmパッケージのgrm関数を用いて，サンプルデータScienceを分析してみる。このデータは科学態度に対するデータであり，4段階評定になっている。\n\n## なければインストールしておく\n# install.packages(\"ltm\")\nlibrary(ltm)\ndata(Science)\nresult.grm &lt;- grm(Science)\nprint(result.grm)\n\n\nCall:\ngrm(data = Science)\n\nCoefficients:\n             Extrmt1  Extrmt2  Extrmt3  Dscrmn\nComfort      -10.768   -5.645    3.097   0.411\nEnvironment   -2.154   -0.790    0.627   1.570\nWork          32.102    9.261  -24.402  -0.074\nFuture       -30.602  -11.806   10.455   0.108\nTechnology    -2.462   -0.885    0.642   1.650\nIndustry      -2.870   -1.529    0.286   1.642\nBenefit      -21.232   -5.982   10.297   0.136\n\nLog.Lik: -2998.129\n\n\n結果で示されているのは3つの閾値(Extrmt)と，それぞれの閾値に対する識別力(Dcrmn)である。 段階反応モデルも，ロジスティックモデル同様IRF，IIF，TIFを描画できるから，IRF(ltmパッケージではICCという引数を用いる)を描いてみよう。\n\nplot(result.grm, items = 2, type = \"ICC\")\n\n\n\n\n\n\n\n\nこの図には各カテゴリに対する反応確率が，\\(\\theta\\)の関数として表示されている。これを見ると，科学的態度の\\(\\theta\\)が高くなるにつれて，回答する確率が最も高いカテゴリが1から2，2から3へと変わっていくことが見て取れる。\nしかし次の項目はどうだろうか。\n\nplot(result.grm, items = 1, type = \"ICC\")\n\n\n\n\n\n\n\n\nこれを見ると，反応カテゴリ1，2のピークが存在せず，ほとんど3の反応で被覆されており，\\(\\theta=3\\)を超えたところでやっと反応カテゴリ4がでてくることになる。プロットは\\(-4 \\le \\theta \\le +4\\)の範囲であるから，この幅を負の方向に広げればピークが出てくるのかもしれないが，実際的ではない。データによってはカテゴリ反応のピークが出てこないものもあり，そうしたものは適切な反応段階の設計になっていないことが疑われる。\n昨今では心理尺度の設計に対しても，IRTのアプローチを取ることが推奨されている。因子分析アプローチとのもう一つの違いである，単因子を仮定する点についても拡張され，多次元IRTモデルも提案されている(mirtパッケージなどが提供されている)。もはや，IRTのアプローチを取らない理由がないのである。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#まとめ",
    "href": "chapter14.html#まとめ",
    "title": "14  多変量解析(その1)",
    "section": "14.3 まとめ",
    "text": "14.3 まとめ\nここでは探索的因子分析，検証的因子分析(構造方程式モデリング)，項目反応理論のそれぞれについて，その基本的な考え方と実践的な使い方を紹介した。\nこれらに共通する考え方は，分散共分散行列あるいは相関行列をもとに，潜在変数を仮定した測定モデルを構築するという点である。相関行列が順序尺度水準に対するテトラコリック/ポリコリック/ポリシリアル相関係数であれば，カテゴリカルな因子分析をしていることになるし，それは(多段階)項目反応理論をしていることでもある。つまり尺度水準に対応した相関係数が算出できるアプリケーションであれば，モデルの適用は同じ手順で行うことができる(lavaanでも変数の尺度水準の設定ができる。そのほかのアプリケーションとしてはMplusが有名である)。\nそれぞれのモデルの持つ歴史的背景や理論的系譜を知ることは，モデルの適用に有益な知見をもたらすが，ユーザ視点でいえば使えるものはなんでも使うべきであり，とくに調査協力者(=回答者，受験生)の回答のしやすさといった観点から調査研究を設計することが肝要であろう。数学的限界やソフトウェアの都合によって，ましてや研究者の無理解や怠慢によって，回答しにくい調査デザインを適用することは，調査研究の質を低下させることになることを忘れてはならない。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#課題",
    "href": "chapter14.html#課題",
    "title": "14  多変量解析(その1)",
    "section": "14.4 課題",
    "text": "14.4 課題\nこのチャプターで学んだ多変量解析の手法について、以下の課題に取り組んでください。各課題は、実践的なデータ分析を通じて、理論と実践の両面から理解を深めることを目的としています。 例として，psychパッケージのsmall.msqデータセット(気分状態質問紙)を使用します。このデータセットは14の変数，200ケースです。9\n14の変数は，エネルギー的覚醒状態変数(活動的active, 注意深さalert，覚醒arousal，ねむさsleepy，疲れtired，ウトウトdrowsy)，緊張的覚醒変数(不安anxious，落ち着かないjittery，神経質なnervous，穏やかなcalm，リラックスしたrelaxed，気楽なat.ease)と，性別gender，薬物条件drug からなります。性別と薬物条件を除いた2因子構造であると考えられ，\n\n14.4.1 課題1：探索的因子分析の実践\n探索的因子分析を実施してください。\n分析の手順：\n\n因子数の決定\n因子分析の実行(斜交回転)\n結果の解釈（因子負荷量、共通性、独自性）\n\n\n\n14.4.2 課題2：確認的因子分析の実践\nlavaanパッケージを使って，2因子モデルを推定してください。 モデル指定の際に，ordered = TRUE オプションを入れることで，観測変数を順序尺度水準として推定します。\n分析の手順：\n\n理論的モデルの構築(パス図の作成)\nlavaanパッケージを用いたモデルの推定\nモデルのプロット\n適合度指標の評価\nモデルの修正(必要な場合)\n\n\n\n14.4.3 課題3：多段階項目反応理論の実践\nltmパッケージの段階反応モデル(GRM)を適用してください。一次元性を仮定したモデルなので，エネルギー的覚醒項目セット，緊張的覚醒項目セットに分け，それぞれにGRMモデルを適用します。\n分析の手順：\n\nデータの分割\nGRMの適用\n項目特性曲線の描画\n項目情報関数の描画\n\n\n\n14.4.4 課題4: 多次元多段階項目反応理論の実践\nmirtパッケージ(Multidimensional IRT)を用いて，多次元モデルを実行します。コードは次のようになります。\n\nlibrary(mirt)\n# 2因子(model = 2)，段階反応(itemtype = 'graded')を指定\nresult.mirt &lt;- mirt(dat, model = 2, itemtype = \"graded\")\n# 出力に際して斜交回転\nsummary(result.mirt, rotate = \"geominQ\")\n\n# 多次元ICCの描画\nplot(result.mirt, type = 'trace', which.items = 1)\n# 多次元IICの描画\nplot(result.mirt, type = 'info')\n\n\n\n\n\nBernaards, Coen A., and Robert I. Jennrich. 2005. “Gradient Projection Algorithms and Software for Arbitrary Rotation Criteria in Factor Analysis.” Educational and Psychological Measurement 65: 676–96. https://doi.org/10.1177/0013164404272507.\n\n\nアデラ＝マリアイスヴォラヌ, サシャエプスカンプ, ローレンスウォルドープ, and デニーボースブーム. (2022) 2024. 心理ネットワークアプローチ入門:行動科学者と社会科学者のためのガイド. Translated by 樫原潤 and 小杉考司. 勁草書房.\n\n\n宮川雅巳. 1997. グラフィカルモデリング (統計ライブラリー). 朝倉書店.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房. http://ci.nii.ac.jp/ncid/BB27527420.\n\n\n岡太彬訓, and 今泉忠. 1994. パソコン多次元尺度構成法. 共立出版.\n\n\n新納浩幸. 2007. Rで学ぶクラスタ解析. オーム社.\n\n\n西里静彦. 2010. 行動科学のためのデータ解析–情報把握に適した方法の利用. 培風館.\n\n\n足立浩平. 2006. 多変量データ解析法: 心理・教育・社会系のための入門. ナカニシヤ出版.\n\n\n高根芳雄. 1980. 多次元尺度法. 東京大学出版会.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter14.html#footnotes",
    "href": "chapter14.html#footnotes",
    "title": "14  多変量解析(その1)",
    "section": "",
    "text": "これは因子の複雑さを表す指標で、各項目(変数)がどれだけ単純に(あるいは複雑に)因子に負荷しているかを表す指標である。値が1に近い場合，その項目は基本的に1つの因子にのみ強く負荷することあらわしている。値が大きくなるほど、その項目が複数の因子に分散して負荷していることになる。この値は，項目\\(j\\)の因子\\(k\\)に対する負荷量を\\(a_{jk}\\)としたとき，\\(\\frac{(\\sum_k a_{jk}^2)^2}{\\sum_k a_{jk}^4}\\)で算出する。↩︎\nらばーん，とは変な名前だと思われるかもしれないが，LAtent VAriable ANalysisすなわち潜在変数分析の意味である。↩︎\nlavaanguiというパッケージを用いれば，モデルの指摘もGUIでできる。↩︎\nStd.allは観測変数も潜在変数もその分散を1に標準化したもの。Std.lvは潜在変数だけその分散を1に標準化したものである。↩︎\n実際，日本最大級の学力テストである大学入学共通試験においてもCATの導入が検討されたが，膨大な項目プールの必要性(予備調査)や，地方や離島などの遠隔地などでも都市部と同じ通信環境，実行環境の準備などを考えると現実的でないということから見送られている現状がある。現行の紙とペン(マークシート)を用いた受験システムは，毎年50万人程度が同時に受けても一桁パーセント以下の誤謬率しかないという驚異の精度で運用されている極めて優れた実践システムであり，その社会的インパクトの大きさから考えても，CATの導入は慎重にならざるを得ない。↩︎\n最尤推定にすると，全問正答あるいは全問誤答の場合には，その項目の母数が無限大になってしまうから，実用的でないからである。↩︎\n最尤推定にすると，全問正答あるいは全問誤答の場合には，その項目の母数が無限大になってしまうから，実用的でないからである。↩︎\nこれは元々2500以上のサンプルがあるMotivational State Questionnaire (MSQ)の一部。全体はpsychToolsパッケージに入っている。↩︎\nこれは元々2500以上のサンプルがあるMotivational State Questionnaire (MSQ)の一部。全体はpsychToolsパッケージに入っている。↩︎",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多変量解析(その1)</span>"
    ]
  },
  {
    "objectID": "chapter15.html",
    "href": "chapter15.html",
    "title": "15  多変量解析(その2)",
    "section": "",
    "text": "15.1 そのほかの分析1；距離行列を用いるもの",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "chapter15.html#クラスター分析",
    "href": "chapter15.html#クラスター分析",
    "title": "15  多変量解析(その2)",
    "section": "15.2 クラスター分析",
    "text": "15.2 クラスター分析",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "chapter15.html#多次元尺度構成法",
    "href": "chapter15.html#多次元尺度構成法",
    "title": "15  多変量解析(その2)",
    "section": "15.3 多次元尺度構成法",
    "text": "15.3 多次元尺度構成法",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "chapter15.html#そのほかの分析2共頻行列を用いるもの",
    "href": "chapter15.html#そのほかの分析2共頻行列を用いるもの",
    "title": "15  多変量解析(その2)",
    "section": "15.4 そのほかの分析2；共頻行列を用いるもの",
    "text": "15.4 そのほかの分析2；共頻行列を用いるもの",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>多変量解析(その2)</span>"
    ]
  },
  {
    "objectID": "practices.html",
    "href": "practices.html",
    "title": "17  演習問題",
    "section": "",
    "text": "17.1 最終課題\nこのコースでは，確率的に生じるデータを意識しながら，そのメカニズムから生成されるデータを乱数によって具体化し，そうしたサンプルデータに基づいて検定と分析のロジックを学んできました。 検定には，サンプルサイズ，有意水準，検出力，効果量が相互に関わっており，確率的判断がこれらの関数としてどのように現れるかを確認しました。 また仮想データをつくれるところから，QRPsのシミュレーションを行ったり，例数設計が行えることも見てきました。 線形モデルやそのほか発展的なモデルについても，データ生成メカニズムの観点からアプローチできます。\nより発展的な内容になりますが，確率的プログラミング言語を用いれば，データ生成メカニズムを記述することでモデルパラメータの推定が可能になります。 確率的プログラミング言語を利用するには，プログラミングの技術，ベイズ統計の理論，MCMCによる近似の理論と方法についての知識が必要です。 これらは，このコースで扱ってきたR言語による非ベイズ統計的モデルへの乱数近似アプローチのちょっと先にあるものです。\n本コースの内容を十分に身につけていれば，すぐにでも対応できるでしょう！",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>演習問題</span>"
    ]
  },
  {
    "objectID": "practices.html#最終課題",
    "href": "practices.html#最終課題",
    "title": "17  演習問題",
    "section": "",
    "text": "無相関検定において，真の状態が母相関\\(\\rho=0.4\\)であったときに，サンプルサイズ\\(n=20\\)のデータをとって検定を行うとします。この時の帰無仮説の分布と，真の状態の分布を重ねて図示し，\\(\\alpha=0.05\\)の臨界値，検出力を可視化する図を描くコードを書いてください。(参考；南風原 (2002) ,Pp.144)\n2要因Betweenデザインの分散分析において，交互作用のみ有意になるようなサンプルデータを作るコードを書いてください。また，サンプルデータが正しくできているかどうかを確認するために，anovakunでの分析結果も出力させてください。\n2つの変数X,Yをもつ3つの群があり，群ごとX,Yの相関を見るとすべて\\(r = -0.3\\)程度の負の相関を持っているが，3つの群をあわせてX,Yの相関を見ると正の相関を示すようなデータセットを作流コードを書いてください。なお，出来上がったデータは群ごとに色分けした散布図で図示するようにしてください。\n\nヒント：群ごとに回帰分析のサンプルデータを作ることを考え，傾きは一貫して\\(\\beta_1=-0.3\\)であるのに対し，群ごとの切片\\(\\beta_0\\)を適当に調整すると良いでしょう。\nねらい：このようなデータは，相関を見るときに可視化することの重要性を伝えるとともに，階層線形モデルの必要性を理解することに役立ちます。\n\n\n\n\n\n\n南風原朝和. 2002. 心理統計学の基礎: 統合的理解のために. 有斐閣. http://amazon.co.jp/o/ASIN/4641121605/.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>演習問題</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bernaards, Coen A., and Robert I. Jennrich. 2005. “Gradient\nProjection Algorithms and Software for Arbitrary Rotation Criteria in\nFactor Analysis.” Educational and Psychological\nMeasurement 65: 676–96. https://doi.org/10.1177/0013164404272507.\n\n\nGabry, Jonah, Rok Češnovar, and Andrew Johnson. 2023. Cmdstanr: R\nInterface to ’CmdStan’.\n\n\nHadley, Wickham. 2014. “Tidy Data.” Journal of\nStatistical Software 59: 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nRevelle, William. 2021. Psych: Procedures for Psychological,\nPsychometric, and Personality Research. Evanston, Illinois:\nNorthwestern University. https://CRAN.R-project.org/package=psych.\n\n\nRosseel, Yves. 2012. “lavaan: An\nR Package for Structural Equation Modeling.”\nJournal of Statistical Software 48 (2): 1–36. https://doi.org/10.18637/jss.v048.i02.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nZeileis, Achim. 2005. “CRAN Task Views.” R\nNews 5 (1): 39–40. https://CRAN.R-project.org/doc/Rnews/.\n\n\nアデラ＝マリアイスヴォラヌ, サシャエプスカンプ, ローレンスウォルドープ,\nand デニーボースブーム. (2022) 2024.\n心理ネットワークアプローチ入門:行動科学者と社会科学者のためのガイド.\nTranslated by 樫原潤 and 小杉考司. 勁草書房.\n\n\nキーラン・ヒーリー. (2018) 2021.\nデータ分析のためのデータ可視化入門. Translated by 瓜生真也,\n江口哲史, and 三村喬生. 講談社.\n\n\nクルシュケJ. K. (2014) 2017. ベイズ統計モデリング: R, JAGS,\nStanによるチュートリアル 原著第2版. Translated by 前田和寛 and\n小杉考司. 共立出版.\n\n\nシ. 2016. 計算機言語のまとめノート. 暗黒通信団.\n\n\nシャロン・バーチュ・マグレイン. (2011) 2018.\n異端の統計学ベイズ. Translated by 冨永星. 草思社.\n\n\nランダー，J.P. (2017) 2018. みんなのr 第2版. Translated by\n高柳慎一, 津田真樹, 牧山幸史, 松村杏子, and 簑田高志. マイナビ出版.\n\n\n佐藤坦. 1994. はじめての確率論: 測度から確率へ. 共立出版.\n\n\n南風原朝和. 2002. 心理統計学の基礎: 統合的理解のために. 有斐閣.\nhttp://amazon.co.jp/o/ASIN/4641121605/.\n\n\n———. 2014. 心理統計学の基礎: 続・統合的理解のために. 有斐閣.\n\n\n吉田伸生. 2021. 確率の基礎から統計へ. 新装版. 日本評論社.\n\n\n吉田寿夫, and 村井潤一郎. 2021.\n“心理学的研究における重回帰分析の適用に関わる諸問題.”\n心理学研究 92 (3): 178–87. https://doi.org/10.4992/jjpsy.92.19226.\n\n\n宮川雅巳. 1997. グラフィカルモデリング (統計ライブラリー).\n朝倉書店.\n\n\n小杉考司. 2018. 言葉と数式で理解する多変量解析入門. 北大路書房.\nhttp://ci.nii.ac.jp/ncid/BB27527420.\n\n\n小杉考司, 紀ノ定保礼, and 清水裕士. 2023.\n数値シミュレーションで読み解く統計のしくみ〜Rでためしてわかる心理統計.\n技術評論社.\n\n\n岡太彬訓, and 今泉忠. 1994. パソコン多次元尺度構成法. 共立出版.\n\n\n平岡和幸, and 堀玄. 2009. プログラミングのための確率統計.\nオーム社. http://amazon.co.jp/o/ASIN/4274067750/.\n\n\n新納浩幸. 2007. Rで学ぶクラスタ解析. オーム社.\n\n\n松村優哉, 湯谷啓明, 紀ノ定保礼, and 前田和寛. 2021. 改訂2版\nRユーザのためのRStudio[実践]入門:\nTidyverseによるモダンな分析フローの世界. 技術評論社.\n\n\n株式会社ホクソエム, trans. (2016) 2017. Rプログラミング本格入門:\n達人データサイエンティストへの道. 単行本. 共立出版.\n\n\n永田靖, and 吉田道弘. 1997. 統計的多重比較法の基礎.\nサイエンティスト社.\n\n\n池田功毅, and 平石界. 2016.\n“心理学における再現可能性危機：問題の構造と解決策.”\n心理学評論 59 (1): 3–14. https://doi.org/10.24602/sjpr.59.1_3.\n\n\n河野敬雄. 1999. 確率概論. 京都大学学術出版会.\n\n\n石田基広, 市川太祐, 高柳慎一, and 福島真太朗, trans. (2015) 2016.\nR言語徹底解説. 共立出版.\n\n\n総務省. 2020.\n“統計表における機械判別可能なデータ作成に関する表記方法.”\n統計企画会議申し合わせ. https://www.soumu.go.jp/main_content/000723697.pdf.\n\n\n西内啓. 2017. 統計学が最強の学問である[数学編]:\nデータ分析と機械学習のための新しい教科書. ダイヤモンド社.\n\n\n西里静彦. 2010.\n行動科学のためのデータ解析–情報把握に適した方法の利用. 培風館.\n\n\n豊田秀樹. 2009. 検定力分析入門: Rで学ぶ最新データ解析ー.\n東京図書.\n\n\n———. 2017. もうひとつの重回帰分析. 東京図書.\n\n\n足立浩平. 2006. 多変量データ解析法:\n心理・教育・社会系のための入門. ナカニシヤ出版.\n\n\n高根芳雄. 1980. 多次元尺度法. 東京大学出版会.\n\n\n高橋康介. 2018. 再現可能性のすゝめ. Edited by 石田基広. Vol. 3.\nWonderful r. 共立出版.",
    "crumbs": [
      "References"
    ]
  }
]