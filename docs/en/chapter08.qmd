# Testing for Difference in Means

The test of mean difference is a method used to draw conclusions from a research design. This is because random assignment offsets individual differences and background factors, allowing for the verification of the average causal effect.
To generalize these results, knowledge of inferential statistics is indeed necessary, with sample size and Type 1 and 2 errors remaining crucial factors.

## One-Sample Test


We'll start with an example of permutation sample testing. This method is used to determine whether the sample mean is statistically significantly different from a specific value, which is known or theoretically assumed as the population mean.
For example, when collecting data based on a 7-point scale, we determine whether or not it's accurate to say that the average of a certain item significantly deviates from the midpoint of 4. Suppose we obtained data from a 7-point scale with a sample size of 10. Here, we represent it by generating 10 normal random numbers with an average of 4 and a standard deviation of 1. In reality, these values should be obtained as responses to scale categories from individuals.

```{r,oneTsample}
library(tidyverse)
set.seed(17)
n <- 10
mu <- 4
X <- rnorm(n, mean = mu, sd = 1)
print(X)
```

In this case, the sample mean is `r round(mean(X),3)`. From here, we will test whether we can obtain a more extreme value from a population where $\mu = 4$. If we follow the procedure for null hypothesis testing, it would look something like this.

1. The null hypothesis is that the population mean is the theoretical value (here the midpoint of the scale 4), in other words, $\mu =4$. The alternative hypothesis is $\mu \neq 4$.
2. The test statistic is the sample distribution that the sample mean derived from a normal population follows. It turns into the T statistic used for interval estimation when the population variance is unknown.
3. We set the decision criterion at 5%, in accordance with the conventions of psychology.

Next, we will carry out the computation and determination of test statistics. R can handle this all at once with the `t.test` function.

```{r,Os.ttest}
result <- t.test(X, mu = mu)
print(result)
```

As a result, the realized value of our test statistic is `r round(result$statistic,3)`, and the probability of getting a value higher than this from a t-distribution with `r result$parameter` degrees of freedom is `r round(result$p.value,3)`. Since this is larger compared to the 5% level, we can conclude that it's not a rare case. In other words, it's not unusual to get a sample mean of `r round(mean(X),3)` from a normal population with a mean of 4, and we cannot say it's statistically significantly different.

When writing reports, consider these actual values and p-values and indicate as "$t(9)=0.66776, p=0.5151, n.s.$" The "n.s." here stands for "not significant".

In this example, we generated normal random numbers with a mean of 4 and concluded that the mean does not necessarily differ from 4. This may seem self-evident and perhaps even pointless at first glance. However, let's consider the following example.

```{r,Os.ttest2}
n <- 3
mu <- 4
X <- rnorm(n, mean = mu, sd = 1)
mean(X) %>%
  round(3) %>%
  print()
result <- t.test(X, mu = mu)
print(result)
```

In this instance, the sample size is $n=3$, and the sample mean was `r round(mean(X),3)`. If the t-value exceeds the 5% critical value, it is considered 'extreme' compared to what would typically be obtained from a population mean of 4, and is therefore statistically significantly different. Remember, even though the mean was indeed set to 4 when generating random numbers, it's perfectly possible for a small part drawn from the population mean to veer significantly from it.

## Two-Sample Test

Let's now consider the test of two samples. This test is performed when looking at the average causal effect by randomly assigning groups, like an experimental group and a control group. The null hypothesis is that "there is no difference between the groups," and the alternative hypothesis is the negation of that. Also, assuming samples from a normal population, the test statistic here also follows a t-distribution. Let's confirm this again following the procedure of the null hypothesis test.

1. The null hypothesis is "There is no difference in the mean of the two groups." If we denote the mean of the two groups as $\mu_1$ and $\mu_2$ respectively, the null hypothesis is represented as $\mu_1 = \mu_2$ or $\mu_1 - \mu_2 = 0$. The alternative hypothesis is $\mu_1 \neq \mu_2$ or $\mu_1-\mu_2 \neq 0$.
2. The test statistic is a sampling distribution of sample means derived from a normal population. In case of unknown population variance, it becomes the T statistic used for interval estimation.
3. We will set the criterion of judgment at 5%, following the convention in psychology.

Let's generate random sample data to verify this.
First, let’s denote the sample sizes of each group as `n1` and `n2`. To keep things simple, this example assumes that the sample size of both groups is `10`. Then we proceed to the mean of both groups. We expressed the first group's population mean as $\mu_1$, and the second group's population mean as $\mu_2 = \mu_1 + d$. The `d` represents the difference. If `d=0`, the population means are equal, and if `d \neq 0`, the population means are different. At the end, we set the population standard deviation (SD) for both groups.

The test conducted here assesses whether it is reasonable to assume that this difference, $d$, arises from a population with a mean of zero. The test statistic $T$ is calculated using the following formula.

Without specific Japanese text provided, the translation cannot be accomplished. The formula presented appears to be a statistical test formula, likely relevant for determining a T-score in experimental psychology or similar fields. It isn't in Japanese and therefore doesn't require translation. It's advisable to provide the Japanese text to be translated.

Here, $U^2_p$ is called the pooled unbiased variance, which is an estimate of the overall population variance calculated by combining the two groups. If we denote the sample variances of each group as $S^2_1, S^2_2$, they are calculated by the following equation.

You've encountered a mathematical formula! This formula looks more complex than it actually is. Don't worry, we will break it down for you.

$$ U^2_p = \frac{n_1S^2_1+ n_2S^2_2}{n_1 + n_2 -2} $$

This is a typical formula for Pooled Variance, denoted here as $ U^2_p $. Pooled variance is a type of weighted average of the variances of two or more groups.

In the numerator of the fraction, you have `n_1S^2_1 + n_2S^2_2`, which represents the sum of the product of each group size `n` and its variance `S^2`. Here, `n_1` and `S^2_1` refer to the size and variance of Group 1, while `n_2` and `S^2_2` are the size and variance of Group 2.

The denominator, `n_1 + n_2 - 2`, is the total number of observations from both groups minus 2. This serves to adjust the variance calculation, taking into account the degrees of freedom in the data.

To understand this in simpler terms, think of pooled variance as a way to find the "average scatter" in your data when you're looking at more than one group at a time. It's an essential tool for understanding and interpreting data, helping you uncover the underlying patterns in your analysis.

Understanding these mathematical concepts is key to mastering psychological statistics with R and RStudio. So, keep practicing, and don't hesitate to ask questions as they come up. Keep up the good work!

In essence, these formulas consider differences in sample sizes. They provide unbiased variance for the entire pool by first multiplying the sample variance of each group by their respective sample sizes, then subtracting $1$ from the total sample size of the pool.

With that in mind, let's take a look at the specific numerical data.
In addition, we generate data with random numbers, confirm the sample mean, and then conduct a test using the `t.test` function.

```{r,Ts.ttest}
n1 <- 10
n2 <- 10
mu1 <- 4
sigma <- 1
d <- 1
mu2 <- mu1 + (sigma * d)

set.seed(42)
X1 <- rnorm(n1, mean = mu1, sd = sigma)
X2 <- rnorm(n2, mean = mu2, sd = sigma)

X1 %>%
  mean() %>%
  round(3) %>%
  print()
X2 %>%
  mean() %>%
  round(3) %>%
  print()

result <- t.test(X1, X2, var.equal = TRUE)
print(result)
```

In this instance, we have set the population means to $\mu_1 = 4, \mu_2 = 4+1$. However, the sample means are `r round(mean(X1),3)` and `r round(mean(X2),3)`, respectively, and no significant difference is observed in the samples. Consequently, the t value is `r abs(result$statistic)`, and the p-value under the degrees of freedom `r result$parameter` is `r result$p.value`. As this exceeds the 5% level, we cannot conclude to accept the alternative hypothesis, in other words, we cannot claim there is a significant difference.

In this setup, there should be a difference in the population mean ($4 \neq 4 + 1$), so this is a case where a type II error is being made, which is an incorrect judgment. In practical research settings, note that it would be impossible to know whether such a judgment error occurred, as we have no way of knowing about the population mean or any differences in it.

Moreover, in this case, to indicate that there are two groups for easy understanding, we prepared two objects, `X1` and `X2`. However, in practice, there is often a variable indicating group divisions within the data frame, which you would likely write in the form of a `formula` as follows.

```{r}
dataSet <- data.frame(group = c(rep(1, n1), rep(2, n2)), value = c(X1, X2)) %>%
  mutate(group = as.factor(group))
t.test(value ~ group, data = dataSet, var.equal = TRUE)
```


## Two-Sample Test (Welch's Correction)

The previous t.test function added an option, `var.equal = TRUE`. This is a test that assumes equal variances between the two groups. While t-tests historically first appeared in this form, it's not a premise that can be immediately made whether the variances of the two groups are equal. Testing for equality of variances is typically done using the Levene test, and in R, the `car` and `lawstat` packages provide corresponding functions. Here, we will show an example using the `leveneTest` function from the `car` package.

```{r}
library(car)
leveneTest(value ~ group, data = dataSet, center = mean)
```

Upon reviewing the results, as indicated by the p-value, we were unable to reject the null hypothesis that the variances of the two groups are equal. Consequently, we can presume their equality and proceed to the t-test. If we were to reject this, it would mean that the null hypothesis of equal variances does not hold. In this case, we would need to remove the assumption of equal variances. The execution is straightforward: simply set `var.equal` to `FALSE`.

```{r}
result2 <- t.test(value ~ group, data = dataSet, var.equal = FALSE)
print(result2)
```
Upon closer inspection, the title has changed to Welch Two Sample t-test. This refers to a t-test with Welch's adjustment applied. You may also have noticed that the degrees of freedom has become a real number (`r result2$parameter`). By adjusting the degrees of freedom in this way, it is possible to correct for deviations from the assumption of homoscedasticity. Of course, when reporting this, one would write it as "$ t($ `r round(result2$parameter,3)` $) = $ `r round(result2$statistic,3)`, $p = $ `r round(result2$p.value,3) `". Therefore, it can be considered that if the degrees of freedom are real, they are adjusted.

However, the assumption of equal variance is a special system for cases where it is not equal, so it's sufficient to only use the Welch’s adjusted test from the start. From this perspective, the default setting for the 't.test' function in R is 'var.equal = FALSE', and it does not assume equal variance unless specifically specified. This is more desirable as it eliminates the need for repeated testing.

## Paired Two-Sample T-Test

## Assignments Similar to Writing a Report

