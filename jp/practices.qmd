# 演習問題

このコースでは，確率的に生じるデータを意識しながら，そのメカニズムから生成されるデータを乱数によって具体化し，そうしたサンプルデータに基づいて検定と分析のロジックを学んできました。

検定には，サンプルサイズ，有意水準，検出力，効果量が相互に関わっており，確率的判断がこれらの関数としてどのように現れるかを確認しました。

また仮想データをつくれるところから，QRPsのシミュレーションを行ったり，例数設計が行えることも見てきました。
線型モデルやそのほか発展的なモデルについても，データ生成メカニズムの観点からアプローチできます。

また，一般線型モデル，一般化線型モデル，一般化線型混合モデル，階層線型モデルと，線型モデルの展開について見てきました。

いずれのモデルも，データによりフィットした，より適切なモデルにするための工夫です。もちろん複雑なモデルであればあるほど良い，というわけではありません。むしろモデルは単純な方がいいのですが，単純だからという理由でデータに合わないモデルを適用するのは，適切ではありません。

モデルが複雑になるにつれて，推定方法も最小二乗法，最尤法，ベイズ法と展開してきました。いずれも同じ母数を推定するための手法ですから，ここに優劣はなく，最終的な結論も同じになるべきです。確率の解釈の違いなどもありますので，主義主張の対立と思われることもありますが，ユーザとしては実用面での有用性から選ぶということでも良いかもしれません。

より発展的な内容として，確率的プログラミング言語を用いれば，データ生成メカニズムを記述することでモデルパラメータの推定が可能になルコとにも触れました。

確率的プログラミング言語を利用するには，プログラミングの技術，ベイズ統計の理論，MCMCによる近似の理論と方法についての知識が必要です。ですが，これらの技術を身につけると，分析の可能性はもっと広がります。既存の統計モデルを当てはめるのではなく，あなた自身の考えたモデルを作り上げることもできます。その自由度は無限ともいえるほどで，想像力を働かせて分析モデルをデザインするのは大変楽しいことでもあります。また，そういう観点を得ることができれば，従来の統計モデルがどういう設計をされているのかについても理解が進みます。

MCMCは確率を乱数で具現化する手法です。MCMCに限らず，確率を乱数で具現化し，具体的なデータとして分析することで，統計モデルを使いこなせるようになりましょう。ツールは使うべきもので，使われるべきものではないからです。

## 最終課題

+ 無相関検定において，真の状態が母相関$\rho=0.4$であったときに，サンプルサイズ$n=20$のデータをとって検定を行うとします。この時の帰無仮説の分布と，真の状態の分布を重ねて図示し，$\alpha=0.05$の臨界値，検出力を可視化する図を描くコードを書いてください。(参考；@Haebara200206 ,Pp.144)

+ 2要因Betweenデザインの分散分析において，交互作用のみ有意になるようなサンプルデータを作るコードを書いてください。また，サンプルデータが正しくできているかどうかを確認するために，`anovakun`での分析結果も出力させてください。

+ 2つの変数X,Yをもつ3つの群があり，群ごとX,Yの相関を見るとすべて$r = -0.3$程度の負の相関を持っているが，3つの群をあわせてX,Yの相関を見ると正の相関を示すようなデータセットを作るコードを書いてください。なお，出来上がったデータは群ごとに色分けした散布図で図示するようにしてください。
    + ヒント：群ごとに回帰分析のサンプルデータを作ることを考え，傾きは一貫して$\beta_1=-0.3$であるのに対し，群ごとの切片$\beta_0$を適当に調整すると良いでしょう。
    + ねらい：このようなデータは，相関を見るときに可視化することの重要性を伝えるとともに，階層線型モデルの必要性を理解することに役立ちます。

+ brmsパッケージを用いて，ベイズ統計の観点から重回帰分析を実行してください。従属変数を自由に設定し，複数の説明変数を含むモデルを構築してください。事前分布の設定，MCMCの収束診断，事後分布の可視化，および最尤推定との比較を含めた包括的な分析を行ってください。データは何を用いても構いません。
    + ヒント：`brm()`関数で`prior`オプションを使って事前分布を明示的に設定し，`plot()`と`pp_check()`を用いて結果を評価してください。サンプルデータとしては`mtcars`や`iris`，`psych::bfi`などが利用できます。
    + ねらい：ベイズ統計の実践的な応用と，頻度主義統計との違いを理解することを目的とします。

+ 探索的因子分析と確認的因子分析の比較分析を行ってください。同一データセットに対して，まず探索的因子分析（EFA）を実行して因子構造を探索し，次にその結果を基に確認的因子分析（CFA）モデルを構築してください。両手法の結果を比較し，それぞれの利点と限界について考察してください。データは何を用いても構いません。
    + ヒント：`psych`パッケージの`fa()`関数でEFAを実行し，`lavaan`パッケージの`cfa()`関数でCFAを実行してください。適合度指標の比較も含めてください。サンプルデータとしては`psych::bfi`（ビッグファイブ性格検査），`psych::ability`（認知能力検査），`HolzingerSwineford1939`（Holzinger & Swinefordの認知テスト）などが利用できます。
    + ねらい：測定モデルの探索と検証の違い，およびそれぞれの統計的手法の特徴を理解することを目的とします。

+ 階層線型モデル（HLM）またはGLMMを用いて，ネストされたデータ構造を持つ分析を実行してください。個人がグループにネストされている状況を想定し，ランダム切片モデルとランダム傾きモデルの両方を比較してください。また，ICCを計算してグループレベルの効果の大きさを評価してください。データは何を用いても構いません。
    + ヒント：`brms`パッケージの`(1|group)`（ランダム切片）と`(1+x|group)`（ランダム切片ランダム傾き）の記法を使い分けてください。サンプルデータとしては`lme4::sleepstudy`（睡眠研究データ），`nlme::Orthodont`（歯科矯正データ），`mlmRev::Exam`（学校別試験成績データ），`mlmRev::Exam`（機械データ）などが利用できます。
    + ねらい：階層構造を持つデータの適切な分析手法を理解し，個人レベルとグループレベルの変動を区別する重要性を学ぶことを目的とします。


```{r}
#| eval: FALSE
#| include: FALSE

pacman::p_load(ggplot2)
# パラメータ設定
rho <- 0.4
n <- 20
alpha <- 0.05
df <- n - 2
# 臨界値の計算（双方向）
critical_value_t <- qt(1 - alpha / 2, df)
# 非心パラメータ（ncp）の計算
ncp <- sqrt(n) * rho / sqrt(1 - rho^2)
# 理論的なt分布の値を計算
t_values <- seq(-5, 5, length.out = 1000)
null_density <- dt(t_values, df)
true_density <- dt(t_values, df, ncp = ncp)
# データフレームの作成
data <- data.frame(
  t_value = rep(t_values, 2),
  density = c(null_density, true_density),
  group = factor(rep(c("Null Distribution", "True Distribution"), each = length(t_values)))
)

# 検出力の計算
power <- pt(critical_value_t, df, ncp = ncp, lower.tail = FALSE) + pt(-critical_value_t, df, ncp = ncp, lower.tail = TRUE)
## 検算
pacman::p_load(pwr)
pwr.r.test(n = 20, r = 0.4)
h # ggplotでプロット
ggplot(data, aes(x = t_value, y = density, color = group, fill = group)) +
  geom_line(size = 1) +
  geom_vline(xintercept = c(-critical_value_t, critical_value_t), linetype = "dashed", color = "black") +
  labs(title = "Distribution of t Values", x = "t Value", y = "Density") +
  scale_color_manual(values = c("red", "blue")) +
  scale_fill_manual(values = c("red", "blue"), guide = "none") +
  annotate("text", x = critical_value_t + 1, y = max(null_density) * 0.5, label = sprintf("Power = %.2f", power), color = "black") +
  theme_minimal()
```

```{r}
#| eval: FALSE
#| include: FALSE
# 必要なパッケージを読み込む
pacman::p_load(tidyverse)

# パラメータ設定
set.seed(123)
n <- 99 # 全体のサンプルサイズ
n_groups <- 3 # サブグループの数
group_size <- n %/% n_groups # サブグループのサイズ

# グループごとにデータを生成
x <- rnorm(n)
y <- numeric(n)

# 各サブグループで負の相関を持つデータを生成
for (i in 1:n_groups) {
  start_index <- (i - 1) * group_size + 1
  end_index <- i * group_size
  intercept <- (i - 2) * 5 # グループごとに異なる切片
  slope <- -0.3 # 一貫した負の相関
  y[start_index:end_index] <- intercept + slope * x[start_index:end_index] + rnorm(group_size, sd = 1)
}

# 全体として正の相関を持つように調整
# 全体の正の相関を持たせるためにyにxを加算しますが、サブグループ内の負の相関は保たれます

# データフレームの作成
data <- data.frame(
  x = x,
  y = y,
  group = factor(rep(1:n_groups, each = group_size))
)

# 散布図をプロット
ggplot(data, aes(x = x, y = y, color = group)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Overall Positive Correlation with Negative Correlation in Subgroups",
    x = "X",
    y = "Y",
    color = "Group"
  ) +
  theme_minimal()

cor(data$x, data$y)
data %>%
  group_by(group) %>%
  summarise(r = cor(x, y))
```
