# 多変量解析(その1)

ここでは心理系で最もよく使われる分析法のひとつ，因子分析を中心に多変量解析の全体的な解説を行う。
多変量解析はその名のとおり，変数が多く含まれるデータの解析であり，その目的は情報の要約にある。多くの変数が含まれる時にひとつ一つの変数を解釈していくのは大変な労力であるから，要領よくまとめることができればそれに越したことはないからである。

この目的から派生して，多変量解析にはさまざまな意味解釈が付随する。以下にモデルの解釈と対応する多変量解析技術を列記してみた。

+ 要約する＝すべての情報を使わず，いくつかの情報を捨象することでもある。
+ 1つの合成変数にまとめ上げる：総合評価として一次元化することでもある(主成分分析)。
+ 2つ3つなど少数の次元に写像する：多次元空間に可視化することでもある(多次元尺度法)。
+ 少数のグループに変数を分類する：グループごとの特徴を記述したり分析することで解釈を深める(クラスター分析)。
+ 観測変数の背後にある少数の潜在変数にまとめ上げる：構成概念を測定して数値を割り振る(因子分析)。
+ 観測変数が0/1のバイナリデータに対して1つの潜在変数からの影響を考える：正答誤答を意味するテストと考える(項目反応理論)
+ 観測された解凍パターンから潜在変数を仮定したグループ分類を行う：隠れた購買層の発見などに使われる(潜在クラス分析)
+ 潜在変数間の構造的な関係もモデル化する：回帰分析や因子分析などの線形関数関係をデータ全体に当てはめて考察する(構造方程式モデリング)
+ 潜在変数を仮定せず，変数同士の結びつきの強さを可視化する：ノードnodeとタイtieで位相的な関係を描画したり，数学的構造を含めたモデリングを行ったりする(ネットワーク分析)

これらのモデルに共通する本質的な特徴は「変数間関係をデータから見出すこと」である。変数間関係をどのようなもので表現するかによって，多少モデルの扱い方は変化する。一般的には分散共分散(相関係数)を変数間関係とするが，この場合は間隔尺度水準以上のデータが得られていることが必要になる。もし順序尺度水準でしか得られていないのであれば，ピアソンの相関係数の代わりにポリコリック相関係数やポリシリアル相関係数と呼ばれる相関係数を用いることになる。0/1のバイナリデータの場合はさらに特殊で，ピアソンの相関係数の代わりにテトラコリック相関係数を用いることになる。

変数間関係は必ずしも相関係数だけではない。カテゴリカル変数の場合は，あるカテゴリと他のカテゴリが同時に選択される・発生する頻度(共頻関係)を，その変数間関係の指標として捉えることができる。共頻関係を分析する手法としては双対尺度法[@Nishisato2010] (対応分析や数量化III類と原理的に同じ)などが知られている。このようなカテゴリデータの分析は，自由記述などの自然言語を形態素解析し，多変量解析で分析するテキストマイニングなどで応用されている。

また，変数間関係を変数同士の類似度，すなわち距離であると考えることもできる。距離の公理を満たすデータがあれば，それを元に多次元尺度構成法[@Takane198002;@Okada19940115]で可視化したり，クラスター分析で分類[@clusterAnalysis; @Adachi200607]したりすることができる。

さらに，相関係数は変数間の直線的な関係の強さを意味するが，隠れた変数による擬似相関の可能性も含まれるため，偏相関係数を使って周囲の変数からの影響を統制した変数間関係を考えることもある。この手法はグラフィカルモデリング[@Miyagawa1997]やネットワーク分析[@Network2024]で用いられるものである。

いずれにせよ，こうした変数間関係をもとに，外的な基準があればそこに対するフィッティングを目的として未知数を推定するし，外的な基準がなければモデル的仮定に基づいてデータから構造化していくことになる。またその目的の基本は情報の要約であるが，可視化に重点をおいたモデルや潜在得点の推定に重点を置いたモデルなど，各種モデルによって得意とする場所や理論的に強調されるところは異なる。

やや本筋から外れるが，こうした多変量を同時に扱うための数学的基盤として，線形代数の知識が必要となることが少なくない。線形代数はベクトルや行列の演算体系であり，その利点は「計算」と「可視化」を統合する観点を得られるところにある。より深く理解したいものにとっては，これらの学習も合わせて行うことを期待する。

## 因子分析

ここでは心理学で最もよく用いられる手法の一つである，因子分析法について概説する。
因子分析法は測定についての統計モデルである。類似の手法として主成分分析があげられるが，主成分分析は測定のモデルというより要約のモデルというべきである。

因子分析のモデルは次の式で表される。
$$ z_{ij} = a_{j1}f_{i1}+a_{j2}f_{i2}+cdots+a_{jm}f_{im}+d_jU_{ij} $$

ここで$z_{ij}$は個人$i$の項目$j$に対するスコアを標準化したものである。$a_{j.}$は項目$j$の因子負荷量(factor loadings)，$f_{i.}$は個人$i$の因子得点(factor score)，$d_j$は項目$j$の独自因子負荷量，$U_{ij}$は項目$j$に伴う個人$i$の独自因子得点である。$a_{j.}$で表される$m$個の因子を共通因子と呼ぶ。一般に$m$は項目数$M$よりもかなり小さい。例えば性格検査のBIG-fiveは$M=25$で$m=5$である。YG正確検査は$M=120$で$m=12$である。この意味で多変量解析の目的の一つ，情報圧縮のモデルであるということもできる。

これに対して主成分分析は次のように表される。
$$ P_{i} = w_1X_{i1} + w_2X_{i2} + \cdots + w_MX_{iM} $$

ここで$X_{i.}$は個人$i$の項目$j$に対する反応を表し，この重み付き線型結合で主成分$P$を形成する。ここでの未知数は$w_j$であるから，一つの合成変数に作るための最適な重みを見つけることが目的となる。その基準の一つが，生成される合成変数$P$が個人$i$の特徴を最大限際立たせるように，すなわち$P_i$の分散を最大にすることと考える。もちろん一つの合成変数で$M$個の変数が持つ情報をすべて反映させることは難しいので，第二，第三の合成得点(主成分)を作ることも可能である。

そうすると，情報圧縮という観点から見た$m$個の共通因子，$m$個の主成分の違いは何だろうか。これはすでに述べたように，因子分析は測定のモデルなので，得られたデータ$z_{ij}=\frac{X_{ij}-\bar{X_j}}{\sigma_j}$には誤差$d_jU_{ij}$が含まれていると考えているのに対し，主成分分析では$X_{ij}$にそれを仮定せず，得られた値をそのまま用いているところが異なる。

実践的な面では，心理尺度のような反応に誤差が仮定されるものには因子分析を用い，公的な記録など値に誤差が想定されないものには主成分分析を用いることが相応しい。因子分析が心理学やテスト理論の領域で広まり，主成分分析が経済学，商学，社会学の領域で広まったのはそうした背景による。

計算論的には，いずれも変数間関係を元に最大限説明できる要素を抽出するというところで，行列の固有値分解を用いるという点が同じであるから，統計パッケージによっては同じメニューで異なる出力になっているものも少なくない。しかし上で述べたように，モデルの設計上の違いがあることは知っておいて損はないだろう。また，因子分析は変数間関係として相関行列を，主成分分析は分散共分散行列を用いることが多い。これは因子分析を用いる心理学的な領域では，測定値に絶対的な意味がなく相対的な意味(ex.より外向的，より内向的)しかないことに対し，他の社会科学領域では絶対的な意味がある(ex.国家間の貿易黒字・赤字の額など)場合が多いからである。また，主成分分析は多くの変数を情報圧縮する目的で第一主成分のみに注目することが多いのに対し，因子分析は測定しているものの考え方から複数の因子を考えることが多い。因子分析において単因子で考えるか多因子で考えるかについては，知能検査において知能を一般的な単一の因子で考えるのか，各領域に対応する複数の因子があるのかといった，理論的な相違がその黎明期にみられたことを反映している。

類似の手法ではあるが，こうした背景を知っておくことで適切な手法を用いることができるようになるだろう。

### 探索的因子分析

特に断りなく単に因子分析というとき，探索的因子分析(Exploratory Factor Analysis)を指すことが多い。探索的というのは，因子負荷量(因子から項目へのパス係数，因子と項目の関係の強さを反映したもの)はもちろん共通因子の数についても事前に定めず，データから因子構造を探ることを目的とするものだからである。

探索的因子分析は次のステップで進められる。

1. 因子数の決定
2. 因子負荷量の推定(因子軸の回転)
3. 因子得点の推定

もちろん分析に入る前に，分析対象となるデータの記述統計や可視化を通じて基本的な項目属性を把握していることが前提である。

#### 因子数の決定

因子分析を数学的に語れば，$M$個の項目相互の相関係数を表した相関行列$\mathbb{R}$を固有値分解することに尽きる。相関係数はピアソンの積率相関係数を用いることが一般的であるが，項目が順序尺度水準であるとかバイナリ変数であるとかいった場合は，それに応じた相応しい相関係数を用いる。

固有値分解とは相関行列の次元性を見ることでもある。$M$個の項目もつ情報は$M$次元あると考える。例えば2変数$X,Y$があれば，変数$X$をx軸，変数$Y$をy軸に取った2次元空間に核反応をプロットすることでデータ全体の関係を表現できるだろう。しかしこれらの二変数が相関しているなら，変数$X,Y$を直交させた空間で表現する必要は必ずしもなく，より分散の大きくとれる二次元基底を見つけることができるに違いない。これが因子分析，主成分分析に共通する考え方であり，最大の分散を持つ次元に注目するのが主成分分析，多次元のなかで有用な次元を共通因子，それ以外を誤差因子と区別して多因子(多次元)で考えるのが因子分析である。




ここで共通因子の数を決めるのは分析者であり，「有用な次元」の決定は主観的な側面を含むことに注意しよう。もちろんデータの構造から適した次元数を考える手法はいろいろ提案されており，昨今はより客観的基準で因子数を決定するのが一般的であるが，数学的な特徴から実践的な意味合いをもつ共通次元とみなすのは，あくまでも分析者の責任において行われるものである。

因子数を決定する手法として，スクリープロットをつかった平行分析がある。
次のコードを見ながら具体的に見ていこう。分析には`psych`パッケージを用い，データは`psych`パッケージの持つサンプルデータ，`bfi`を用いる。
これは性格テストのビッグファイブ因子それぞれについて5項目で測定したデータである。

```{r parallel}
#| message: FALSE
library(tidyverse)
library(psych)
dat <- psych::bfi |> select(-gender, -education, -age)
# 並行分析
fa.parallel(dat)
```

データ行列から得られる固有値の大きい順に折れ線グラフを描いたものを，スクリープロットという。
デフォルトではPCすなわち主成分分析Principle Component Analysisのスクリープロットと，FAすなわち因子分析Factor Analysisのスクリープロットが表示されている。この違いは上で述べたように，データに誤差を仮定するかどうかの違いにある。因子分析はこれを仮定するため1つの項目のもつ情報量が1単位以下になる(相関係数$r_{jj}$が$1.0$より小さくなる。正確には，$r_{jj} = 1-h_j^2 = u^2 < 0$であり，ここで$h_j^2$は共通性と呼ばれる共通因子負荷量の二乗和，$u_j^2$は独自性因子負荷量の二乗和)ため，主成分分析のそれより必ず低くなる。

プロットされているのはActual Data, Simulated Data, Resmapled Dataとなっているのがわかるだろう。実際のデータは何らかの意味構造を有しているだろうから，その相関関係にも偏りが生じ，よく説明できる次元とそうでない次元とが生まれるため，徐々に減衰するカーブで表示される。これに対してSimulated Dataは同じサイズの乱数データから，Resampled Dataは実際のデータをごちゃ混ぜにした行列を作って得られた固有値構造を表している。乱数や撹拌したデータは実際の意味構造を持たず，どの次元も均等に無意味になるため，フラットな線で表示されるだろう。このフラットな線と実データの線を比べ，フラットなラインよりも大きな意味がある次元は無意味ではない，と考えて因子数を決めるのが平行分析の考え方である。この考え方に基づくと，因子分析解も主成分分析解も6因子(6成分)がてきせつであるということになる。

なお図中には固有値が1.0のところにもラインが引かれている。これはかつて使われていたガットマン基準というもので，項目1つ分の分散も持たないような因子は共通因子たり得ない，という考え方である。この考え方によると3因子が妥当ということになる。ただし判断の基準として，共通因子で分散全体の何%を説明したか，というのもあり，たとえば3因子までで50%も説明しないようであれば半分以上の情報を捨てることになるから，4，5因子まで採用するという考え方もあり得る。

#### 因子負荷量の推定

因子の数が決まると，その過程のもとで因子負荷量の推定に入る。例えば次のようにして結果を得る。

```{r fa}
result.fa <- fa(dat, nfactors = 6, fm = "ML", rotate = "geominQ")
```

`psych`パッケージの`fa`関数は実に多くのオプションを持っているが，ここでは因子数(`nfactors`)，推定法(`fm`)，回転法(`rotate`)の3つを指定した。
因子数はすでに述べたので，推定法と回転法について解説する。

推定法は，ここでは**最尤法(`ML`)**を指定した。サンプルサイズが200を超えるような大きなデータであれば，多変量正規分布のもとからデータが得られたと仮定して因子負荷量を推定するのが最も適切だろう。サンプルサイズが小さい場合は，**最小二乗法系列(`ULS,OLS,WLS,GLS`など)**の推定法を指定し，データとモデルのずれを最も小さくするような手法にするのが良い。特段の指定がなければ**最小残差法(`minres`)**が選ばれる。これは最小二乗法と同じだが，アルゴリズムが改善されていて収束しやすいという特徴がある。推定法として主成分解(`pa`)を選べば，残差を推定しないモデルとなる。アルゴリズムの違い，仮定の違いなどでいろいろ変えうるが，基本的にこれで大きく変化が出るようなものではない。

回転法は因子負荷量を推定した後で，さらに解釈をしやすくするためのものである。因子分析や主成分分析は，データの持ってる空間的特徴の軸を見つけ直すという説明はすでにした通りだが，この軸は原点こそ決まっているが，線形代数的変換によって軸を任意の方向に回転させることができる。であれば最も解釈がやりやすい方向に回転させるのが実践上便利である。この解釈がやりやすい方向というのを数学的に言い換えるならば，一つは項目と因子の関係が**単純構造**にあることだろう。単純構造とは，ある項目が特定の因子に寄与しているのなら，そのほかの因子には寄与していないということである。例えば，外向性を測定する項目が第1因子に重く負荷しているのであれば，第2,3,4,5因子には負荷していないほうが解釈しやすい。因子はデータの空間的特徴を表す軸(次元)なのだから，事後的にその軸がの意味であったかを考察する必要があるので，「この因子はこの項目にもあの項目にも影響している」という状況は悩みの種だからである。

この基本方針のもと，いくつかの計算法が考えられている。もっとも古典的なバリマックス回転は，因子負荷量の二乗和の分散が最大になるように回転角を定める。ほかにも，オブリミン回転やジオミン回転などさまざまな回転方法が考えられており，これらについて詳しくは @kosugi2018 などを参照されたい。
また，回転方法は大きく分けて**斜交回転**と**直交回転**とに分けられる。直交回転は回転後の軸が直交する，すなわち因子間相関を仮定しない方法であり，斜交回転は因子間相関を仮定する回転方法である。後者の方が数学的な仮定が緩いため，分析の手順としてはまず斜交回転を行い，因子間相関が十分にひくく直交をかていできるなら直交回転をやり直す，という方法をとるべきである。ちなみにここでは`geominQ`というジオミン回転の斜交版を適用して結果を出力している。 

@GPArotation パッケージには多くの回転法が含まれており，回転法を`rotate`オプションで選択することができるので，ヘルプなどを見て理解を深めて欲しい。

因子軸の回転についても，推定法と同じように絶対的な基準はなく，それぞれの考え方や仮定に基づくアルゴリズムの違いがあるだけである。推定法と違って，因子負荷量は異なる回転法を施すと大きく変わることがある。因子軸の回転は解釈を容易にするためのものであるから，分析者にとって都合の良い回転方法を指定していいが，その回転方法が何で，どういう仮定があるのかについては，自身の言葉で説明できるようになっていた方がいい。

#### 出力結果を確認する

推定法，回転法についての概略を踏まえた上で，結果を見てみよう。
```{r}
print(result.fa, sort = T, cut = 0.3)
```

この出力では`sort`オプションと`cut`オプションを指定した。`sort`オプションは因子負荷量の大きい順に並べ替えてくれるものであり，`cut`オプションは因子負荷量の表示を抑制するものである。あくまで表示上のオプションであり，実際は各因子から各項目へのパス($5 \times 25$本)が計算されている。

まず表示されているのが因子負荷行列であり，項目の因子ごとの負荷量に加え，共通性$h_j^2$と独自性$u_j^2=1-h_j^2$，複雑度complexityが示されている[^14.1]。なお，ここで表示されている因子負荷量などは回転後のパターン行列であり，斜交回転の場合は，因子軸の負荷量をどう考えるかによって因子パターンと因子構造とに分かれる。因子パターンは変数を斜交座標系に直交に投影した影のようなものであり，変数から因子への直接的な効果を表すと考えられる。因子構造は因子構造は変数を各因子軸に平行に投影した影のようなものであり，変数と因子の間の単純相関を表している。

[^14.1]: これは因子の複雑さを表す指標で、各項目(変数)がどれだけ単純に(あるいは複雑に)因子に負荷しているかを表す指標である。値が1に近い場合，その項目は基本的に1つの因子にのみ強く負荷することあらわしている。値が大きくなるほど、その項目が複数の因子に分散して負荷していることになる。この値は，項目$j$の因子$k$に対する負荷量を$a_{jk}$としたとき，$\frac{(\sum_k a_{jk}^2)^2}{\sum_k a_{jk}^4}$で算出する。

その下には負荷量の平方和SS loagingsがあり，これが説明する分散の大きさである。それを比率にしたもの(Proportion Var)，累積比率にしたもの(Cumulative Var)がある。今回は累積して44%の説明しかしていないことになるから，56%もの情報をカットしているので，情報圧縮の観点から言えば少し捨てすぎている危険性もある。

続いて，回転行列に斜交回転を指定しているから，因子間相関が出力されている。これを見ると絶対値最大で-0.36がみられる。全ての因子間相関が$\pm 0.3$に収まるようであれば，直交回転を考えても良い。

その後に出力されているのは適合度に関する指標である。各指標に関する解説は割愛する。

#### 因子得点の推定

ここまでで因子と項目の関係を探索的に求める方法について見てきたが，心理学的な研究としては因子と回答者の関係についても興味関心をもつだろう。すなわち，「外向性が高い人は誰か」「情緒不安定性が低い人はどういう特徴を持つか」といった，人に対する理解を深めることである。

数学的には，行列の固有値分解のときに固有値と同時にえらえる固有ベクトルが因子負荷量になる。この相関行列などを構成する時点で，すでに個人の相がもつ情報は要約されて欠落している。なので，因子の構造が明らかになってから，逆算的に個人の得点を考えることになる。因子分析のモデル式で見たように，因子得点は$f_{i.}$であるが，右辺の因子負荷量がすでに定まっているのなら，左辺も実測値から与えられているので，方程式を解くように答えを求めることができるのである。

`psych::fa`関数はデフォルトで因子得点を返すようになっており，以下のコードで確認できる。

```{r}
head(result.fa$scores, 10)
```

ここで一部`NA`が返されているところがある(例えばID 61630)，これは回答の中に欠測値が含まれていた場合におこる。

```{r}
head(dat, 10)
```

因子得点はモデル式から逆算的に推定するので，一箇所でも値が見つからなければ答えが出ないのである。
また，この推定法による因子得点は標準化されたスコアなので単位がなく，相対的に比較することしかできない。
加えて，推定された相対的なスコアであるから，以下の研究プロセスにおいて差の検定などをすることは不適切である，という考え方もある。

実践的には簡便的因子得点と呼ばれる因子得点の計算方法がある。これは因子分析の結果，当該因子に関係する項目に着目し，その評定値を平均することで算出するものである。
先の具体的にみていこう。第一因子はE2,E1,N4,E4,E5から構成されていると考えたとする。ここでE4,E5は因子負荷量が負であるから，評定値を逆転して考える必要がある。これを踏まえて，たとえば以下のように計算する。

```{r}
Fscore1.raw <- dat |>
  # 第一因子に該当しそうな項目だけ抜き出す
  select(E2, E1, N4, E4, E5) |>
  # 逆転項目の評定値を反転する
  mutate(
    E4 = 7 - E4,
    E5 = 7 - E5
  )

# 行ごとに欠損値を除いた平均値を計算する
Fscore1 <- apply(Fscore1.raw, 1, function(x) mean(x, na.rm = TRUE))
summary(Fscore1)
```

ここでは6件法の評定値を逆転させるために，7から引くという操作をして，欠測を除いて平均するようにしている。
こうすることで，尺度値の持つ意味(中点以上が賛成，未満が反対といったような)を踏まえて考えることができるし，全てが欠測でない限りスコアの算出ができるという利点がある。

ただしこの方法は，因子負荷量による項目ごとの重みづけを考えないこと，因子分析法によって除外したはずの誤差分散の情報を含んだスコアにしていること，といった短所をもつ。また，そもそも評定値が尺度構成法で正しくスコアリングされたものであるべきだが，実践的にそのような工夫をしている例はほとんど見られないため，非常に精度の低い，荒い推定値になっていると言わざるを得ない。

とはいえ，推定法でもとめたものと簡便法で求めたものは，非常に高く相関するので，心理学のデータがそれほどの精度を持つものでないと割り切れるのであれば，勘弁法でも十分だろう。

```{r}
cor(result.fa$scores[, 1], Fscore1, use = "pairwise")
plot(result.fa$scores[, 1], Fscore1)
```

### 確認的因子分析

ここまで探索的因子分析について詳しく解説してきた。
そこでは因子数や因子負荷量は事前の情報がなく，データによって語らせる方法で後付け的に解釈を行なっていくのであった。数値例にもあるように，第一因子は主にE因子の項目(外向性,Extraversion)から構成されているが，中にはN因子(情緒不安定性,Neuroticism)の項目も一部含まれており(N4)，解釈に頭を悩ませることも少なくない。そもそもBig5の名前にあるように理論的には5因子なのだが，データは6因子を示す，ということもある。

このように探索的因子分析はデータに沿った解釈をするしかないのだが，性格検査のような理論的背景や仮定があるのなら，そちらを重視したいということもあるだろう。
このような場合は，因子の構造や仮定を盛り込んだモデルをデータに当てはめるという，確認的因子分析(Confirmatory Factor Analysis, CFA)を用いる。これは構造方程式モデリング(Structural Equation Modeling)の枠組みで因子分析をとらえたものであり，項目と潜在変数の関係を方程式で表して推定する。

構造方程式モデリングは，分散共分散構造/相関構造の要素に潜在変数を含んだ方程式を当てはめた時の係数を推定するモデルである。方程式はパス図と呼ばれる表現方法で図示されることが多い。パス図では相関的関係を双方向の矢印で，回帰的関係を単方向の矢印で表現し，観測変数を矩形で，潜在変数を楕円で表す。パス図の表現を使うと，因子分析と主成分分析の違いは一目瞭然である。

![主成分分析(左)と因子分析(右)のパス図表現](images/14_fa_pca.png)

また，探索的因子分析と確認的因子分析の違いも明白である。

![EFA(左)とCFA(右)のパス図表現。簡略化するために誤差因子は描画しない。](images/14_efa_cfa.png)

確認的因子分析では，どの因子がどの項目に影響しているかを個別に指定している。言い換えるなら，影響がないと仮定するパスの係数を$0$に固定しているともいえる。この図では確認的因子分析モデルの因子間相関が$0$であると仮定しているため，パスを引いていない(引くことももちろん可能である)。

この方法ではモデルの方が先にあり，このモデルから考えられる分散共分散行列の式を実際の分さ共分散行列に当てはめることになる。当てはめる，すなわち係数を推定する方法は，大きく分けて最小二乗法，最尤法，ベイズ法であるが，実際は各種推定法のアルゴリズム名まで把握しておくといいだろう。さらに推定後，モデルと実際の分散共分散行列がどれほど一致しているか，即ち**適合度**Model Fit Indicesをみてその評価を行うことになる。適合度指標も複数あるため，それらを見ながら総合的に評価することになる。

Rでの具体例を見ておこう。パッケージ`lavvaan`を用いて[^14.3]以下のようにモデルを与える[^14.4]。

[^14.3]: らばーん，とは変な名前だと思われるかもしれないが，LAtent VAriable ANalysisすなわち潜在変数分析の意味である。

[^14.4]: `lavaangui`というパッケージを用いれば，モデルの指摘もGUIでできる。

```{r}
library(lavaan)
# モデル指定
model <- "
Neuroticism =~ N1 + N2 + N3 + N4 + N5
Agreeableness =~ A1 + A2 + A3 + A4 + A5
Extraversion =~ E1 + E2 + E3 + E4 + E5
Openness =~ O1 + O2 + O3 + O4 + O5
Conscientiousness =~ C1 + C2 + C3 + C4 + C5
"
```

ここで`model`オブジェクトは文字列として入力されていることに注意しよう。シングル，あるいはダブルクォーテーションでモデル記述を囲むのである。また，潜在変数名を左辺に置き，それを構成する観測変数を右辺に置く**測定方程式**は，`=~`という演算子で繋ぐ。変数同士の相関的関係は`~~`という演算子を，回帰的関係は`~`という演算子を用いる。特に潜在変数同士の関係を記述する方程式は**構造方程式**と呼ばれる。

ここでは因子間相関に関する記述はないが，デフォルトで$0$と指定しないところには相関のパスが仮定される。係数をゼロに指定したい場合は，`
Neuroticism ~~ 0 * Openness`のように記述するといいだろう。

さて，モデルの指定が終われば，データと推定法を指定して推定させよう。
ここでは推定法(estimator)オプションを最尤法(ML)とした。また，要約を出力するときに，適合度指標(fit.meassures)と標準化係数(standardized)も表示するよう指定してある。
```{r}
# モデル推定
model.fit <- sem(model, estimator = "ML", data = dat)
summary(model.fit, fit.measures = TRUE, standardized = TRUE)
```

出力として，まずモデルの要約(推定法など)が示され，続いて適合度指標(CFI, TFI, AIC, BIC, RMSEA, SRMR)などが表示される。これらの適合度指標は大きく3つのカテゴリーに分類できる：

まずは，飽和モデルとヌルモデルの間の比較指標である。CFIやTFIがこれに該当する。ヌルモデルを0，飽和モデルを1としたとき，今回のモデルがどこに位置づくかを示す。ここで飽和モデルとはデータに完全に適合するモデルであり，すべての観測変数間にすべての可能なパスが引かれたものを指す。逆にヌルモデル(独立モデル)は観測変数間に関連性がまったくないと仮定する最も制約の強いモデルである。


次に，尤度に基づく相対指標である。AIC，BIC，SABICがこれに該当する。尤度はデータが確率モデルにどれほど近いかを表したものであるから，データやモデルが異なれば比較はできない。当該データに対する相対比較として用いる。AIC(赤池情報量規準，Akaike Information Criterion)は対数尤度とパラメータの数で計算されており，対数尤度が大きくパラメータ数が少ない方が良いモデルだと考える規準である。-2LL + 2p(LLは対数尤度，pはパラメータ数)で算出され，小さければ小さいほど当てはまりが良いと判断する。BIC(ベイジアン情報量規準)は，AICよりもサンプルサイズに対して強いペナルティを与えている。SABICはサンプルサイズを調整したBICの亜形である。

最後に，実データの分散説明量の残量に関する指標がある。RMSEAとSRMRがこれに該当する。RMSEA(Root Mean Square Error of Approximation)はモデルの近似誤差を評価しており，0.05未満が良好とされる。SRMR(Standardized Root Mean Square Residual)は実データと予測データの残差の標準化された平方根で， 0.08未満が良好とされる。

モデル評価の際は、これらの指標を総合的に検討することが推奨される。単一の指標だけではなく、複数の指標を参照し、それらが一貫して良好な適合を示すかを確認することが重要である。

続く出力で，推定値Estimator や検定統計量が表示される。心理学の場合は，全ての変数を標準化した推定値`Std.all`を参照することが多い[^14.5]。ここでパス係数が小さかったり，統計的に有意にならないものを削除することでモデルの適合度は上げることができる。
ただし，適合度を上げることが研究の目的になってはならない。仮定を当てはめるのだから，適当とされる目安を達成できない場合は仮定を省みて改良することはあるだろうが，適合度を上げるために仮定に合わないパスを引くようなことは，「頑張って有意にする」のと同じQRPsである。

[^14.5]: `Std.all`は観測変数も潜在変数もその分散を1に標準化したもの。`Std.lv`は潜在変数だけその分散を1に標準化したものである。

ところで，パッケージを使えば，パス図も自動で描いてくれる。他にも，`lavaanExtra`，`tidySEM`，`lavaanPlot`,`lavaanPlot2`など開発中のものも含めて様々なものがあるが，ここでは古典的な`semPlot`パッケージによる出力例を示す。
```{r}
library(semPlot)
semPaths(model.fit, what = "stand", style = "lisrel")
```

以上が因子分析法の概略である。

因子分析法は測定に関するモデルであり，心理尺度を作成する場合は非常によく用いられるものである。しかしあくまでもデータや項目間の相関関係から共通次元を見出すものであるから，構成概念を直接測定したとか，構成概念の存在が証明されたかのような利用・解釈は適切ではない。例えば，何らかの話題についての文言，極端な話「ラーメンに関する記述」を用意して，そこに量的な評価を加えれば，ラーメン因子だろうが豚骨因子だろうが，何らかの解釈ができる因子を抽出することはできる。そのことと，人が心理的に豚骨因子を内在化しているということにはならない。

また構造方程式モデリングによって潜在変数間に回帰や相関のパスを仮定することはできるが，そのことが実際どのような形で顕現化するかについて考えておく必要がある。モデルが非常に適合していて，潜在変数間に強い影響関係があったとしても，測定方程式のパス係数が低かったりすると，結局一方の因子得点が1単位増えたことで，従属する潜在変数がどのように変化し，それがどのように行動・測定値に反映されるかを意識しよう。そのような実態的な影響がない，つまり妥当性のない統計モデルは机上の空論に過ぎないからである。

測定とその実際的影響については，因子分析モデルの一種とも言える項目反応理論と問題意識を軌を一にする。

## 項目反応理論

つづいて項目反応理論(Item Response Theory, IRT)を取り上げる。項目反応理論は古典的テスト理論(Classical Test Theory, CTT)との対比で，現代テスト理論と呼ばれることもある。テスト理論を背景に持つものであるから，従属変数としてバイナリ変数を前提としている(0が誤答，1が正答を表す)。これは言い換えれば因子分析において従属変数がバイナリであるものといってもよく，実際にカテゴリカル因子分析との数学的等価性が明らかになっている。

もう一つ因子分析と異なる側面としては，因子分析が性格心理学を背景に因子構造を探索することに重点が置かれていることに対して，項目反応理論は因子得点をより精緻にすることに重点が置かれている。また性格心理学の場合は何因子構造であるかということがすでに学術的な問いであるが，項目反応理論を一とするテスト理論においては「学力」の一因子構造であることが望ましいとされる。このことから，因子分析を使った心理尺度の構成は「単純構造」が良いものであると考え項目を洗練(取捨選択)することが多いのに対し，項目反応理論は第一因子の負荷量が十分大きければ(一般に30%程度の分散説明率があれば良い)一因子構造であると考えるし，いかなる項目であっても何らかの情報をもたらすものと考えて項目をプールする(捨てない)という方針で進められることが多い点である。

項目反応理論の各種モデルを用いた，コンピュータ適応型テスト(Computer Adaptive Test, CAT)が現在のテスト理論の主流である。これは受験者の回答パターンに応じて項目プールから動的に次々問題を提供し，効率よく受験者の能力を推定していくものである。CATに必要なのはIRTを背景にしたモデルはもちろんのこと，各能力水準を測定するのに適した項目プールであり，また項目プールというデータベースとの連携システムである[^14.6]。

[^14.6]: 実際，日本最大級の学力テストである大学入学共通試験においてもCATの導入が検討されたが，膨大な項目プールの必要性(予備調査)や，地方や離島などの遠隔地などでも都市部と同じ通信環境，実行環境の準備などを考えると現実的でないということから見送られている現状がある。現行の紙とペン(マークシート)を用いた受験システムは，毎年50万人程度が同時に受けても一桁パーセント以下の誤謬率しかないという驚異の精度で運用されている極めて優れた実践システムであり，その社会的インパクトの大きさから考えても，CATの導入は慎重にならざるを得ない。

### ロジスティックモデル

IRTはバイナリデータに対する単因子モデルである。バイナリデータであるから，連続値を前提とするピアソンの積率相関係数ではなく，テトラコリック相関係数を用いてその次元性を解析する。また因子に該当する被験者母数$\theta$によって項目反応が回帰されるモデルでもあるから，ロジスティック回帰分析のようなモデル化をすることになる。

被験者母数は標準正規分布が仮定されるが，これを累積正規分布の形で表現するとロジスティックカーブがよくあてまるし，項目の特徴を表現するための項目母数を線形モデルの中に組み込むためには，ロジスティックモデルで表現する方がわかりやすいという側面もある[^14.7]。以下に標準正規分布と累積正規分布，並びにロジスティック関数による正規累積分布の近似を示す。なお，ロジスティックモデルで扱われる関数は，以下のように係数(1.702)を用いると，よりよく近似することが知られている。

$$ f(x) = \frac{1}{1 + exp(-1.702x)} $$


[^14.7]: 正規累積関数を使ってモデル化・表現することももちろん可能である。

```{r logistic_curve}
#| message: FALSE
#| dev: "ragg_png"
library(ggplot2)
library(patchwork)

# データの準備
x <- seq(-4, 4, length.out = 1000)
normal_df <- data.frame(
  x = x,
  density = dnorm(x),
  cdf = pnorm(x),
  logistic = 1 / (1 + exp(-1.702 * x))
)

# 1. 標準正規分布
p1 <- ggplot(normal_df, aes(x = x, y = density)) +
  geom_line() +
  labs(title = "標準正規分布",
       x = "x",
       y = "確率密度") +
  theme_minimal()

# 2. 累積正規分布
p2 <- ggplot(normal_df, aes(x = x, y = cdf)) +
  geom_line() +
  labs(title = "累積正規分布",
       x = "x",
       y = "累積確率") +
  theme_minimal()

# 3. ロジスティック曲線
p3 <- ggplot(normal_df, aes(x = x, y = logistic)) +
  geom_line() +
  labs(title = "ロジスティック曲線による近似",
       x = "x",
       y = "確率") +
  theme_minimal()

# 3つのプロットを横に並べて表示
p1 + p2 + p3
```


このロジスティック関数を用いて，項目母数を使って項目の特徴を描画することを考えよう。IRTのロジスティックモデルには，パラメータが1つのもの，2つのもの...とさまざまなものが考えられているが，パラメータ数が多いモデルはパラメータ数が少ないモデルに含まれる(特殊形)である。

#### 1PLモデル

まずは1パラメータロジスティックモデル(1PLモデル)を考えよう。このモデルは，項目母数$b$を用いて，以下のように表現される。

$$ P(Y_{ij} = 1 | \theta_i, b_j) = \frac{1}{1 + exp(-1.702(\theta_i - b_j))} $$

ここで，$Y_{ij}$は被験者$i$が項目$j$に正答したかどうかを表すバイナリ変数であり，$\theta_i$は被験者$i$の能力を表す被験者母数である。また，$b_j$は項目$j$の**困難度(difficulty)**を表す項目母数である。というのも，この$b_j$が大きくなるとロジスティック曲線は右に寄る，また小さくなると左に寄るからである。横軸は被験者母数$\theta$であり，縦軸は通過率であるから，曲線が右にシフトすることはより能力が高くなければ通過率が上昇しないことを表すからである。

```{r 1PLmodel}
#| dev: "ragg_png"
logistic_1pl <- function(theta, b) {
  1 / (1 + exp(-1.702 * (theta - b)))
}

x <- seq(-4, 4, length.out = 1000)
normal_df <- data.frame(
  x = x,
  default = logistic_1pl(x, 0), #deafult
  easy = logistic_1pl(x, -1),
  hard = logistic_1pl(x, 1)
)

ggplot(normal_df) +
  geom_line(aes(x = x, y = default, color = "デフォルト(b=0)")) +
  geom_line(aes(x = x, y = easy, color = "易しい(b=-1)")) +
  geom_line(aes(x = x, y = hard, color = "難しい(b=1)")) +
  scale_color_brewer(palette = "Set2") + 
  labs(title = "1pl logistic model",
       x = "theta",
       y = "通過率",
       color = "難易度") +  # 凡例のタイトルを追加
  theme_minimal() +
  theme(legend.position = "bottom")  # 凡例を下部に配置
```

#### 2PLモデル

2パラメータロジスティックモデル(2PLモデル)は，1PLモデルに加えて項目母数$a$を含める。この母数は**識別力**と呼ばれる。

$$ P(Y_{ij}=1|\theta_i,a_j,b_j) = \frac{1}{1+exp(-1.702a_j(\theta_i-b_j))} $$

これが識別力と呼ばれるのは，ロジスティック曲線の傾きを変えるからである。傾きが強くなって急激に上昇することは，ある$\theta$の値で急に正誤の確率が変わることを意味し，逆に傾きが緩くなることは特定の$\theta$の値でも正誤の違いが大きくないことを意味するからである。ちなみにカテゴリカル因子分析の文脈では，困難度$b_j$が閾値に，識別力$a_j$が因子負荷量に相当する。

```{r 2PLmodel}
#| dev: "ragg_png"
logistic_2pl <- function(theta, a, b) {
  1 / (1 + exp(-1.702 * a * (theta - b)))
}

x <- seq(-4, 4, length.out = 1000)
normal_df <- data.frame(
  x = x,
  default = logistic_2pl(x, 1, 0), #deafult
  easy = logistic_2pl(x,1.5, -1),
  hard = logistic_2pl(x,0.5, 1)
)

ggplot(normal_df) +
  geom_line(aes(x = x, y = default, color = "デフォルト(b=0,a=1)")) +
  geom_line(aes(x = x, y = easy, color = "b=-1, a=1.5")) +
  geom_line(aes(x = x, y = hard, color = "b=1, a=0.5")) +
  scale_color_brewer(palette = "Set2") + 
  labs(title = "1pl logistic model",
       x = "theta",
       y = "通過率",
       color = "モデルと設定") +  # 凡例のタイトルを追加
  theme_minimal() +
  theme(legend.position = "bottom")  # 凡例を下部に配置
```

#### 3PLモデル

実践的には2PLモデルが最もよく用いられるが，理論的には3，4，5PLモデルまで提案されており，
それぞれ次のように表現される。

$$ P(Y_{ij}=1|\theta_i,a_j,b_j,c_j) = c_j + \frac{1-c_j}{1+exp(-1.702a_j(\theta_i-b_j))} $$

$$ P(Y_{ij}=1|\theta_i,a_j,b_j,c_j,d_j) = c_j\frac{d_j-c_j}{1+exp(-1.702a_j(\theta_i-b_j))} $$


$$ P(Y_{ij}=1|\theta_i,a_j,b_j,c_j,d_j,e_j) = c_j + \frac{d_j-c_j}{\{1+exp(-1.702a_j(\theta_i-b_j))\}^{e_j}} $$


ここで$c_j$は下方漸近線母数，$d_j$は上方漸近線母数，$e_j$は非対称性母数と呼ばれている。このように，モデルとしては徐々にパラメータ数を増やして表現しているが，推定すべきパラメータの数が増えるとより大きい標本サイズ(受験者数)が必要となるし，等価など運用シーンでも複雑になることから，あまり用いられるものではない。

#### ロジスティックモデルと分析の実際

項目の特徴を表現するロジスティック曲線は，**項目反応関数**(Item Responose Function)あるいは**項目特性曲線**(Item Characteristic Curve)と呼ばれる。`ltm`パッケージや`exametrika` パッケージなど，IRTモデルを実行するためのRパッケージは複数あり，これを使って実践例を見てみよう。

ここでは著者が開発した`exametrika`パッケージとそのサンプルデータを用いて，実際にIRTを実行してみよう。`exametrika`パッケージにはサンプルデータが複数含まれている。今回用いるJ15S500は，500人の被験者が15問の項目に回答したサンプルデータである。

```{r}
#| message: FALSE
## なければインストールしておく
# install.packages("exametrika")
library(exametrika)
result.2pl <- IRT(J15S500, model = 2, verbose = FALSE)
print(result.2pl)
```

数値的な特徴としては，`Item Parameters`のところに`slope`(識別力)，`location`(困難度)が示されており，またそれぞれの標準誤差が示されている。
続く`Item Fit Indices`は項目ごとの適合度，`Model Fit Indices`はテスト全体のモデル適合度であるが，IRTモデルはSEMの適合度の観点から言えば非常に当てはまりは悪い。これはバイナリデータに対するモデリングであることなどを考えると，ある程度は仕方がないことであるとも言える。

IRTの良さは，こうした数値的特徴というよりも，項目分析の時におけるIRTの可視化のしやすさにあると言えるだろう。`exametrika`パッケージでは，`plot`関数を用いて項目特性曲線を描画することができる。

```{r}
plot(result.2pl, item = 1:5, type = "IRF", overlay = TRUE)
```

また，IRF関数をテストの全項目に対して加算したテスト反応関数(Test Response Function)を描画することもできる。

```{r}
plot(result.2pl, type = "TRF")
```

さらに項目反応関数を変換した**項目情報関数**(Item Information Function)を描画することもできる。項目情報関数は，その項目において最も分散が大きくなるところ，すなわち$\theta =0.5$をピークにする関数であり，以下のように定義される。
$$ I_j(\theta) = \frac{P_j^{\prime}(\theta)^2}{P_j(\theta)(1-P_j(\theta))} $$

要するに，正答と誤答の確率の差が大きいほど，その項目の情報量が大きくなるということである。
この関数を`exametrika`パッケージでプロットするには次のように`type = "IIF"`と指定する。

```{r}
plot(result.2pl, item = 4, type = "IRF")
plot(result.2pl, item = 4, type = "IIF")
```

IIFが示すのは，項目反応理論における信頼性の概念であるとも言える。すなわち，IRTにおいては信頼性が$\theta$の関数として表現され，どの領域匂いってその項目が最も効率的に機能するかを評価するのである。

確認しておくと，古典的テスト理論においては，テストの全体に対する真分散の割合で信頼性をとらえていたのであった。また因子分析においては項目における共通性$h_j^2$で信頼性をとらえていた。つまりテスト全体から各項目へと進んで行ったのだが，現代テスト理論においては関数・項目の機能性を評価するようにと発展してきたのである。

すでに述べたように，IRTの観点からは，難易度が高すぎる・低すぎる項目であっても，削除するようなことはない。そうした項目は，高い能力・低い能力を査定する時に必要なのである。実戦に際してそうした項目は大きな分散を持ち得ないから，共通性も低くなりがちであるが，だからと言ってそうした項目を削除するようなことはしない。この辺りに，テスト理論と因子分析との思想的な違いがあると言える。

テスト全体の情報関数は，テストに含まれる項目情報関数の総和で表現される。この関数を`exametrika`パッケージでプロットするには次のように`type = "TIF"`と指定する。

```{r}
plot(result.2pl, type = "TIF")
```

これを見ると，この15項目からなるテストは全体として$\theta=-1$のあたりをピークとしており，相対的にやや$\theta$が低い受験者に対して精緻な情報を提供するようになっていることがわかる。事前に項目母数がわかっている多くの項目プールがあり，それらを組み合わせてテストを作成する場合には，このような情報関数を用いて事前にテストの精度をデザインして適用することができる。

被験者母数$\theta$の推定については，受験者の回答パターンから推定される。一般に標準正規分布を事前分布としたベイズ推定が用いられる[^14.7]。`exametrika`パッケージでは，分析と同時に被験者母数の推定も行われている。

```{r theta estimation}
head(result.2pl$ability)
```


[^14.7]: 最尤推定にすると，全問正答あるいは全問誤答の場合には，その項目の母数が無限大になってしまうから，実用的でないからである。

### IRTモデルの展開

IRTモデルは基本的にバイナリデータに対するモデルであるが，多段階反応，多値反応に対するモデルも提案されている。たとえばリッカート尺度のような多段階反応に対しては，**段階反応モデル**(Graded Response Model)や**部分採点モデル**(Partial Credit Model)が提案されている。これらを用いることの利点は，心理尺度データに対して順序尺度水準を仮定できるところにある。

心理学では基本的に，段階評定はせいぜい順序尺度水準の精度しか持ち得ないと考えられていながら，その数学的な利便性から(あるいはそこまでの精度がないとそもそも信用されていなかったから)，間隔尺度水準とみなしてピアソンの積率相関係数を算出し，一般的な因子分析を行ってきた。こうした「みなし」が必要だった理由のひとつが，統計パッケージにGRMやPCMが実装されていなかったからである。昨今では，GRMと2PLモデルとの数学的等価性から同じ潜在変数モデルとして推定する統計ソフトウェア(Mplusなど)もあるし，Rには`ltm`パッケージなどGRM，PCMを提供するものもある。つまり，ツールがないからという言い訳はもう通用しない時代である。

多段階モデルで分析できるさらなる利点は，適切な反応段階を考えられる点である。リッカート法といえば5件法，7件法であるというのが一般的に考えられているが，このことに特段の理論的根拠はない。それよりも，回答者が5，7段階のカテゴリ反応をしっかりと弁別できるのかどうかを考えるべきである[^14.8]。

[^14.8]: よく受ける質問に，「先行研究が5段階であったら，5段階でとらなければならないか」とか，「5件法の尺度と7件法の尺度を混ぜて使ってもいいか」というものがある。これらに対する正しい回答は，先行研究のスコアリング・標準化手続きを利用するのであれば先行研究に従わなければならないが，そうでないならば(探索的因子分析など，自らのデータで因子負荷量を決め，スコアリングを行うのであれば)，回答者の反応しやすい段階にデザインするべきであり，回答者の反応しやすさが不明なのであれば多段階項目反応理論を用いて各カテゴリに対する反応も検証するべきである，となる。

具体例で見てみよう。`ltm`パッケージの`grm`関数を用いて，サンプルデータ`Science`を分析してみる。このデータは科学態度に対するデータであり，4段階評定になっている。

```{r grm}
#| message: FALSE
## なければインストールしておく
# install.packages("ltm")
library(ltm)
data(Science)
result.grm <- grm(Science)
print(result.grm)
```

結果で示されているのは3つの閾値(Extrmt)と，それぞれの閾値に対する識別力(Dcrmn)である。
段階反応モデルも，ロジスティックモデル同様IRF，IIF，TIFを描画できるから，IRF(`ltm`パッケージでは`ICC`という引数を用いる)を描いてみよう。

```{r grm plot1}
plot(result.grm, items = 2, type = "ICC")
```

この図には各カテゴリに対する反応確率が，$\theta$の関数として表示されている。これを見ると，科学的態度の$\theta$が高くなるにつれて，回答する確率が最も高いカテゴリが1から2，2から3へと変わっていくことが見て取れる。

しかし次の項目はどうだろうか。
```{r grm plot2}
plot(result.grm, items = 1, type = "ICC")
```

これを見ると，反応カテゴリ1，2のピークが存在せず，ほとんど3の反応で被覆されており，$\theta=3$を超えたところでやっと反応カテゴリ4がでてくることになる。プロットは$-4 \le \theta \le +4$の範囲であるから，この幅を負の方向に広げればピークが出てくるのかもしれないが，実際的ではない。データによってはカテゴリ反応のピークが出てこないものもあり，そうしたものは適切な反応段階の設計になっていないことが疑われる。

昨今では心理尺度の設計に対しても，IRTのアプローチを取ることが推奨されている。因子分析アプローチとのもう一つの違いである，単因子を仮定する点についても拡張され，多次元IRTモデルも提案されている(`mirt`パッケージなどが提供されている)。もはや，IRTのアプローチを取らない理由がないのである。

## まとめ

ここでは探索的因子分析，検証的因子分析(構造方程式モデリング)，項目反応理論のそれぞれについて，その基本的な考え方と実践的な使い方を紹介した。

これらに共通する考え方は，分散共分散行列あるいは相関行列をもとに，潜在変数を仮定した測定モデルを構築するという点である。相関行列が順序尺度水準に対するテトラコリック/ポリコリック/ポリシリアル相関係数であれば，カテゴリカルな因子分析をしていることになるし，それは(多段階)項目反応理論をしていることでもある。つまり尺度水準に対応した相関係数が算出できるアプリケーションであれば，モデルの適用は同じ手順で行うことができる(`lavaan`でも変数の尺度水準の設定ができる。そのほかのアプリケーションとしてはMplusが有名である)。

それぞれのモデルの持つ歴史的背景や理論的系譜を知ることは，モデルの適用に有益な知見をもたらすが，ユーザ視点でいえば使えるものはなんでも使うべきであり，とくに調査協力者(=回答者，受験生)の回答のしやすさといった観点から調査研究を設計することが肝要であろう。数学的限界やソフトウェアの都合によって，ましてや研究者の無理解や怠慢によって，回答しにくい調査デザインを適用することは，調査研究の質を低下させることになることを忘れてはならない。

## 課題

このチャプターで学んだ多変量解析の手法について、以下の課題に取り組んでください。各課題は、実践的なデータ分析を通じて、理論と実践の両面から理解を深めることを目的としています。
例として，`psych`パッケージの`small.msq`データセット(気分状態質問紙)を使用します。このデータセットは14の変数，200ケースです。[^14.8]

14の変数は，エネルギー的覚醒状態変数(活動的`active`, 注意深さ`alert`，覚醒`arousal`，ねむさ`sleepy`，疲れ`tired`，ウトウト`drowsy`)，緊張的覚醒変数(不安`anxious`，落ち着かない`jittery`，神経質な`nervous`，穏やかな`calm`，リラックスした`relaxed`，気楽な`at.ease`)と，性別`gender`，薬物条件`drug` からなります。性別と薬物条件を除いた2因子構造であると考えられ，

[^14.8]: これは元々2500以上のサンプルがあるMotivational State Questionnaire (MSQ)の一部。全体は`psychTools`パッケージに入っている。

```{r}
#| include: FALSE
#| echo: FALSE
#| eval: FALSE
# 準備
library(psych)
library(tidyverse)
data(small.msq)
summary(small.msq)

dat <- small.msq %>% select(-gender,-drug)
```


### 課題1：探索的因子分析の実践

探索的因子分析を実施してください。

分析の手順：

1. 因子数の決定
2. 因子分析の実行(斜交回転)
3. 結果の解釈（因子負荷量、共通性、独自性）

```{r EFA practice}
#| include: FALSE
#| echo: FALSE
#| eval: FALSE
# EFAの例
fa.parallel(dat, cor="poly")
result <- fa(dat, cor = "poly", nfactors = 2, rotate='geominQ')
print(result, sort= T, cut = 0.3)
```

### 課題2：確認的因子分析の実践

`lavaan`パッケージを使って，2因子モデルを推定してください。
モデル指定の際に，`ordered = TRUE` オプションを入れることで，観測変数を順序尺度水準として推定します。

分析の手順：

1. 理論的モデルの構築(パス図の作成)
2. `lavaan`パッケージを用いたモデルの推定
3. モデルのプロット
4. 適合度指標の評価
5. モデルの修正(必要な場合)

```{r cfa practice}
#| include: FALSE
#| echo: FALSE
#| eval: FALSE
# CFAの例
library(lavaan)
model <- '
f1 =~ active + alert + aroused + sleepy + tired + drowsy
f2 =~ anxious + jittery + nervous + calm + relaxed + at.ease
'
result.sem <- sem(model, data = dat,
              ordered = TRUE)
summary(result.sem, fit.measures = TRUE, standardized = TRUE)

library(lavaanPlot)
lavaanPlot(result.sem)
```

### 課題3：多段階項目反応理論の実践

`ltm`パッケージの段階反応モデル(GRM)を適用してください。一次元性を仮定したモデルなので，エネルギー的覚醒項目セット，緊張的覚醒項目セットに分け，それぞれにGRMモデルを適用します。


分析の手順：

1. データの分割
2. GRMの適用
3. 項目特性曲線の描画
4. 項目情報関数の描画

```{r grm practice}
#| include: FALSE
#| echo: FALSE
#| eval: FALSE
# GRMの例
library(ltm)
EA <- dat %>% dplyr::select(active, alert, aroused, sleepy, tired, drowsy)
TA <- dat %>% dplyr::select(anxious, jittery, nervous, calm, relaxed, at.ease)

result.EA <- grm(EA)
plot(result.EA, type = "ICC")
result.TA <- grm(TA)
plot(result.TA, type = "ICC")
```

### 課題4: 多次元多段階項目反応理論の実践

`mirt`パッケージ(Multidimensional IRT)を用いて，多次元モデルを実行します。コードは次のようになります。

```{r mirt example}
#| eval: false
library(mirt)
# 2因子(model = 2)，段階反応(itemtype = 'graded')を指定
result.mirt <- mirt(dat, model = 2, itemtype = "graded")
# 出力に際して斜交回転
summary(result.mirt, rotate = "geominQ")

# 多次元ICCの描画
plot(result.mirt, type = 'trace', which.items = 1)
# 多次元IICの描画
plot(result.mirt, type = 'info')
```

