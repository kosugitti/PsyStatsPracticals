# 多変量解析(その2)

先の章では分散共分散行列(相関行列)に基づいた線形モデルを中心に紹介した。
しかし，多変量解析はそれだけではない。
むしろ測定モデルとして潜在変数を仮定する手法だけに固執するあまり，心理測定の仮定に違反するようなデータであっても因子分析を適用したり，モデルの適合度を優先しすぎて不自然な設定に走ったりするような誤用が多く見られる。

心理学においては因子分析が構成概念を測定していると「純粋に」信じられて多用されてきたが，かつてはさまざまな多変量解析技法が必要に応じて開発，使用されていたのである。
ここでは分析のスタートになる行列の種類で区分し，いくつかの多変量解析モデルを紹介する。

## 距離行列を用いるもの
距離とは，次の四つの公理を満たす数字のことをいう。

1. **非負性**: $d(x,y) \geq 0$ (距離は0以上)
2. **同一性**: $d(x,y) = 0 \Leftrightarrow x = y$ (距離が0なのは同じ点の場合のみ)
3. **対称性**: $d(x,y) = d(y,x)$ (距離は方向に依存しない)
4. **三角不等式**: $d(x,z) \leq d(x,y) + d(y,z)$ (遠回りは最短距離以上)

距離行列とは行列の要素が距離を表しているもので，一般に正方・対称行列になる。この点は分散共分散行列や相関行列と同じで，この行列演算によって分散共分散を用いたモデルとは別の解釈が成立する分析を作ることができる。

ちなみにRでは距離行列を作るのに`dist`関数を用いる。オプションとして特段の指定がなければユークリッド距離が用いられるが，他にも以下のようなオプションがある。

- `"euclidean"`: ユークリッド距離。$d(x,y) = \sqrt{\sum_{i=1}^N (x_i-y_i)^2}$で表される。
- `"maximum"`: チェビシェフ距離。$d(x,y) = max(|x_i - y_i|)$で表される。
- `"manhattan"`: マンハッタン距離。$d(x,y)=\sum (|x_i - y_i|)$で表される。
- `"canberra"`: キャンベラ距離。$d(x,y) = \sum \frac{|x_i-y_i|}{|x_i+y_i|}$で表される
- `"binary"`: バイナリ距離。ジャッカード距離ともいう。0/1のデータに対する距離で，$d(x,y) = \frac{b+c}{a+b+c+d}$で表される(aは両方1，bはxが1でyが0，cはxが0でyが1，dは両方0)。両方に共通して1である要素が多いほど距離が小さくなる。
- `"minkowski"`: ミンコフスキー距離。一般化された距離とも言われ，係数$p$でさまざまな距離を表現できる。$d(x,y)=\left(\sum |x_i-y_i|^p\right)^{\frac{1}{p}}$で表される。例えば$p=1$ならばマンハッタン距離，$p=2$ならユークリッド距離である。$p=\infty$の時を特にチェビシェフの距離，または優勢次元距離という。

### 心理学における距離データ

数字を何とみなすか，によって心理学でも距離データを扱うことはできる。
尺度評定の差分を(得点間の)距離とみなすこともできるし，相関係数も$1.0-|r_{jk}|$のようにすれば距離とみなすことができる。
社会心理学におけるソシオメトリックデータは，対人関係の選好評定だが，これも対人間の距離とみなすことができるだろう。
実験心理学における刺激の混同率や汎化勾配，2つの茂樹が同じか違うかを判断する課題への反応潜時，刺激の代替価・連想価は類似性と考えられるから，これも距離データと言えるだろう[@Takane198002]。
距離行列は一個体からの評定や反応からでも生成できるから，小サンプルの実験計画であっても距離行列を得ることができる。

このように，類似性あるいは非類似性を距離と見做して用いることができる。
この利点は，回答者の自然な判断に任せられること，つまり「総合的に判断して，似ているか，似ていないか」といった回答をデータにできることである。研究者はついつい複数の類似した項目で多角的に聞かねばならない，と思いがちだが，下位の評定次元を実験者が準備することは回答者の自由度を束縛している側面もあり，また回答者の負担を考えると必ずしもいいことばかりではない。さらに項目によっては社会的な望ましさバイアスなども含まれるから，「総合的に評価してもらいたい」というのはそういったバイアスから逃れられる側面もある。

距離を対象に考える多変量解析モデルも，そのほかのモデルと同様，要約や分類を目的にしている。また，多次元データを少数の次元に要約するために可視化する手法として用いられることもある。まずは分類を目的としたモデルから見ていこう。

### クラスター分析

クラスター分析は類似したものをまとめてクラスター(塊)を形成する分析方法である。
クラスター分析の中でも多くの手法・モデルが考えられており，いくつかの側面から分類することができる。

#### モデルの階層性

##### 階層的クラスター分析

まずはクラスターが階層性を持つかどうか。階層的クラスター分析は距離の短いものから順にまとめていき，クラスターのクラスター，クラスターのクラスターのクラスター，といったように順次大きなグループにまとめ上げていく。

結果は*デンドログラム*と呼ばれるツリー状のプロットで表されることが一般的で，適当なところで分割して利用する。適切なクラスター数に関する一般的な基準はほとんどなく，実用性に応じてツリーをカットすることが多い。

`iris`データによる実行例を示す。

```{r hierarical clustering}
data(iris)
d_matrix <- dist(iris[,-5], method = 'euclidean')
result.h <- hclust(d_matrix, method = 'ward.D2')
plot(result.h, main = "Hierarchical Clustering Dendrogram")
```
階層的クラスター分析のクラスター同士を上位クラスターにまとめていく方法がいくつか考案されており，Rでは`hclust`関数の`method`オプションで指定することができる。

- `"ward.D"` / `"ward.D2"`: ウォード法。クラスター内の分散を最小化する
- `"single"`: 最短距離法。クラスター間の最短距離でリンク
- `"complete"`: 最長距離法。クラスター間の最長距離でリンク
- `"average"`: 平均法(群平均法)。クラスター間の平均距離でリンク
- `"mcquitty"`: McQuitty法。重み付き群平均法の一種
- `"median"`: メディアン法。重み付き群中心法の一種
- `"centroid"`: 重心法。

最もよく使われるのがウォード法で，実践的にもこの手法による分類が最も解釈しやすい。なお，`ward.D`オプションはバグがあるので用いないことが望ましく，バグを修正した`ward.D2`を用いること[^15.1]。

^[15.1]: バグがあるのになぜ修正しないのか，と思われるかもしれない。`ward.D`はRのバージョン3.0.3以前まで用いられていたが，ユーザからの指摘で正しく分散が計算されていないことが発覚。Rの基本関数に誤りがあったことを認め，戒めとするために元のコードを残している。Rはフリーでオープンなソフトウェアであり，無償ではあるが，フリーだからといっていけないのではなく，こうした自浄作用があることを示すための措置と思われる。ちなみに筆者が以前プロプライエタリな統計ソフトを使っていた時にもおかしな挙動を発見したことがある。メーカに指摘すると「次のバージョンでは修正されているのでそちらを新たに購入しろ」という回答であったため，そのソフトウェアから決別することにした。科学的営みにおいて，有償サポートがあることが真実を担保しない例である。

得られたクラスターの結果は`cutree`関数で任意のクラスター数に分割できる。今回，`iris`データは3種類のirisがあることがわかっているので，クラスター数3にしてその分類精度を確認してみよう。

```{r cut tree}
clusters <- cutree(result.h, k = 3)
table(clusters, iris$Species)
```

##### 非階層的クラスター分析

非階層的クラスター分析として有名なのは，`k-means`法による分類である。アルゴリズムは次のとおりである。

1. 指定されたクラスター数の重心ベクトルをランダムに生成する。
2. 各データ点を最も近い重心のクラスターに所属させる。
3. クラスターごとに，重心を再計算する。
4. 2.-3. のステップを繰り返し，変動がなくなると推定終了とする。

この方法は任意のクラス数に分類できること，大規模なデータであっても比較的早く収束することが利点である。
Rによるサンプルは以下のとおりである。

```{r kmeans}
result.k <- kmeans(d_matrix, centers = 3)
table(result.k$cluster, iris$Species)
```

#### クラスタリングの境界

ここまでのクラスタリングは，各個体がどのクラスターに所属するかが明確に定まっていたが，境界がそこまで明確でない中間的なデータ点もあるかもしれない。各データ点が1つのクラスターにのみ所属するという明確なクラスタリングのことを，ハードクラスタリングとかクリスプクラスタリングという。これに対して，各データ点が複数のクラスターに部分的に所属している，あるいは所属度が例えば0-1などの連続値で表現されるような，緩やかな所属をゆるするクラスタリングもあり，これらを総称してファジィクラスタリングとかソフトクラスタリングと呼ぶ。

ファジィクラスタリングの例として，fuzzy c-means法を挙げる。

```{r fuzzy c-means}
pacman::p_load(e1071)
result.c <- cmeans(d_matrix, centers = 3, m = 2)
head(result.c$membership)
table(result.c$cluster, iris$Species)
```

`fuzzy c-means`は`e1071`パッケージに含まれている。
モデルの指定の時に，ファジィ度パラメータ`m`を指定する。通常1.5から3程度で，大きいほど曖昧さをゆるす。
出力として`membership`という所属確率が返される。この所属確率が最大のものをハードな分類として使用することができる。

`membership`をプロットしてみると，明らかに所属するクラスが明確なものと，曖昧なデータもあることがわかる。
心理学的応用としては，パーソナリティの分類や症状の分類などが考えられるだろう。
```{r fuzzy-cmeans plot}
#| message: FALSE
#| dev: "ragg_png"
#| echo: FALSE
# membershipの行ごとの分散を計算
row_vars <- apply(result.c$membership, 1, var)

# 分散が大きい点と小さい点を選択
high_var_point <- which.max(row_vars)
low_var_point <- which.min(row_vars)
medium_var_point <- order(row_vars)[length(row_vars) %/% 2]

selected_points <- c(high_var_point, medium_var_point, low_var_point)
membership_subset <- result.c$membership[selected_points, ]

# 1行3列の図
par(mfrow = c(1, 3))
for (i in 1:3) {
  plot(1:3, membership_subset[i, ], type = "b", pch = 16, 
       xlab = "Cluster", ylab = "Membership Probability", 
       main = paste("Point", selected_points[i]),
       ylim = c(0, 1), xaxt = "n")
  axis(1, at = 1:3, labels = paste("C", 1:3))
}
par(mfrow = c(1, 1))
```

#### モデルベースか否か

### 多次元尺度構成法

## 共頻行列を用いるもの
## 偏相関行列を用いるもの

