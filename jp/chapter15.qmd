# 多変量解析(その2)

先の章では分散共分散行列(相関行列)に基づいた線型モデルを中心に紹介した。
しかし，多変量解析はそれだけではない。
むしろ測定モデルとして潜在変数を仮定する手法だけに固執するあまり，心理測定の仮定に違反するようなデータであっても因子分析を適用したり，モデルの適合度を優先しすぎて不自然な設定に走ったりするような誤用が多く見られる。

心理学においては因子分析が構成概念を測定していると「純粋に」信じられて多用されてきたが，かつてはさまざまな多変量解析技法が必要に応じて開発，使用されていたのである。
ここでは分析のスタートになる行列の種類で区分し，いくつかの多変量解析モデルを紹介する。

## 距離行列を用いるもの
距離とは，次の四つの公理を満たす数字のことをいう。

1. **非負性**: $d(x,y) \geq 0$ (距離は0以上)
2. **同一性**: $d(x,y) = 0 \Leftrightarrow x = y$ (距離が0なのは同じ点の場合のみ)
3. **対称性**: $d(x,y) = d(y,x)$ (距離は方向に依存しない)
4. **三角不等式**: $d(x,z) \leq d(x,y) + d(y,z)$ (遠回りは最短距離以上)

距離行列とは行列の要素が距離を表しているもので，一般に正方・対称行列になる。この点は分散共分散行列や相関行列と同じで，この行列演算によって分散共分散を用いたモデルとは別の解釈が成立する分析を作ることができる。

ちなみにRでは距離行列を作るのに`dist`関数を用いる。オプションとして特段の指定がなければユークリッド距離が用いられるが，他にも以下のようなオプションがある。

- `"euclidean"`: ユークリッド距離。$d(x,y) = \sqrt{\sum_{i=1}^N (x_i-y_i)^2}$で表される。
- `"maximum"`: チェビシェフ距離。$d(x,y) = max(|x_i - y_i|)$で表される。
- `"manhattan"`: マンハッタン距離。$d(x,y)=\sum (|x_i - y_i|)$で表される。
- `"canberra"`: キャンベラ距離。$d(x,y) = \sum \frac{|x_i-y_i|}{|x_i+y_i|}$で表される
- `"binary"`: バイナリ距離。ジャッカード距離ともいう。0/1のデータに対する距離で，$d(x,y) = \frac{b+c}{a+b+c+d}$で表される(aは両方1，bはxが1でyが0，cはxが0でyが1，dは両方0)。両方に共通して1である要素が多いほど距離が小さくなる。
- `"minkowski"`: ミンコフスキー距離。一般化された距離とも言われ，係数$p$でさまざまな距離を表現できる。$d(x,y)=\left(\sum |x_i-y_i|^p\right)^{\frac{1}{p}}$で表される。例えば$p=1$ならばマンハッタン距離，$p=2$ならユークリッド距離である。$p=\infty$の時を特にチェビシェフの距離，または優勢次元距離という。

### 心理学における距離データ

数字を何とみなすか，によって心理学でも距離データを扱うことはできる。
尺度評定の差分を(得点間の)距離とみなすこともできるし，相関係数も$1.0-|r_{jk}|$のようにすれば距離とみなすことができる。
社会心理学におけるソシオメトリックデータは，対人関係の選好評定だが，これも対人間の距離とみなすことができるだろう。
実験心理学における刺激の混同率や汎化勾配，2つの茂樹が同じか違うかを判断する課題への反応潜時，刺激の代替価・連想価は類似性と考えられるから，これも距離データと言えるだろう[@Takane198002]。
距離行列は一個体からの評定や反応からでも生成できるから，小サンプルの実験計画であっても距離行列を得ることができる。

このように，類似性あるいは非類似性を距離と見做して用いることができる。
この利点は，回答者の自然な判断に任せられること，つまり「総合的に判断して，似ているか，似ていないか」といった回答をデータにできることである。研究者はついつい複数の類似した項目で多角的に聞かねばならない，と思いがちだが，下位の評定次元を実験者が準備することは回答者の自由度を束縛している側面もあり，また回答者の負担を考えると必ずしもいいことばかりではない。さらに項目によっては社会的な望ましさバイアスなども含まれるから，「総合的に評価してもらいたい」というのはそういったバイアスから逃れられる側面もある。

距離を対象に考える多変量解析モデルも，そのほかのモデルと同様，要約や分類を目的にしている。また，多次元データを少数の次元に要約するために可視化する手法として用いられることもある。まずは分類を目的としたモデルから見ていこう。

### クラスター分析

クラスター分析は類似したものをまとめてクラスター(塊)を形成する分析方法である。
クラスター分析の中でも多くの手法・モデルが考えられており，いくつかの側面から分類することができる。

#### モデルの階層性

##### 階層的クラスター分析

まずはクラスターが階層性を持つかどうか。階層的クラスター分析は距離の短いものから順にまとめていき，クラスターのクラスター，クラスターのクラスターのクラスター，といったように順次大きなグループにまとめ上げていく。

結果は*デンドログラム*と呼ばれるツリー状のプロットで表されることが一般的で，適当なところで分割して利用する。適切なクラスター数に関する一般的な基準はほとんどなく，実用性に応じてツリーをカットすることが多い。

`iris`データによる実行例を示す。

```{r hierarical clustering}
data(iris)
d_matrix <- dist(iris[, -5], method = "euclidean")
result.h <- hclust(d_matrix, method = "ward.D2")
plot(result.h, main = "Hierarchical Clustering Dendrogram")
```
階層的クラスター分析のクラスター同士を上位クラスターにまとめていく方法がいくつか考案されており，Rでは`hclust`関数の`method`オプションで指定することができる。

- `"ward.D"` / `"ward.D2"`: ウォード法。クラスター内の分散を最小化する
- `"single"`: 最短距離法。クラスター間の最短距離でリンク
- `"complete"`: 最長距離法。クラスター間の最長距離でリンク
- `"average"`: 平均法(群平均法)。クラスター間の平均距離でリンク
- `"mcquitty"`: McQuitty法。重み付き群平均法の一種
- `"median"`: メディアン法。重み付き群中心法の一種
- `"centroid"`: 重心法。

最もよく使われるのがウォード法で，実践的にもこの手法による分類が最も解釈しやすい。なお，`ward.D`オプションはバグがあるので用いないことが望ましく，バグを修正した`ward.D2`を用いること[^15.1]。

[^15.1]: バグがあるのになぜ修正しないのか，と思われるかもしれない。`ward.D`はRのバージョン3.0.3以前まで用いられていたが，ユーザからの指摘で正しく分散が計算されていないことが発覚。Rの基本関数に誤りがあったことを認め，戒めとするために元のコードを残している。Rはフリーでオープンなソフトウェアであり，無償ではあるが，フリーだからといっていけないのではなく，こうした自浄作用があることを示すための措置と思われる。ちなみに筆者が以前プロプライエタリな統計ソフトを使っていた時にもおかしな挙動を発見したことがある。メーカに指摘すると「次のバージョンでは修正されているのでそちらを新たに購入しろ」という回答であったため，そのソフトウェアから決別することにした。科学的営みにおいて，有償サポートがあることが真実を担保しない例である。

得られたクラスターの結果は`cutree`関数で任意のクラスター数に分割できる。今回，`iris`データは3種類のirisがあることがわかっているので，クラスター数3にしてその分類精度を確認してみよう。

```{r cut tree}
clusters <- cutree(result.h, k = 3)
table(clusters, iris$Species)
```

##### 非階層的クラスター分析

非階層的クラスター分析として有名なのは，`k-means`法による分類である。アルゴリズムは次のとおりである。

1. 指定されたクラスター数の重心ベクトルをランダムに生成する。
2. 各データ点を最も近い重心のクラスターに所属させる。
3. クラスターごとに，重心を再計算する。
4. 2.-3. のステップを繰り返し，変動がなくなると推定終了とする。

この方法は任意のクラス数に分類できること，大規模なデータであっても比較的早く収束することが利点である。
Rによるサンプルは以下のとおりである。

```{r kmeans}
result.k <- kmeans(d_matrix, centers = 3)
table(result.k$cluster, iris$Species)
```

#### クラスタリングの境界

ここまでのクラスタリングは，各個体がどのクラスターに所属するかが明確に定まっていたが，境界がそこまで明確でない中間的なデータ点もあるかもしれない。各データ点が1つのクラスターにのみ所属するという明確なクラスタリングのことを，ハードクラスタリングとかクリスプクラスタリングという。これに対して，各データ点が複数のクラスターに部分的に所属している，あるいは所属度が例えば0-1などの連続値で表現されるような，緩やかな所属をゆるするクラスタリングもあり，これらを総称してファジィクラスタリングとかソフトクラスタリングと呼ぶ。

ファジィクラスタリングの例として，fuzzy c-means法を挙げる。

```{r fuzzy c-means}
pacman::p_load(e1071)
result.c <- cmeans(d_matrix, centers = 3, m = 2)
head(result.c$membership)
table(result.c$cluster, iris$Species)
```

`fuzzy c-means`は`e1071`パッケージに含まれている。
モデルの指定の時に，ファジィ度パラメータ`m`を指定する。通常1.5から3程度で，大きいほど曖昧さをゆるす。
出力として`membership`という所属確率が返される。この所属確率が最大のものをハードな分類として使用することができる。

`membership`をプロットしてみると，明らかに所属するクラスが明確なものと，曖昧なデータもあることがわかる。
心理学的応用としては，パーソナリティの分類や症状の分類などが考えられるだろう。
```{r fuzzy-cmeans plot}
#| message: FALSE
#| dev: "ragg_png"
#| echo: FALSE
# membershipの行ごとの分散を計算
row_vars <- apply(result.c$membership, 1, var)

# 分散が大きい点と小さい点を選択
high_var_point <- which.max(row_vars)
low_var_point <- which.min(row_vars)
medium_var_point <- order(row_vars)[length(row_vars) %/% 2]

selected_points <- c(high_var_point, medium_var_point, low_var_point)
membership_subset <- result.c$membership[selected_points, ]

# 1行3列の図
par(mfrow = c(1, 3))
for (i in 1:3) {
  plot(1:3, membership_subset[i, ],
    type = "b", pch = 16,
    xlab = "Cluster", ylab = "Membership Probability",
    main = paste("Point", selected_points[i]),
    ylim = c(0, 1), xaxt = "n"
  )
  axis(1, at = 1:3, labels = paste("C", 1:3))
}
par(mfrow = c(1, 1))
```

#### モデルベースか否か

階層的クラスタリングや，k-means, fuzzy c-meansの非階層的クラスタリングモデルでは，クラスター数の決定について客観的な指標がなかbunった。そこで，確率モデルとしてクラスター分析を考え，モデル適合度の観点から評価することを考える。


ある変数についてヒストグラムを描き，次のような出力を得たとしよう。
```{r bimodal}
#| message: FALSE
#| dev: "ragg_png"
#| echo: false
pacman::p_load(tidyverse, patchwork)
set.seed(12345)
mu1 <- -2
sig1 <- 1
mu2 <- 2
sig2 <- 1.5

N1 <- 1000
N2 <- 3000

dat <- data.frame(
  x = c(rnorm(N1, mu1, sig1), rnorm(N2, mu2, sig2)),
  id = c(rep(1, N1), rep(2, N2))
) %>%
  mutate(id = as.factor(id))
dat %>%
  ggplot(aes(x = x)) +
  geom_density() +
  theme_minimal()
```

このようなデータに対して，正規分布モデルを当てはめるのは適切だろうか。
正規分布は単峰で左右対称であることが特徴だから，無理やり当てはめるとおかしなことになるだろう。
ここには隠れた二つの正規分布があると考え，それぞれが混ざり合って出てきたものと考えたほうが良い。

```{r towmodels}
#| message: FALSE
#| dev: "ragg_png"
#| echo: false

# 単一正規分布モデル
p1 <- dat %>%
  ggplot(aes(x = x)) +
  geom_density(alpha = 0.5, fill = "lightblue") +
  stat_function(
    fun = dnorm,
    args = list(mean = mean(dat$x), sd = sd(dat$x)),
    color = "red", size = 1.2
  ) +
  labs(
    title = "単一正規分布モデル",
    x = "値", y = "密度"
  ) +
  theme_minimal()

# 混合正規分布モデル
p2 <- dat %>%
  ggplot(aes(x = x)) +
  geom_density(alpha = 0.5, fill = "lightblue") +
  stat_function(
    fun = function(x) (N1 / (N1 + N2)) * dnorm(x, mu1, sig1),
    color = "red", size = 1, linetype = "dashed"
  ) +
  stat_function(
    fun = function(x) (N2 / (N1 + N2)) * dnorm(x, mu2, sig2),
    color = "red", size = 1, linetype = "dashed"
  ) +
  stat_function(
    fun = function(x) {
      (N1 / (N1 + N2)) * dnorm(x, mu1, sig1) +
        (N2 / (N1 + N2)) * dnorm(x, mu2, sig2)
    },
    color = "red", size = 1.2
  ) +
  labs(
    title = "混合正規分布モデル",
    x = "x", y = "密度"
  ) +
  theme_minimal()

# patchworkで並べる
p1 + p2
```

統計モデルとしては，混合正規分布モデル(Gaussian Mixture Model, GMM)と呼ばれるのだが，これは異なる2つの群を生成モデルとしているクラスター分析であると考えることもできる。GMMでは観測されたデータが複数の正規分布を含んでいると考え，各群に想定される正規分布の平均，分散および群の混合率を推定することでデータの潜在的な特徴を明らかにする。今回は簡便のために1変数でのモデルにしたが，複数の変数がある場合は多変量正規分布で考えることになる。

確率モデルになっているので，尤度を用いてデータとの適合度を計算することができる。潜在的な分布がいくつあるのかをBICなどを基準に選定することで，客観的にクラス数を決定することができるのが利点である。

パッケージを使って具体的なデータを分類してみよう。

```{r mclust_iris}
#| message: FALSE
#| dev: "ragg_png"
pacman::p_load(mclust)

# irisデータから数値変数のみを取得
iris_data <- iris[, 1:4]

# mclustによるクラスタリング
gmm_result <- Mclust(iris_data)

# 結果の表示
summary(gmm_result, parameters = TRUE)

# 分類結果の可視化
plot(gmm_result, what = "classification")

# 真の種と比較
table(iris$Species, gmm_result$classification)

# BICによるモデル選択結果
plot(gmm_result, what = "BIC")
```

`mclust`では，各クラスターの共分散行列の構造によって異なるモデルが考慮される。モデル名は3文字のコードで表現され，それぞれの文字が以下の意味を持つ：

- **1文字目(Volume)**: 各クラスターの大きさ(体積)
  - `E` = Equal(等しい)
  - `V` = Variable(異なる)

- **2文字目(Shape)**: 各クラスターの形状
  - `E` = Equal(等しい)
  - `V` = Variable(異なる)

- **3文字目(Orientation)**: 各クラスターの向き
  - `E` = Equal(等しい)
  - `V` = Variable(異なる)
  - `I` = Identity(単位行列，球形)

例えば：
- **`EII`**: 全クラスターが同じ大きさの球形(等分散球形)
- **`VII`**: 各クラスターが異なる大きさの球形(異分散球形)
- **`EEE`**: 全クラスターが同じ大きさ・形状・向きの楕円
- **`VVV`**: 各クラスターが異なる大きさ・形状・向きの楕円(最も一般的)

これらの組み合わせにより，14種類の共分散構造から最適モデルをBICを参考に自動的に選定される。今回は`VEV (ellipsoidal, equal shape)`の2クラスターモデルが最適として判断された(実際は3種あるが，データからは2種類が最適と判断されている)。

潜在的に分類する手法は，例えばマーケティング業界では購買層を探索的に見出す手法として使われる。潜在的なクラスが順序尺度水準であることを想定すれば，潜在ランクモデルと呼ばれ，テスト理論の応用モデルとして提案されている。@TDE2022 ではIRTのような精緻な$\theta_i$の推定よりも段階的な推定の方が実践的意義が高いことから，潜在ランクモデルを活用する利点が論じられている。

#### バイクラスタリング

Cattelのデータキューブのところで触れたように，Observation $\times$ Variablesのデータセットがあったとき，変数の共変動から個人を分類することも，個人の共変動から変数を分類することもできる。そしてまた，両者を同時に分類するバイクラスタリング(Biclustering)という手法も提案されている。

BiclusteringはTwo-Mode Clusteringとも呼ばれ，様々なモデルが提案されているが，ここでは @TDE2022 のテスト理論の観点から見てみよう。

テスト理論はデータがバイナリであり，この分析の確率モデルとしてはベルヌーイ分布を置いたIRTが一般的である。しかし上で述べたように，IRTで推定されるような潜在得点$\theta$の精度は実質的な意味が見えにくいことがある。すなわち，$\theta$が$0.01$ポイント違うことが，どのような違いに相当するのか。$\theta$を$0.5$ポイント上昇させるために受検者はどのような努力をすれば良いのか。また実用上も，数段階の診断結果や，単純な合否の2段階に分割してのフィードバックをするのであれば，そこまで細かい分類は必要ないかもしれない。

@TDE2022 のバイクラスタリングでは，テストデータにおける項目を複数のフィールドに，受検者を複数のクラスに分類する。受験者の分類は正答率に応じて順序づけることができ，ランクとして表現することができる(ランクで表現されるモデルは特にランクラスタリングとよばれる)。

荘島のバイクラスタリングモデルを形式化するために，主要な行列を定義しよう。$J$を項目数，$S$を受検者数，$C$を潜在クラス/ランク数，$F$を潜在フィールド数とする。

バイクラスター参照行列$\boldsymbol{\Pi}_B$は次のように定義される：

$$
\boldsymbol{\Pi}_B=\left[\begin{array}{ccc}
\pi_{11} & \cdots & \pi_{1F} \\
\vdots & \ddots & \vdots \\
\pi_{C1} & \cdots & \pi_{CF}
\end{array}\right]=\left\{\pi_{fc}\right\}
$$

ここで各要素$\pi_{fc}$は，クラス/ランク$c$の受検者がフィールド$f$の項目に正答する確率を表す。

クラス所属行列$\mathbf{M}_C$とフィールド所属行列$\mathbf{M}_F$は次のように定義される：

$$
\mathbf{M}_C=\left[\begin{array}{ccc}
m_{11} & \cdots & m_{1C} \\
\vdots & \ddots & \vdots \\
m_{S1} & \cdots & m_{SC}
\end{array}\right], \quad
\mathbf{M}_F=\left[\begin{array}{ccc}
m_{11} & \cdots & m_{1F} \\
\vdots & \ddots & \vdots \\
m_{J1} & \cdots & m_{JF}
\end{array}\right]
$$

ランククラスタリングにおけるランク所属行列$\mathbf{M}_R$は，クラス所属行列$\mathbf{M}_C$に，前後のクラスのつながりを緩やかに繋げるフィルター行列$\mathbf{F}$をかけることで得られる。

$$
\mathbf{M}_R = \mathbf{M}_C \mathbf{F}
$$

フィルタ行列は，例えばランク数が6の場合，次のような行列になる。
$$
\mathbf{F}=\left[\begin{array}{rrrrrrr}
0.864 & 0.120 & & & & & \\
0.136 & 0.760 & 0.120 & & & & \\
& 0.120 & 0.760 & 0.120 & & & \\
& & 0.120 & 0.760 & 0.120 & & \\
& & & 0.120 & 0.760 & 0.120 & \\
& & & & 0.120 & 0.760 & 0.136 \\
& & & & & 0.120 & 0.864 \\
\end{array}\right]
$$

これらを踏まえて，尤度関数は次のように定義され，EMアルゴリズムによって推定される。

$$
l(\mathbf{U}\mid \boldsymbol{\Pi}_B) = \prod_{s=1}^S\prod_{j=1}^J\prod_{f=1}^F\prod_{c=1}^C \left(\pi_{fc}^{u_{sj}} (1-\pi_{fc})^{1-u_{sj}}\right)^{z_{sj} m_{sc} m_{jf}}
$$

これを実装したパッケージ`exametrika`とそのサンプルコードをみて，実践例を見てみよう。

```{r exametrika}
#| message: FALSE
#| dev: "ragg_png"
pacman::p_load(exametrika)
result.Ranklustering <- Biclustering(J35S515,
  nfld = 5, ncls = 6,
  method = "R", verbose = F
)
plot(result.Ranklustering, type = "Array")
```

`exametrika`パッケージのバイクラスタリングは，引数にデータ，フィールド数`nfld`，クラス(ランク)数`ncls`，および手法(`B`ならバイクラスタリング，`R`ならランクラスタリング)をとる。
ここではパッケージに含まれているサンプルデータ`J35S515`を使っているが，これは35項目からなるテストで515人の受検者からの回答を得たものである。

分析結果としてアレイプロットがしめされている。この図の左は，行ごとに受検者，列ごとに項目からなり，正答を黒い四角(■)，誤答を白い四角(□)で表現したローデータである。右に示されているのは分析結果による同様の表示で，ランクごと，フィールドごとに類似したパターンがまとめられていることがわかる。

フィールドの分類とランクをみることで，受検者には次のランクに進むにはどの領域の項目に正答すれば良いか，といった情報を提供することができる。また，フィールドやランクへの所属は確率で表現され(ファジィクラスタリング)，受検者にはランクアップOdds, ランクダウンOddsを提供することができる。

フィールド所属行列，ランク所属行列を可視化したプロファイルの出力を以下に示す。
```{r biclustering}
#| message: FALSE
#| dev: "ragg_png"
#| #| fig-width: 10
#| fig-height: 8
plot(result.Ranklustering, type = "FRP", nc = 2, nr = 3)
plot(result.Ranklustering, type = "RMP", students = 1:9, nc = 3, nr = 3)
```

バイクラスタリングは多値モデルも開発されており，心理尺度の新しい分析手法として期待されている。というのも，クラスタリングは表層的な反応パターンによる分類で，因子分析法やIRT(GRM)のようなデータ生成メカニズムを仮定しないことから，潜在変数の意味解釈といった理論的問題を避けることができるからである。また，項目についてのクラスタリングは得点もしくは変化得点を質的な意味に割り当てることが容易であることもその理由として挙げられる。

なお，パッケージ`exametrika`には @TDE2022 に含まれる12のモデル全てを実装している。詳しくは[サイト](https://kosugitti.github.io/exametrika/index.html)を参照のこと。


### 多次元尺度構成法

多次元尺度構成法(Multidimensional Scaling,MDS)は，一言で言えば距離行列から地図を復元する手法である。
MDSは大きく分けて計量(metric)MDSと非計量(non-metric)MDSがある。前者は距離行列の固有値分解から得られる固有ベクトルを座標とみなす，ストレートな表現方法であり，データが比率尺度水準以上の誤差のない数値であることが求められる。これによって， どうしてそのような解釈が可能なのかについては，Young-Householderの定理というMDSの基礎になった数学定理があるので，興味がある人は調べて見てほしい。

以下に計量MDSの例を見てみよう。Rは`eurodist`というヨーロッパ各都市の距離についてのサンプルデータを持っており，これを使って基本関数`cmdscale`でMDSを実行してみよう。

```{r cmdscale}
#| message: FALSE
#| dev: "ragg_png"
#| fig-asp: 1
result <- cmdscale(eurodist, k = 2)

# 美しいプロットの作成
plot(result[, 1], result[, 2],
  type = "n", # 点は描かずにプロット領域だけ作成
  xlab = "次元1", ylab = "次元2",
  main = "ヨーロッパ都市間の多次元尺度構成法",
  cex.main = 1.2,
  cex.lab = 1.1
)

# 都市名をラベルとして表示
text(result[, 1], result[, 2],
  labels = rownames(result),
  cex = 0.8,
  col = "darkblue",
  font = 2
)

# グリッドを追加
grid(lty = 2, col = "lightgray")
```

アテネ(Athens)が右上，ストックホルム(Stockholm)が下に来ていることから，南北が反転していると思われるが，それぞれの相対的な位置は大体復元できていることがわかるだろう。関数`cmdscale`は引数`k`で次元数を指定でき，今回は2次元を指定したが，実際は地球が球体だから`k=3`とすることが正しいが，事前知識がない場合は可視化のために低次元を指定することが一般的である。

`eudodist`は実際の距離データであるから，比率尺度水準の誤差のないデータと見做せるだろう。しかし，心理学的な応用場面においては，比率尺度水準のデータや誤差のないデータは考えにくい。このとき用いられるのが，非計量MDSである。これを簡単に言えば，データの持つ大小関係を反映した多次元空間に対象を付置する(地図の座標を与える)ものである。すなわち，対象$i$と$j$の距離を$d_{ij}$と表すとすると，
$$ d_{ij} < d_{kl} \to \delta_{ij} < \delta_{kl} $$

という関係が保持されるような座標$\delta_{ij}$を求めるものである。非計量MDSのはしりであったKruscalの方法は，データとの適合を表すstress値として，
$$ \sum_{i>j} e_{ij}^2 = \sum_{ij} (\delta_{ij}-d_{ij})^2 $$

という最適化関数を最小化するように座標を求める。最適化関数は多くの研究者によって様々に提唱されており，Rでは便利なパッケージ`smacof`のアルゴリズム(Scaling by Majorizing a COmplicated Function)による実行がいいだろう。

実例で見てみよう。`smacof`パッケージの持つ`FaceExp`データセットを用いる。これは顔表情の類似度評定データで、異なる表情間の知覚的類似性を測定したものである。
```{r smacof}
#| message: FALSE
#| dev: "ragg_png"
#| fig-width: 12
#| fig-height: 5
pacman::p_load(smacof)

# 次元数2から10でストレス値を計算
dimensions <- 2:10
stress_values <- numeric(length(dimensions))

for (i in seq_along(dimensions)) {
  result_temp <- mds(FaceExp, ndim = dimensions[i], type = "ordinal")
  stress_values[i] <- result_temp$stress
}
stress_values
# ストレス値のプロット
plot(dimensions, stress_values,
  type = "b", pch = 16,
  xlab = "次元数", ylab = "ストレス値",
  main = "次元数とストレス値の関係（顔表情データ）",
  cex.main = 1.1,
  cex.lab = 1.0,
  col = "blue", lwd = 2
)
grid(lty = 2, col = "lightgray")
```

ここでは次元数を様々に変えて，適合度指標であるストレス値を得てプロットしている。一般的に，ストレス値が0.05以下なら優秀，0.1以下なら良好，0.2以下なら普通とされる。

プロットや値を見てみると，3次元でも十分な解が得られそうだ。改めて3次元であることを指定して分析し，プロットしてみよう。なお，`mds`関数の`type`引数に順序尺度水準であることを明記することで，適切な分析が行われる。

```{r smacof2}
#| dev: "ragg_png"
#| fig-width: 8
#| fig-height: 6
# 3次元でのMDS結果
result <- mds(FaceExp, ndim = 3, type = "ordinal")
result

# 3次元MDSの可視化（3つの2次元プロット）

# 次元1 vs 次元2
plot(result$conf[, 1], result$conf[, 2],
  type = "n",
  xlab = "次元1", ylab = "次元2",
  main = paste("次元1 vs 次元2\nStress =", round(result$stress, 3)),
  cex.main = 1.0,
  cex.lab = 0.9
)
text(result$conf[, 1], result$conf[, 2],
  labels = rownames(result$conf),
  cex = 0.7,
  col = "darkblue",
  font = 2
)
grid(lty = 2, col = "lightgray")

# 次元1 vs 次元3
plot(result$conf[, 1], result$conf[, 3],
  type = "n",
  xlab = "次元1", ylab = "次元3",
  main = paste("次元1 vs 次元3\nStress =", round(result$stress, 3)),
  cex.main = 1.0,
  cex.lab = 0.9
)
text(result$conf[, 1], result$conf[, 3],
  labels = rownames(result$conf),
  cex = 0.7,
  col = "darkblue",
  font = 2
)
grid(lty = 2, col = "lightgray")

# 次元2 vs 次元3
plot(result$conf[, 2], result$conf[, 3],
  type = "n",
  xlab = "次元2", ylab = "次元3",
  main = paste("次元2 vs 次元3\nStress =", round(result$stress, 3)),
  cex.main = 1.0,
  cex.lab = 0.9
)
text(result$conf[, 2], result$conf[, 3],
  labels = rownames(result$conf),
  cex = 0.7,
  col = "darkblue",
  font = 2
)
grid(lty = 2, col = "lightgray")
```

```{r faceTable}
print(result$conf)
```

このプロットから，顔表情間の知覚的類似性の構造を読み取ることができる。第一次元は快不快（valence）、第二次元は覚醒度（arousal）、第三次元は自然性（naturalness）を表していると解釈できる。

より客観的な命名がしたい場合，座標と外部変数との相関などを求めて考える。相関係数はベクトルの角度に基づく$\cos(\theta)$であり，軸上に外部変数の補助線を引くなどすればわかりやすい。こうした方法については， @kosugi2016a に詳しい。

## 共頻行列を用いるもの

続いて共頻行列に基づく分析について考えてみよう。
共頻行列とはカテゴリカルなデータのクロス表であり，カテゴリが同時に生起していることを表す。このことが当該カテゴリの近さを表す共変量だとして分析を行う。

カテゴリカルなデータであるから，応用範囲は広い。よく知られているのがテキストマイニングにおける応用例である。

### テキストマイニングとは

テキストマイニングとは自然言語データを統計的に処理するための，一連の技法の名称である。小説やWeb上の記事，自由記述回答や逐語録など自由に書かれた自然無言語表現を統計的に処理し，そこから意味のある知見を引き出す。

日本語のテキストマイニングは，基本的に形態素解析＋多変量解析の技術の総称であると言える。日本語は英語をはじめとする単語で分かち書きされる文章とは違い，一連の文字列から品詞ごとに文章を区分する必要がある。この文章から品詞の切り分け，活用前の原型を抽出するなどの操作を形態素解析という。形態素解析には日本語辞書に基づく形態素解析エンジンが必要で，MECABやJANOME，ChaSenなどが知られている。これらをRやPythonで扱うためのパッケージも存在する。

形態素解析によって，文章の中に当該単語が何回出現したかを表す文書単語行列を作成する。これは一般に，単語の数が非常に多くなるから疎な矩形行列になる。この矩形行列を対象に特異値分解を行なったり，単語$\times$単語の(正方)共頻行列にすることで，多変量解析の対象となる。

多変量解析モデルはどのようなものでもよく，ここで紹介したクラスター分析や多次元尺度法などがよく用いられる。行列のサイズが大きくなりがちなので，一般的な多変量解析では少ない次元での適合度が悪くなりがちである。ある程度は適合度を諦めて可視化のために低次元にするか，自己組織化マップ(Self-Organization Mapping)[^15.2]などのアプローチで強制的に分類・可視化する方法などがとられることが多い。

[^15.2]: 自己組織化マップ（SOM）とは，Kohonen Mapとも呼ばれる。計算機科学社のKohonenによって開発された教師なし学習アルゴリズムの一種である。高次元データを2次元の格子上にマッピングし，データの構造を保持しながら可視化する手法である。

多変量解析で分析することももちろん可能だが，ビッグデータと機械学習の組み合わせにより，形態素ではなくトークン[^15.3]を単位に用いることも多い。

[^15.3]: トークンとは，機械学習においてテキストを構成する最小の意味単位であり，必ずしも形態素に限らず類似性をもとに区切られた数値ベクトルである。

ここでは形態素解析によるテキストマイニングの例を見てみよう。
Rで形態素解析をするには，`RMeCab`パッケージを用いることが多い。最新版は作者のGithubサイト[^15.4] から導入するのがいいだろう。まずは形態素解析エンジン`Mecab`をインストールし，その後で`RMecab`をインストールする[^15.5]。詳しくは作者のサイト[^15.4]を参照して欲しい。

最近はこの一連の手間を省いてくれる，`gibasa`というパッケージがある。`gibasa`はCRANに登録されており，内部でMeCabのバイナリファイルを含んでいるので，外部ファイルを準備する必要がない仕組みになっている[^15.6]。ここではこのパッケージを使った例を見てみよう。

[^15.4]:　RMecabのGithubサイト[https://github.com/IshidaMotohiro/RMeCab](https://github.com/IshidaMotohiro/RMeCab)
[^15.5]:　Mecabの公式サイト[http://taku910.github.io/mecab/](http://taku910.github.io/mecab/)
[^15.6]:　gibasaの解説[https://zenn.dev/paithiov909/articles/gibasa-intro](https://zenn.dev/paithiov909/articles/gibasa-intro)

パッケージを読み込んで，適当な文章を形態素解析してみよう。
```{r gibasa}
pacman::p_load(tidyverse, gibasa)
text <- "私は昨日，カフェでコーヒーを飲んだ。"
dat <- gibasa::tokenize(text)
dat
```

`tokenize`関数によって文字列が品詞ごとに区分され，またその特徴が`feature`列に入っていることがわかるだろう。
この`feature`列に入っているのは`gibasa`が内蔵する`MeCab`の戻り値であり，このラベルを整理するためには`prettify`関数を用いる。

また，実際のテキストマイニングにおいては助詞や記号は使いにくいし，動詞も活用する前の原型で考えるほうがいいだろう。
これらをパイプ演算子で繋ぎながら処理すると次のようになる。

```{r gibasa2}
gibasa::prettify(dat, col_select = c("POS1", "Original")) |>
  dplyr::filter(POS1 %in% c("名詞", "動詞", "形容詞"))
```

ここでは1つの文だけで分析を行ったが，次に実際のテキストマイニングの例として，京都ラーメンに関する感想文を使った分析例を見てみよう。

ここでの処理の流れは次のとおりである。

1. 文章を形態素に分割する
2. 文章ごとに単語の出現度数を書いた文書$\times$単語行列をつくる(`dtm`)
3. 文書単語行列から単語$\times$単語の距離行列を作る
4. 距離行列を対象に計量MDSを使って可視化


```{r kyoto_ramen_data}
# 京都ラーメンに関する感想文データの作成
ramen_reviews <- c(
  "京都ラーメンは意外とコッテリしたのが多いんだよね。",
  "濃厚なスープと細麺の組み合わせが絶妙で美味しかった。",
  "ドロドロとした鶏ガラベースのスープが京都らしくて好み。",
  "老舗のラーメンは深いコクがあり，麺との絡みが良くて好き。",
  "京都駅近くの店で食べた醤油ラーメンのチャーシューが柔らかくて最高。",
  "京都ラーメンといえば天下一品のコッテリが魅力的。",
  "背脂がたっぷりのラーメンも京都で食べると格別だった。",
  "細麺とスープのバランスが絶妙で，また食べたくなる味。",
  "京都らしい濃い醤油ラーメンに九条ネギがよく合っていた。",
  "老舗の店主が作る丁寧なラーメンは心に残る味わいだった。"
)

# トークン化して，品詞を選び出し，文章ごとにカウントする
dat_count <- ramen_reviews |>
  # テキストを形態素に分解（単語・品詞・活用形など）
  gibasa::tokenize() |>
  # 品詞情報と原型を選択・整理
  gibasa::prettify(col_select = c("POS1", "Original")) |>
  # 分析対象を名詞・形容詞に限定
  dplyr::filter(POS1 %in% c("名詞", "形容詞")) |>
  # データ変換処理
  dplyr::mutate(
    doc_id = forcats::fct_drop(doc_id),
    # 原型がある場合は原型を、ない場合は現在の形を使用
    token = dplyr::if_else(is.na(Original), token, Original)
  ) |>
  # 文書ID別・単語別に出現回数をカウント
  dplyr::count(doc_id, token)

# 文章単語行列の作成
dtm <- dat_count |>
  tidyr::pivot_wider(
    id_cols = doc_id,
    names_from = token,
    values_from = n,
    values_fill = 0 # 欠損値（その文書にその単語が出現しない）を0で埋める
  )

dtm
```

```{r textminig_example}
#| dev: "ragg_png"
#| fig-width: 8
#| fig-height: 6
distance_matrix <- dist(t(dtm[,-1]))

# 多次元尺度構成法（MDS）の実行
mds_result <- cmdscale(distance_matrix, k = 2) # 2次元での古典的MDS

# ggplotで可視化（ggrepelを使用してラベル重複を回避）
pacman::p_load(ggrepel) # ラベル重複回避のためのパッケージ

# MDS結果をデータフレームに変換
mds_df <- data.frame(
  dim1 = mds_result[, 1], # 第1次元の座標
  dim2 = mds_result[, 2], # 第2次元の座標
  word = rownames(mds_result) # 単語名
)

# ggplotで散布図を作成
ggplot(mds_df, aes(x = dim1, y = dim2)) +
  geom_point(color = "darkblue", size = 2, alpha = 0.7) + # 点をプロット
  geom_text_repel( # ggrepelを使用してラベル重複を回避
    aes(label = word), # 単語をラベルとして表示
    size = 3.5, # ラベルサイズ
    color = "darkblue", # ラベル色
    fontface = "bold", # 太字
    box.padding = 0.3, # ラベル周りの余白
    point.padding = 0.3, # 点周りの余白
    max.overlaps = Inf # 重複制限なし
  ) +
  labs(
    title = "ラーメンレビュー単語の多次元尺度構成法",
    x = "次元1",
    y = "次元2"
  ) +
  theme_minimal() + # シンプルなテーマ
  theme(
    plot.title = element_text(hjust = 0.5, size = 14), # タイトル中央寄せ
    panel.grid = element_line(linetype = "dashed", color = "lightgray") # グリッド線
  )
```

ここでプロットされているのは，単語同士の類似度が近いものは近くに，遠いものは遠くにプロットされる地図である。これをみると，「麺」と「スープ」や「組み合わせ」「バランス」といった言葉が近くに付置されており，関係が深そうなことが読み取れる。

ただし，「九条ネギ」が「九」と「条」に別れているように，機械的に分解することの限界もあり，こうした場合は辞書を変更したり，特定の単語・専門用語などを強制的に取り出すような工夫をする必要がある。形態素解析を経由したテキストマイニングは，多変量解析に入る前の事前クリーニングにかなりの労力を要することが少なくない。

また，今回は10件程度の例であったが，基本的にテキストマイニングは膨大なデータや学習された辞書をもちいることが主流になってきている。これは生成AIとの相性もよく(生成AIはすでに自然言語についてある程度学習済みのものとも言える)，形態素解析や統計モデルを経ないテキストマイニングがこれから主流になってくるかもしれない。

## 偏相関行列を用いるもの

最後に偏相関行列を用いる統計モデルを紹介しておこう。

(ピアソンの)相関行列は多くの線形モデルで多用されているが，変数$X$と$Y$の背後に，両者に影響する$Z$がある場合，見掛け上相関が高くなっているが，$Z$の影響を除外する(統計的に統制する)とそれほど相関が高くない，ということもあり得る。

この統計的に統制するというのは，$X$を$Z$で回帰して得られた残差$R_x$と，$Y$を$Z$で回帰して得られた残差$R_y$との相関であり，偏相関と呼ばれる。この相関の方が，本来的な関係を捉えていると言えるかもしれない。ここでは$X,Y$の2変数の例であったが，多変量の相関行列に関しても同様に当該2変数以外の変数で(重)回å帰をし，その残差同士の相関を出した偏相関行列を考えることができる。これは逐一回帰分析をしなくても，次のような行列演算で一括して求めることができる。

相関行列を$\mathbf{R}$とすると，偏相関行列$\mathbf{P}$の要素は次の式で計算される：
$$p_{ij} = -\frac{r^{ij}}{\sqrt{r^{ii} r^{jj}}}$$

ここで$r^{ij}$は相関行列の逆行列$\mathbf{R}^{-1}$の$(i,j)$要素である。

さて，因子分析は相関行列から始めて関係の強いところを因子にまとめるというイメージだが，この手順を反転させて，偏相関行列の関係の弱いところの繋がりをカットする，という方法で，純粋な変数間関係だけ残るようにして可視化する方法がある。

この分析方法は**グラフィカルモデリング**と呼ばれる。量的な変数の場合は偏相関行列から，質的な変数の場合でも多元分割行列から，変数間の条件付き独立性を検証する方法で進められる。可視化の手段として，変数間関係をノードとタイからなるネットワークで表現することから，最近では**ネットワーク分析**と呼ばれることもある。

心理学においては，因子が心理学的構成概念だと捉えられることが多い。しかし，構成概念とはそもそも単体でそこに「ある」ものではなく，いろいろな現象の総合的な全体である，というシステム論的な観点によれば，このネットワーク分析的なアプローチの方が適しているかもしれない。この理論的な体系と方法論の合体については，@Network2024 を参考にしてほしい。

具体的な例で見てみよう。
例えば，性格検査のBig5データを使って相関行列と，その偏相関行列を見てみよう。

```{r partial_corr}
#| dev: "ragg_png"
#| fig-width: 12
#| fig-height: 10
pacman::p_load(psych, corrplot, RColorBrewer)

# Big5性格データの読み込み（最初の25項目のみ使用）
bfi <- psych::bfi[, 1:25]
bfi_clean <- na.omit(bfi) # 欠損値を除去

# 1. 相関行列の計算
cor_matrix <- cor(bfi_clean)
# 2. 偏相関行列の計算（行列演算で実行）
R_inv <- solve(cor_matrix) # 相関行列の逆行列を計算

# 偏相関行列の各要素を計算
partial_cor_matrix <- matrix(0, nrow = nrow(R_inv), ncol = ncol(R_inv))
for(i in 1:nrow(R_inv)) {
  for(j in 1:ncol(R_inv)) {
    if(i != j) {
      # 偏相関の公式: p_ij = -r^ij / sqrt(r^ii * r^jj)
      partial_cor_matrix[i, j] <- -R_inv[i, j] / sqrt(R_inv[i, i] * R_inv[j, j])
    }
  }
}
# 対角要素は1に設定
diag(partial_cor_matrix) <- 1

# 行名・列名を設定
rownames(partial_cor_matrix) <- rownames(cor_matrix)
colnames(partial_cor_matrix) <- colnames(cor_matrix)
```


```{r compare_heatmaps}
#| dev: "ragg_png"
#| fig-width: 15
#| fig-height: 6

# 3. 相関行列と偏相関行列の比較表示
par(mfrow = c(1, 2))

# 相関行列ヒートマップ
corrplot(cor_matrix, 
         method = "color",
         order = "alphabet", # 同じ順序で比較
         tl.cex = 0.6,
         tl.col = "black",
         col = brewer.pal(n = 8, name = "RdYlBu"),
         title = "相関行列",
         mar = c(0, 0, 2, 0),
         cl.pos = "n") # カラーバーを非表示

# 偏相関行列ヒートマップ
corrplot(partial_cor_matrix, 
         method = "color", # 色で偏相関の強さを表示
         order = "alphabet", # アルファベット順で表示（相関行列と同じ順序で比較）
         tl.cex = 0.6, # 変数名のテキストサイズ
         tl.col = "black", # 変数名の色
         col = brewer.pal(n = 8, name = "RdYlBu"), # 青-黄-赤のカラーパレット
         title = "偏相関行列", # 図のタイトル
         mar = c(0, 0, 2, 0)) # 図の余白設定（下，左，上，右）

par(mfrow = c(1, 1)) #環境を元に戻す
```

相関行列と偏相関行列を並べてみると，所々相関のパターンが違うところが見える。

さて，ネットワークの表現をするには相関行列でも偏相関行列でも構わないのだが，一応ここでは偏相関行列を誓ったネットワークを見てみよう。

ここでは`qgraph`パッケージを用いている。
この関数のオプションとして，推定に`glasso`を用いているが，ガウシアングラフィカルモデルの一種で，L1正則化(lasso)を行い重要でない変数間の関係を0にするモデルである。これでBICを基準に不要なパスをカットして描画される。描画のレイアウトオプションは`spring`だが，これはなるべく自然な配置にしてくれる方法であり，他にもいろいろなオプションがあり得る。

```{r glasso}
pacman::p_load(qgraph)
BICgraph <- qgraph(
  partial_cor_matrix, # 偏相関行列を入力データとして使用
  graph = "glasso", 
  sampleSize = nrow(bfi), # サンプルサイズを指定（統計的有意性の判定に使用）
  tuning = 0, # 正則化パラメータ（0=BIC基準で自動選択、大きいほどスパース）
  layout = "spring", 
  title = "BIC", # グラフのタイトル
  threshold = TRUE, # 弱い結合を除去してスパースなネットワークを作成
  details = TRUE # 詳細な出力情報を表示
)
```

さて，ネットワーク分析は「これが因子！」のような特定の何かに帰着するのではなく，全体像をそのまま見て捉えることが特徴的である。しかしそれでは何がなんだか分かりにくいということもあろう。ということで，ネットワークの中心がどこにあるのか，どのノードとどのノードの結びつきが強いのか，といったことを指標とする。これらは中心性指数と呼ばれ，次のコードで可視化される。

```{r centrality}
centralityPlot(
  list(BIC = BICgraph),
  include = "all"
)
```

ここで示されている各指標の意味は次のとおり。

1. Strength（強度）

+ ノードに接続しているエッジの重みの合計
+ ネットワークでは、そのノードが他のノードとどれだけ強く関連しているかを表す
+ 次の式で計算される。$strength = \sum | w_{ij}|$（絶対値の和）

2. Closeness（近接中心性）

+ 他の全てのノードまでの最短距離の逆数
+ そのノードが他の全てのノードにどれだけ「近い」かを測定
+ 情報伝播や影響の広がりやすさを表すといえる
+ 距離の合計の逆数であり，次の式で計算される。$closeness = \frac{1}{\sum d_{ij}}$

3. Betweenness（媒介中心性）

+ 他のノード間の最短経路上にそのノードが位置する頻度であり，「橋渡し」の役割を果たすノードを特定する。
+ ネットワークでは症状間の連鎖反応の「ハブ」を表す

4. Expected Influence（期待影響力）

+ strengthの改良版で、正と負のエッジを区別したもの
+ 正のエッジは加算、負のエッジは減算。ここでは抑制関係も考慮した真の「影響力」を測定していると考えられる。

ネットワークモデルは，推定法もいろいろ工夫されているし，動的なモデル，時系列モデルなども扱える。今後の発展が楽しみな分析方法の一つと言えるだろう。